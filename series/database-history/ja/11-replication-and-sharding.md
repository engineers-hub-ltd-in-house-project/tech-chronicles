# データベースの地層

## ——RDBからNewSQLまで、データ管理50年の地殻変動

### 第11回：レプリケーションとシャーディング——スケールの壁を越える

**連載「データベースの地層——RDBからNewSQLまで、データ管理50年の地殻変動」**
**著：佐藤裕介（Engineers Hub株式会社 CEO / Technical Lead）**

---

**この回で学べること：**

- 一台のデータベースサーバの限界に直面したとき、エンジニアが取りうる選択肢——スケールアップとスケールアウトの本質的な違い
- MySQLレプリケーションの歴史——2000年のバージョン3.23から始まり、Statement-Based、Row-Based、半同期へと進化した20年
- PostgreSQLのストリーミングレプリケーション——2010年のバージョン9.0で組み込まれるまでの長い道のり
- シャーディングが「壊す」もの——JOIN、トランザクション、一意制約。RDBの前提が崩壊する瞬間
- CAP定理への布石——分散データベースの世界に足を踏み入れるとき、何を犠牲にするかを理解する

---

## 1. MySQLが悲鳴を上げた日

2007年頃、私が運用していたWebサービスのMySQLが限界を迎えた。

兆候はスロークエリログに現れた。ピーク時間帯に、これまで数十ミリ秒で返っていたクエリが数秒かかるようになった。`SHOW PROCESSLIST`を叩くと、接続数が上限に張り付いている。CPUのロードアベレージは常に二桁。ディスクI/Oのwait値が跳ね上がっている。対症療法として不要なインデックスの削除、クエリの最適化、memcachedによるキャッシュ導入を行ったが、根本的な解決にはならなかった。

トラフィックが増え続ける限り、一台のサーバでは処理しきれない。当時の物理サーバはCPU 4コア、メモリ16GB程度だ。今なら鼻で笑われるスペックだが、2007年当時としては「それなりの」構成だった。メモリを32GBに増やしても焼け石に水。ディスクをSSDに換装すればI/Oは改善するが、SSDは当時まだ高価で容量も小さく、本番投入には躊躇する価格だった。

上司に相談し、出た結論は「レプリケーション」だった。

マスタ・スレーブレプリケーションを導入し、読み取りクエリをスレーブに分散させる。`SELECT`文は全体クエリの80%以上を占めていた。読み取りを分散できれば、マスタの負荷は大幅に下がる——理屈は単純だ。だが、実際にやってみると、単純な話では済まなかった。

レプリケーションラグ。マスタに書き込んだデータが、スレーブに反映されるまでにタイムラグがある。ユーザーがデータを更新した直後にページを再読み込みすると、スレーブからは古いデータが返る。「保存したはずのデータが消えている」というクレームが上がった。

アプリケーション側でコネクションの振り分けロジックを実装する必要があった。書き込みはマスタへ、読み取りはスレーブへ。だが、更新直後の読み取りはマスタから返す必要がある。この「書き込み後の読み取り整合性（read-after-write consistency）」の保証は、アプリケーション層で行うしかなかった。

レプリケーションは問題を緩和したが、解決はしなかった。書き込み量が増えれば、マスタ一台がボトルネックになる。読み取りはスレーブを何台追加しても分散できるが、書き込みはマスタ一台に集中する。この壁を越えるには、テーブル自体を複数のサーバに分割する必要がある。

シャーディングだ。

シャーディングに手を出した日から、私のデータベース運用は根本的に変わった。JOINが使えない。トランザクションが保証されない。オートインクリメントのIDが重複する。RDBが当たり前のように提供してくれていた機能が、一つずつ奪われていく。「SQLを書けば正しい結果が返る」という信頼が揺らいだ。

あなたのプロジェクトは、一台のデータベースサーバで足りているだろうか。もし足りなくなったとき、何が起きるか——想像したことはあるだろうか。

---

## 2. スケーリングの二つの道——スケールアップとスケールアウト

### 垂直スケーリング（スケールアップ）の限界

データベースの処理能力が不足したとき、最初に検討するのは垂直スケーリング——つまりハードウェアの増強だ。CPUを高速なものに、メモリをより大容量に、ディスクをより高速なものに交換する。

垂直スケーリングの利点は明白だ。アプリケーションの変更が不要。SQLもスキーマもそのまま。JOINもトランザクションも正常に機能する。データベースの運用体制も変わらない。ハードウェアを交換するだけで性能が向上する。

だが、垂直スケーリングには物理的な天井がある。

2000年代前半、一般的なサーバのCPUは1〜4コア、メモリは数GBから16GB程度だった。最高スペックのサーバに載せ替えても、処理できるクエリの上限は有限だ。そして高スペックなハードウェアの価格は線形ではなく指数関数的に上昇する。CPUコア数を2倍にしても処理能力は2倍にならない（Amdahlの法則）。メモリを2倍にしてもディスクI/Oがボトルネックなら効果は限定的だ。

2010年代以降、クラウドの登場で垂直スケーリングはより手軽になった。AWS RDSのインスタンスタイプを変更するだけで、メモリ64GBから256GBへの増強が数クリックで完了する。しかし、最大のインスタンスタイプにも上限がある。そして、大きなインスタンスほど時間あたりのコストは急激に増加する。

垂直スケーリングは「最初に試すべき」正しい選択肢だ。運用の複雑さを増やさず、アプリケーションの変更なしに性能を改善できるなら、それに越したことはない。だが、いつかは天井にぶつかる。その日が来たとき、次の選択肢が水平スケーリング——スケールアウトだ。

### 水平スケーリング（スケールアウト）の二つの手法

水平スケーリングの基本的なアプローチは二つある。レプリケーションとシャーディングだ。

**レプリケーション**は、同じデータのコピーを複数のサーバに持たせる手法だ。全てのサーバが同一のデータを保持する。読み取りクエリを複数のサーバに分散させることで、読み取り性能を水平にスケールさせる。ただし、書き込みは依然として一台（マスタ）に集中する。

**シャーディング**は、データ自体を複数のサーバに分割する手法だ。各サーバはデータの一部分（シャード）だけを保持する。書き込みも読み取りも分散できるため、理論的にはサーバの追加に比例して性能が向上する。ただし、データが分散しているため、複数シャードにまたがるクエリは困難になる。

```
スケーリングの三つの選択肢

[スケールアップ]           [レプリケーション]         [シャーディング]
                         Master                   Shard A   Shard B
   ┌─────┐           ┌─────────┐              ┌────────┐ ┌────────┐
   │ DB  │           │  全データ │              │データ A │ │データ B │
   │(大) │           │  (書込)  │              │(読/書) │ │(読/書) │
   └─────┘           └────┬────┘              └────────┘ └────────┘
                     ┌────┴────┐
 ハードウェア増強    │         │               データを分割して
 アプリ変更不要    Slave A   Slave B           複数サーバに分散
 天井あり         ┌───────┐ ┌───────┐          書込も分散可能
                  │全データ│ │全データ│          JOINが困難に
                  │(読取) │ │(読取) │
                  └───────┘ └───────┘
                  読取を分散
                  書込は1台に集中
```

この二つの手法は排他的ではない。多くの大規模システムでは、シャーディングとレプリケーションを組み合わせて使う。各シャードがさらにマスタ・スレーブ構成を持ち、可用性と読み取り性能の両方を確保する。

---

## 3. レプリケーションの技術史——コピーの作法

### MySQLレプリケーションの進化

MySQLのレプリケーション機能は、2000年にリリースされたバージョン3.23.15で導入された。当時としては画期的な機能であり、LAMP（Linux, Apache, MySQL, PHP）スタック全盛期のWebサービスを支える基盤技術となった。

初期のMySQLレプリケーションは**Statement-Based Replication（SBR）**——マスタで実行されたSQL文をそのままバイナリログに記録し、スレーブでそのSQL文を再実行する方式だった。

```
Statement-Based Replication（SBR）の仕組み

Master                                    Slave
┌──────────────────┐                    ┌──────────────────┐
│ SQL実行           │                    │                  │
│ UPDATE users      │   バイナリログ     │ SQL再実行         │
│ SET name='Sato'  │ ──────────────→    │ UPDATE users      │
│ WHERE id=1;      │ (SQL文を転送)      │ SET name='Sato'  │
│                  │                    │ WHERE id=1;      │
└──────────────────┘                    └──────────────────┘
```

SBRはシンプルで帯域幅の消費が少ないという利点があったが、本質的な問題を抱えていた。**非決定的な関数**の存在だ。`NOW()`や`UUID()`、`RAND()`を含むSQL文は、マスタとスレーブで実行結果が異なりうる。マスタで`INSERT INTO logs (created_at) VALUES (NOW())`を実行した時刻と、スレーブで同じSQL文が再実行される時刻にはレプリケーションラグ分のずれがある。

MySQL 5.1（2008年）で**Row-Based Replication（RBR）**が導入された。RBRはSQL文ではなく、変更された行のbefore/afterイメージをバイナリログに記録する。マスタ側で`UPDATE`を実行した結果として「この行がこう変わった」という事実を転送するため、非決定的な関数の問題は解消される。

```
Row-Based Replication（RBR）の仕組み

Master                                    Slave
┌──────────────────┐                    ┌──────────────────┐
│ SQL実行           │                    │                  │
│ UPDATE users      │   バイナリログ     │ 行データを適用     │
│ SET name='Sato'  │ ──────────────→    │ id=1 の行:        │
│ WHERE id=1;      │ (行の変更を転送)   │ name: 'Tanaka'   │
│                  │ id=1:              │   → 'Sato'       │
│                  │ name 'Tanaka'→     │                  │
│                  │       'Sato'       │                  │
└──────────────────┘                    └──────────────────┘
```

RBRの代償は、バイナリログのサイズだ。一つの`UPDATE`文が100万行を更新した場合、SBRならSQL文一行で済むが、RBRは100万行分の変更データがバイナリログに書き込まれる。ネットワーク帯域とディスク消費が大幅に増加する。

MySQL 5.7.7以降、デフォルトのバイナリログフォーマットはSTATEMENTからROWに変更された。データの整合性を優先する判断だ。また、MySQL 5.1以降では**Mixed-Based Replication（MBR）**も使用可能で、通常はSBRを使い、非決定的な操作の場合だけ自動的にRBRに切り替える。

### 非同期・半同期・同期——レプリケーションの信頼性

MySQLの標準的なレプリケーションは**非同期**だ。マスタはバイナリログに書き込んだ時点でクライアントにコミット完了を返す。スレーブがそのイベントを受け取ったかどうかは確認しない。

```
非同期レプリケーション

Client        Master                 Slave
  │            │                      │
  │──COMMIT──→│                      │
  │            │── binlog書込 ──→     │
  │←─ OK ─────│                      │
  │            │      （後から転送）    │
  │            │─── イベント送信 ───→ │
  │            │                      │── relay log書込
  │            │                      │── SQL再実行
```

この方式ではマスタの応答速度は最速だが、マスタが突然クラッシュした場合、まだスレーブに転送されていないバイナリログのイベントが失われる可能性がある。マスタのディスクが物理的に破損すれば、そのデータは永久に失われる。

MySQL 5.5で**半同期レプリケーション（Semi-Synchronous Replication）**がプラグインとして導入された。マスタはコミット時に、少なくとも1台のスレーブがバイナリログのイベントをリレーログに書き込んだことを確認してから、クライアントにOKを返す。

```
半同期レプリケーション

Client        Master                 Slave
  │            │                      │
  │──COMMIT──→│                      │
  │            │── binlog書込 ──→     │
  │            │─── イベント送信 ───→ │
  │            │                      │── relay log書込
  │            │←── ACK ─────────── │
  │←─ OK ─────│                      │
  │            │                      │── SQL再実行（後から）
```

半同期レプリケーションは、マスタの応答速度を犠牲にする代わりに、データの耐久性を向上させる。スレーブがイベントを受領したことは確認するが、スレーブでの実行完了は待たない。「半」同期と呼ばれる所以だ。

**完全同期レプリケーション**は、すべてのスレーブでトランザクションが実行・コミットされるまでマスタが待つ方式だ。データの一貫性は最高だが、レイテンシは全ノードの中で最も遅いスレーブに律速される。MySQL Group Replication（MySQL 5.7.17以降で利用可能）は、Paxosベースの合意プロトコルを用いたマルチマスタレプリケーションを提供し、完全同期に近い一貫性を実現している。

### PostgreSQLのレプリケーション史

PostgreSQLのレプリケーション対応は、MySQLと比較して遅かった。

長い間、PostgreSQLでレプリケーションを実現するには外部ツールに頼るしかなかった。Slony-I（2004年）は、トリガーベースのロジカルレプリケーションを提供したが、セットアップの複雑さと運用負荷が課題だった。

転機は2010年、**PostgreSQL 9.0**で**ストリーミングレプリケーション**と**Hot Standby**が組み込まれたときだ。NTT OSSセンターのFujii Masaoが主要開発者として貢献した。

ストリーミングレプリケーションは、WAL（Write-Ahead Log）のレコードをリアルタイムでスタンバイサーバに転送する方式だ。それ以前のWALファイルシッピング（WALファイル単位での転送）と異なり、WALレコードが生成された時点で即座に転送されるため、レプリケーションラグを大幅に削減できた。

Hot Standbyは、ストリーミングレプリケーションで同期されたスタンバイサーバに対して読み取りクエリを実行できる機能だ。これにより、PostgreSQLでもMySQLと同様の「マスタで書き込み、スタンバイで読み取り」構成が可能になった。

その後、PostgreSQLのレプリケーション機能は着実に進化した。

- **9.1（2011年）**: 同期レプリケーション——少なくとも1台のスタンバイへのWAL書き込みを確認してからコミットを完了
- **9.4（2014年）**: 論理デコーディング（logical decoding）——WALから論理的な変更データを抽出する基盤
- **10（2017年）**: ネイティブの論理レプリケーション——テーブル単位、データベース単位でのレプリケーションが可能に

論理レプリケーションの導入は大きな意味を持った。ストリーミングレプリケーションがWALのバイナリデータをそのまま転送する「物理レプリケーション」であるのに対し、論理レプリケーションは行レベルの変更を論理的に転送する。これにより、異なるバージョンのPostgreSQL間でのレプリケーション、特定テーブルだけのレプリケーション、異なるスキーマ構造へのレプリケーションが可能になった。

### レプリケーションラグという宿命

レプリケーションの形態が非同期であれ半同期であれ、レプリケーションラグは避けられない。マスタでの書き込みがスレーブ/スタンバイに反映されるまでのタイムラグだ。

通常の運用では数十ミリ秒から数百ミリ秒程度のラグだが、以下の条件で悪化する。

マスタの書き込み負荷が高い場合、バイナリログ/WALの生成速度がスレーブの適用速度を上回り、ラグが蓄積する。大規模なALTER TABLE操作やバッチ処理は、数分から数十分のラグを引き起こすことがある。ネットワーク帯域が不十分な場合、特にクロスリージョンのレプリケーションでは、物理的な距離に起因するレイテンシが加わる。

レプリケーションラグがもたらす問題は**結果整合性（eventual consistency）**だ。マスタに書き込んだデータは「いつかは」スレーブにも反映されるが、「いつ」かは保証されない。

私が経験した典型的な問題を一つ挙げよう。ユーザーがプロフィールを更新し、直後にプロフィールページを表示する。更新はマスタに書き込まれるが、プロフィールページの表示はスレーブから読み取る。レプリケーションラグが数百ミリ秒あれば、ユーザーには「更新が反映されていない」ように見える。ユーザーは不安になり、もう一度更新ボタンを押す。結果として不要な書き込みが増え、さらに負荷が上がる。

この問題への対処パターンとして、私のチームでは以下の方針を採用した。

第一に、**書き込み直後の読み取りはマスタから行う**。セッション情報にタイムスタンプを記録し、直近N秒以内に書き込みがあった場合はマスタに接続する。第二に、**ラグの許容度をビジネス要件に合わせて定義する**。商品の在庫数は数秒のラグが致命的だが、商品レビューの表示は数分のラグでも許容できる。第三に、**レプリケーションラグを監視する**。ラグが閾値を超えた場合にアラートを飛ばし、原因を調査する。

---

## 4. シャーディング——データを分割する覚悟

### なぜシャーディングが必要になるのか

レプリケーションは読み取りを分散できるが、書き込みは依然としてマスタ一台に集中する。データ量が増え続ければ、一台のサーバのディスク容量を超える日も来る。

シャーディングは、テーブルのデータを複数のサーバに水平分割する手法だ。例えば、1億行のユーザーテーブルを4台のサーバに2,500万行ずつ分散させる。各サーバ（シャード）は独立したデータベースインスタンスとして動作し、それぞれのシャードに対する読み取りも書き込みも分散される。

「shard」は「破片」を意味する。一つの巨大なテーブルを「砕いて」複数のサーバに散らばらせるイメージだ。

### シャーディング戦略——データをどう分割するか

シャーディングの設計において最も重要な判断は、**何をシャーディングキーにするか**と**どのような方式で分割するか**だ。

**範囲ベースシャーディング（Range-Based Sharding）** は、キーの値の範囲でデータを分割する。例えば、ユーザーIDが1〜25,000,000はシャードA、25,000,001〜50,000,000はシャードB、というように。

```
範囲ベースシャーディング

user_id: 1 ~ 25,000,000         → Shard A
user_id: 25,000,001 ~ 50,000,000 → Shard B
user_id: 50,000,001 ~ 75,000,000 → Shard C
user_id: 75,000,001 ~ 100,000,000 → Shard D

利点: 範囲クエリが効率的（WHERE user_id BETWEEN 1 AND 1000）
欠点: データの偏り（ホットスポット）が発生しやすい
      新しいユーザーが常に最後のシャードに集中する
```

範囲ベースの利点は、連続する範囲のデータが同一シャードに格納されるため、範囲クエリが効率的な点だ。欠点は、データの偏り（ホットスポット）が発生しやすい点。ユーザーIDがシーケンシャルなら、最新のユーザー（最もアクティブなことが多い）が最後のシャードに集中する。

**ハッシュベースシャーディング（Hash-Based Sharding）** は、キーのハッシュ値をシャード数で割った余りでデータを分配する。

```
ハッシュベースシャーディング

hash(user_id) % 4 = 0 → Shard A
hash(user_id) % 4 = 1 → Shard B
hash(user_id) % 4 = 2 → Shard C
hash(user_id) % 4 = 3 → Shard D

利点: データが均等に分散される
欠点: 範囲クエリが全シャードに散る
      シャード数の変更（リシャーディング）が困難
```

ハッシュベースの利点は、データが均等に分散されやすい点だ。欠点は、範囲クエリが全シャードに散ってしまう点。`WHERE user_id BETWEEN 1 AND 1000`というクエリは、4つのシャード全てに問い合わせる必要がある。

**ディレクトリベースシャーディング（Directory-Based Sharding）** は、キーとシャードの対応を別の「ディレクトリ」テーブルで管理する。柔軟性は最も高いが、ディレクトリ自体がSPOF（Single Point of Failure）になるリスクがある。

### Consistent Hashing——リシャーディングの悪夢への解

ハッシュベースシャーディングの最大の弱点は、シャード数の変更だ。`hash(key) % 4`で4台に分散していたデータを、5台に増やすと`hash(key) % 5`になる。ほぼ全てのキーのシャード割り当てが変わり、大量のデータ移行が必要になる。

1997年、David Kargerらは「Consistent Hashing and Random Trees」をACMのSTOC（Symposium on Theory of Computing）で発表した。この論文の核心は、ノードの追加・削除時に再配置されるデータを最小限に抑えるハッシュ方式だ。

```
Consistent Hashing の概念

                     0
                  .-´   `-.
                /    Node A  \
              /        │       \
           /           │         \
        Node D ────────┼──────── Node B
           \           │         /
              \        │       /
                \   Node C   /
                  `-.   .-´
                     │
                   2^32

キーもノードもハッシュリング上の位置にマッピングされる
各キーは、リング上で時計回りに最初に見つかるノードに格納される
ノードが追加されても、再配置されるのはその直前のキーだけ
```

Consistent Hashingでは、キーもノードもハッシュリング上にマッピングされる。各キーは、リング上で時計回りに最初に見つかるノードに格納される。ノードが追加された場合、再配置されるのは新ノードと直前のノードの間のキーだけだ。ノード数がN台のとき、1台追加しても再配置されるキーは全体の約1/N。

この概念は2007年のAmazon Dynamo論文で分散データベースの文脈で広く知られるようになり、Cassandra、DynamoDB、Riak など多くの分散データストアの基盤技術となった。

### シャーディングが壊すもの

シャーディングはスケーラビリティをもたらすが、RDBが当たり前に提供していた機能を破壊する。ここが核心だ。

**JOINの崩壊**。ユーザーテーブルがシャードAにあり、注文テーブルがシャードBにある場合、`SELECT * FROM users JOIN orders ON users.id = orders.user_id`はデータベースレベルでは実行できない。アプリケーション側で両方のシャードにクエリを発行し、結果をメモリ上でマージする必要がある。これは単にパフォーマンスの問題ではない。SQLの表現力が根本的に制限されるということだ。

```
シャーディングがJOINを壊す

Shard A (user_id % 2 = 0)          Shard B (user_id % 2 = 1)
┌──────────────────┐              ┌──────────────────┐
│ users (偶数ID)    │              │ users (奇数ID)    │
│ orders (偶数ID)   │              │ orders (奇数ID)   │
└──────────────────┘              └──────────────────┘

Q: user_id=1 の注文を全件取得 → Shard B だけで完結（OK）
Q: 全ユーザーの注文合計金額    → 両シャードに問合せ+アプリ側で集約（遅い）
Q: 注文金額上位10ユーザー     → 両シャードで部分結果取得+マージ+再ソート
```

**トランザクションの崩壊**。ACIDトランザクションは、単一のデータベースインスタンス内で保証される。シャードをまたいだトランザクション——例えば「シャードAのユーザーの残高を減らし、シャードBのユーザーの残高を増やす」——は、二相コミット（2PC）のような分散トランザクションプロトコルなしには実現できない。二相コミットは遅く、コーディネータのSPOFリスクがあり、多くのアプリケーション開発者が避けたがる。

**一意制約の崩壊**。`AUTO_INCREMENT`のIDは各シャードで独立に採番されるため、シャードをまたいだグローバルな一意性が保証されない。UUIDを使う、あるいはTwitterのSnowflakeのようなグローバルID生成サービスを導入する必要がある。

**集約クエリの崩壊**。`COUNT(*)`、`SUM()`、`AVG()`、`ORDER BY ... LIMIT`は全シャードに問い合わせ、結果をアプリケーション側で集約する必要がある。特に`ORDER BY ... LIMIT`は、各シャードから上位N件を取得した後、マージして再ソートし、最終的な上位N件を抽出するというコストの高い処理になる。

私が最も苦労したのは、シャーディング導入後のアプリケーションコードの複雑化だった。SQL一行で済んでいた処理が、「どのシャードに問い合わせるか」「結果をどうマージするか」「エラー時にどう整合性を保つか」というロジックに膨れ上がる。フレームワークのORMはシャーディングを知らない。生SQLを書き、コネクション管理を手動で行い、リトライロジックを自前で実装する。テストも複雑化する。「データベースが一つ」という前提のテストは全て書き直しだ。

### シャーディングミドルウェアの登場

アプリケーション層でのシャーディングの複雑さを軽減するため、シャーディングミドルウェアが登場した。

**Vitess**は2010年にYouTubeで開発された。YouTubeのMySQLは急成長するトラフィックに対応するためシャーディングを導入したが、アプリケーション層にシャーディングロジックを埋め込む方式では管理が困難になっていた。Vitessはアプリケーションとデータベースの間にプロキシ層を挟み、シャーディングを透過的に行う。アプリケーションからは単一のMySQLに見える。2011年にオープンソース化され、現在はCNCF（Cloud Native Computing Foundation）のプロジェクトとして、Slack、Pinterest、GitHubなど多くの企業に採用されている。

MariaDBでは**Spider Storage Engine**（Kentoku SHIBAが開発）がMariaDB 10.0.4からバンドルされ、ストレージエンジンレベルでのシャーディングを提供した。だが、MySQL本体にはシャーディング機能が組み込まれることはなかった。RDBMSの設計思想として、データの分散はアプリケーション層またはミドルウェアの責務と位置づけられていたのだ。

### Google Bigtable——スケーラビリティの新しい方向性

2006年、Googleは「Bigtable: A Distributed Storage System for Structured Data」をOSDI '06（第7回USENIX Symposium on Operating Systems Design and Implementation）で発表した。著者はFay Chang、Jeffrey Dean、Sanjay Ghemawatら。

Bigtableは、RDBの制約を受け入れないアプローチだった。リレーショナルモデルは採用せず、行キー・列ファミリ・タイムスタンプの三次元構造でデータを管理する。SQLは提供しない。JOINは存在しない。ACIDトランザクションは単一行に限定される。その代わり、数千台のコモディティサーバにペタバイト規模のデータを分散格納し、自動的にシャーディングとリバランシングを行う。

Bigtable論文が示したのは、RDBのスケーリング問題に対する根本的に異なる解だった。RDBの機能を維持したままスケールさせるのではなく、RDBの機能の一部を意図的に捨てることでスケーラビリティを手に入れる。この発想は、2009年以降のNoSQLムーブメントの直接的な種子となった。

---

## 5. ハンズオン: レプリケーションとシャーディングを体験する

今回のハンズオンでは、二つの体験を行う。第一に、PostgreSQLの論理レプリケーションを構築し、レプリケーションラグを観測する。第二に、アプリケーション層での簡易シャーディングを実装し、クロスシャードクエリの困難さを体験する。

### 演習概要

1. PostgreSQLのパブリッシャー（マスタ）とサブスクライバー（レプリカ）を構築し、論理レプリケーションを確認する
2. レプリケーションラグを意図的に発生させ、計測する
3. 二つのPostgreSQLインスタンスをシャードとして構成し、ハッシュベースシャーディングを体験する
4. クロスシャードクエリの困難さを体験する

### 環境構築

```bash
# handson/database-history/11-replication-and-sharding/setup.sh を実行
bash setup.sh
```

### 演習1: 論理レプリケーションの構築

setup.shがパブリッシャー（pub）とサブスクライバー（sub）の二つのPostgreSQLコンテナを起動し、論理レプリケーションを構成している。

パブリッシャーに接続して、レプリケーションの状態を確認する。

```bash
docker exec -it db-history-ep11-pub psql -U postgres -d handson
```

```sql
-- パブリケーションの確認
SELECT * FROM pg_publication;

-- レプリケーションスロットの確認
SELECT slot_name, active, confirmed_flush_lsn
FROM pg_replication_slots;

-- パブリッシャーにデータを挿入する
INSERT INTO users (name, email) VALUES
    ('Tanaka', 'tanaka@example.com'),
    ('Suzuki', 'suzuki@example.com'),
    ('Sato', 'sato@example.com');
```

サブスクライバーに接続して、データがレプリケートされていることを確認する。

```bash
docker exec -it db-history-ep11-sub psql -U postgres -d handson
```

```sql
-- サブスクリプションの確認
SELECT * FROM pg_subscription;

-- レプリケートされたデータを確認
SELECT * FROM users;
-- → パブリッシャーで挿入した3行が表示される
```

### 演習2: レプリケーションラグの観測

パブリッシャーで大量データを投入し、サブスクライバーでの反映タイミングを観測する。

パブリッシャーで実行:

```sql
-- 大量データの投入（10万行）
INSERT INTO users (name, email)
SELECT
    'User_' || i,
    'user' || i || '@example.com'
FROM generate_series(1, 100000) AS i;

-- パブリッシャーの行数を確認
SELECT COUNT(*) FROM users;
-- → 100,003
```

サブスクライバーで実行（パブリッシャーとほぼ同時に）:

```sql
-- サブスクライバーの行数を確認（繰り返し実行してラグを観測）
SELECT COUNT(*) FROM users;
-- → 最初は100,003より少ない行数が返る可能性がある
-- → しばらくすると100,003に追いつく

-- レプリケーション統計を確認
SELECT
    received_lsn,
    latest_end_lsn,
    last_msg_send_time,
    last_msg_receipt_time
FROM pg_stat_subscription;
```

### 演習3: 簡易シャーディングの体験

setup.shが二つの独立したPostgreSQLインスタンス（shard-0, shard-1）を構成している。ハッシュベースシャーディングを体験する。

シャード0に接続:

```bash
docker exec -it db-history-ep11-shard0 psql -U postgres -d handson
```

```sql
-- シャード0のデータを確認（user_id が偶数のデータ）
SELECT COUNT(*) FROM orders;
SELECT * FROM orders LIMIT 5;
```

シャード1に接続:

```bash
docker exec -it db-history-ep11-shard1 psql -U postgres -d handson
```

```sql
-- シャード1のデータを確認（user_id が奇数のデータ）
SELECT COUNT(*) FROM orders;
SELECT * FROM orders LIMIT 5;
```

### 演習4: クロスシャードクエリの困難さ

単一シャード内で完結するクエリは通常通り動作する:

```sql
-- シャード0で: 特定ユーザーの注文合計（シャード内で完結）
SELECT user_id, SUM(amount) AS total
FROM orders
WHERE user_id = 2
GROUP BY user_id;
-- → 問題なく動作する
```

しかし、全ユーザーの注文合計を出す場合は両シャードに問い合わせる必要がある:

```sql
-- シャード0で: 全ユーザーの注文合計を出そうとする
SELECT user_id, SUM(amount) AS total
FROM orders
GROUP BY user_id
ORDER BY total DESC
LIMIT 5;
-- → シャード0のデータしか含まれない！（偶数user_idのみ）
```

正しい結果を得るには、両方のシャードに同じクエリを発行し、アプリケーション側でマージする必要がある。これがシャーディングの本質的な困難さだ。

### 後片付け

```bash
docker rm -f db-history-ep11-pub db-history-ep11-sub \
    db-history-ep11-shard0 db-history-ep11-shard1
docker network rm db-history-ep11-net 2>/dev/null || true
```

---

## 6. スケーリングの壁は、設計思想の壁でもある

第11回を振り返ろう。

**レプリケーションは読み取りの分散を実現するが、書き込みはマスタ一台に集中するという制約を持つ。** MySQLは2000年にSBR方式でレプリケーションを導入し、RBR、半同期へと進化した。PostgreSQLは2010年の9.0でストリーミングレプリケーションを組み込み、2017年の10で論理レプリケーションを実現した。いずれの方式でもレプリケーションラグは避けられず、結果整合性の問題をアプリケーション層で対処する必要がある。

**シャーディングは書き込みを含む水平スケーリングを実現するが、RDBの根幹機能を破壊する。** JOIN、トランザクション、一意制約、集約クエリ——これらがシャード境界を越えられなくなる。アプリケーションの複雑度は急激に上昇し、SQLの表現力は大幅に制限される。

**シャーディング戦略の選択——範囲ベース、ハッシュベース、ディレクトリベース——は、データの特性とクエリパターンに依存する。** Consistent Hashing（1997年、Kargerら）はリシャーディング時のデータ移動を最小化する技法として、後の分散データベースの基盤技術となった。

**Vitessのようなシャーディングミドルウェアはアプリケーション層の複雑さを軽減するが、RDBの機能制限そのものは解消しない。** シャーディングの本質的な困難さは、データが物理的に異なるサーバに分散している以上、避けられない。

**Google Bigtableは、RDBの機能を維持したままスケールさせるのではなく、機能の一部を捨てることでスケーラビリティを得るという発想を示した。** この発想がNoSQLムーブメントの直接的な種子となった。

冒頭の問いに戻ろう。「一台のデータベースサーバでは足りなくなったとき、何が起きるのか？」

起きるのは、トレードオフの選択だ。レプリケーションで読み取りを分散させるか。シャーディングで書き込みも分散させるか。シャーディングを選ぶなら、JOINやトランザクションの制約を受け入れるか、それとも——RDBの枠組み自体を捨てて、別のパラダイムに移行するか。

2000年代後半、多くのエンジニアがこの岐路に立った。そして一部の人々は、RDBのスケーリング限界に対する答えとして、根本的に異なるアプローチを選んだ。「RDBの制約を外し、スケーラビリティと可用性を最優先にする」——NoSQLの時代の幕開けだ。

だがその前に、分散システムにおける根本的な制約を理解する必要がある。次回は、「CAP定理——分散システムの不可能三角形」を取り上げる。2000年にEric BrewerがPODCの基調講演で提唱し、2002年にGilbertとLynchが証明したこの定理は、分散データベースの設計判断の根幹をなす。「完璧な分散データベースはなぜ存在しないのか」——この問いに、あなたは答えられるだろうか。

---

### 参考文献

- MySQL 5.7 Reference Manual, "Replication Formats". <https://dev.mysql.com/doc/refman/5.7/en/replication-formats.html>
- MySQL 5.7 Reference Manual, "Semisynchronous Replication". <https://dev.mysql.com/doc/refman/5.7/en/replication-semisync.html>
- Marcelo Altmann, "A Brief History of MySQL Replication". <https://altmannmarcelo.medium.com/a-brief-history-of-mysql-replication-85f057922800>
- PostgreSQL 9.0 Press Release, September 20, 2010. <https://www.postgresql.org/about/news/postgresql-90-final-release-available-now-1235/>
- PostgreSQL Wiki, "Streaming Replication". <https://wiki.postgresql.org/wiki/Streaming_Replication>
- PostgreSQL 10 Release Announcement, October 5, 2017. <https://www.postgresql.org/about/news/postgresql-10-released-1786/>
- David Karger et al., "Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web", STOC '97, pp. 654-663. <https://dl.acm.org/doi/10.1145/258533.258660>
- Fay Chang et al., "Bigtable: A Distributed Storage System for Structured Data", OSDI '06. <https://www.usenix.org/conference/osdi-06/bigtable-distributed-storage-system-structured-data>
- Eric Brewer, "Towards Robust Distributed Systems", PODC 2000 Keynote. <https://people.eecs.berkeley.edu/~brewer/cs262b-2004/PODC-keynote.pdf>
- Seth Gilbert, Nancy Lynch, "Brewer's Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services", ACM SIGACT News, 2002. <https://dl.acm.org/doi/10.1145/564585.564601>
- Vitess Documentation, "History". <https://vitess.io/docs/20.0/overview/history/>

---

**次回予告：** 第12回「CAP定理——分散システムの不可能三角形」では、「MongoDBならスケールする」と信じてプロジェクトを始め、ネットワーク分断で書き込みが消えた日の衝撃から始める。Eric BrewerがPODCで提唱したCAP予想、GilbertとLynchによる形式証明、そしてCAP定理に対する誤解と批判を解き明かす。「完璧な分散データベース」が存在しない理由を理解したとき、データベース設計の視座は根本的に変わる。
