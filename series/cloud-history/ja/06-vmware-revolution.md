# クラウドの考古学

## ——メインフレームからサーバーレスへ、計算資源の民主化史

### 第6回：VMwareの革命——一台の物理マシンに複数のOSを走らせる

**連載「クラウドの考古学——メインフレームからサーバーレスへ、計算資源の民主化史」**
**著：佐藤裕介（Engineers Hub株式会社 CEO / Technical Lead）**

---

**この回で学べること：**

- IBM CP-40（1967年）からVM/370（1972年）に至るメインフレーム仮想化の系譜
- x86アーキテクチャが仮想化困難だった技術的理由——Popek-Goldberg要件と17個の問題命令
- VMwareが採用したバイナリトランスレーションという解決策の独創性
- VMware Workstation（1999年）、ESX Server（2001年）、vMotion（2003年）が変えた世界
- ハイパーバイザType 1とType 2の設計思想の違い
- QEMUとKVMによる仮想化のオーバーヘッド測定の実践

---

## 1. LinuxとWindowsが同時に動いている

2005年のある日、私はVMware Workstationのウィンドウを前にして、しばらく画面を見つめていた。

デスクトップの左側にはLinux（当時はCentOS 4だった）のターミナルが開いている。Apacheの設定をいじり、動作確認をしている。右側にはWindows XPが動いている。Internet Explorerでその設定したWebサイトの表示を確認している。2つのOSが、1台のマシンの上で、同時に動いている。

それまでの私にとって、「OSを切り替える」とはデュアルブートを意味していた。GRUBのメニューでLinuxかWindowsを選び、再起動を待つ。Linux上で開発し、Windowsでの表示を確認したければ、その度に再起動する。あるいは、物理的に2台のマシンを並べる。どちらにしても、OSとは物理マシンと一対一で結びついた存在だった。

VMware Workstationは、その前提を崩した。1台のマシンの上で複数のOSが並列に動く。CPUもメモリもディスクも、物理的には1セットしかないのに、それぞれのOSは「自分だけのハードウェアで動いている」と信じている。騙しているのだ。仮想化とは、突き詰めれば、ゲストOSを巧妙に騙す技術である。

だが、本当の衝撃はその後に来た。

本番環境にVMware ESXを導入し、サーバ統合を始めたときのことだ。それまで10台の物理サーバで個別に動いていたサービスを、2台の物理サーバ上の仮想マシンとして統合した。10台のラックマウントサーバが占有していたスペース、消費していた電力、生み出していた熱——それが2台分に縮まった。物理サーバの利用率は10-15%だった。つまり、CPUの85%以上は何もせずに遊んでいた。仮想化によって、その「遊んでいたリソース」を使い切れるようになったのだ。

私はサーバ統合のROI計算書を経営陣に提出した記憶がある。ハードウェアコストの削減、電力コストの削減、ラックスペースの削減、運用工数の削減。数字は明白だった。仮想化は技術的な好奇心の産物ではない。純粋にビジネス上の合理性がある。

ここで考えてほしい。あなたが今 `aws ec2 run-instances` で起動するインスタンスは、仮想マシンだ。その仮想マシンは、AWSのデータセンターにある物理サーバの上で、他の誰かの仮想マシンと同居している。1台の物理マシンに複数のOSを走らせる——この技術がなければ、クラウドコンピューティングは成り立たない。仮想化はクラウドの前提条件であり、基盤技術であり、存在の根拠だ。

では、その仮想化技術はどこから来たのか。VMwareは何を解決し、なぜそれが可能になったのか。

---

## 2. 仮想化の起源——メインフレームが最初に解いた問題

### IBM CP-40：仮想化はメインフレームから始まった

仮想化の歴史は、1960年代のIBMに遡る。

1967年、IBM Cambridge Scientific CenterでCP-40/CMS（Console Monitor System）の本番運用が開始された。CP-40はIBM System/360 Model 40の特別改造機上で動作し、完全仮想化を実装した最初のオペレーティングシステムだった。1台の物理マシン上で14の仮想マシンを同時にサポートした。

CP-40の動作原理は、後のすべての仮想化技術の原型となった。各仮想マシンは「問題状態（problem state）」で実行される。I/O操作などの特権命令を実行しようとすると例外が発生し、その例外を制御プログラム（CP）が捕捉して、適切にシミュレーションする。仮想メモリ上の参照も同様で、メインメモリに存在しないページへのアクセスはページフォルトを発生させ、制御プログラムが処理する。

この「特権命令の実行を捕捉し、制御プログラムがシミュレーションする」という設計は、trap-and-emulate（トラップ・アンド・エミュレート）と呼ばれる。仮想化の最も基本的なメカニズムだ。

CP-40は研究プロジェクトとしての性格が強かったが、その成果はCP-67/CMSに引き継がれ、1972年8月2日、IBMはSystem/370向けにVM/370を正式に発表した。VM/370は、1台のメインフレーム上で複数のオペレーティングシステムを同時に実行するための製品だ。顧客はもはや「どのOSをロードするか」を選ぶ必要がなかった。すべてのOSを同時にロードできたのだ。

IBMはVM/370のソースコードを顧客に出荷した。顧客はカスタム修正を加えることができた。メインフレームの世界では、仮想化は1970年代の時点で確立された技術であり、日常的な運用手法だった。

```
仮想化の系譜——メインフレームからx86へ:

1967  CP-40/CMS（IBM Cambridge Scientific Center）
  │    ── 完全仮想化を実装した最初のOS
  │    ── 14の同時仮想マシンをサポート
  ↓
1972  VM/370（IBM、System/370向け）
  │    ── 仮想化の商用製品化
  │    ── ソースコード顧客出荷
  ↓
1974  Popek-Goldberg論文
  │    ── 仮想化の形式的要件を定義
  │    ── x86はこの要件を満たさない（後に判明）
  :
  :   ← 約25年間、x86仮想化は「不可能」とされた
  :
1998  VMware創業
  │    ── バイナリトランスレーションでx86仮想化を実現
  ↓
1999  VMware Workstation 1.0
  │    ── x86初の商用仮想化製品（Type 2）
  ↓
2001  VMware ESX Server 1.0
  │    ── ベアメタルハイパーバイザ（Type 1）
  ↓
2003  vMotion + VirtualCenter
  │    ── ライブマイグレーション、集中管理
  ↓
2005  Intel VT-x
  ↓
2006  AMD-V
       ── ハードウェアレベルで仮想化をサポート
```

### Popek-Goldbergの壁——x86はなぜ仮想化できなかったか

1974年、Gerald J. PopekとRobert P. GoldbergはCommunications of the ACM（Vol.17, No.7, pp.412-421）に「Formal Requirements for Virtualizable Third Generation Architectures」と題する論文を発表した。この論文は、コンピュータアーキテクチャが仮想化を効率的にサポートするための形式的な条件を定義した。

中心定理はこうだ。「コンピュータのセンシティブ命令（sensitive instructions）の集合が、特権命令（privileged instructions）の集合の部分集合であれば、効率的な仮想マシンモニタ（VMM）を構築できる」。

直感的に言えば、「VMMの正常な動作に影響を与える可能性がある命令（センシティブ命令）がすべて、実行時にトラップを発生させる（特権命令である）ならば、trap-and-emulate方式でVMMを構築できる」ということだ。IBM System/360はこの条件を（完全にではないが概ね）満たしていた。だから、メインフレームでは仮想化が実現できた。

問題はx86アーキテクチャだった。

x86-32（IA-32）アーキテクチャには、Popek-Goldbergの要件に違反する命令が17個存在した。具体的には、SGDT（Store Global Descriptor Table Register）、SLDT（Store Local Descriptor Table Register）、SIDT（Store Interrupt Descriptor Table Register）、SMSWなどの命令は、特権状態への読み取りアクセスを提供するにもかかわらず、非特権モードで実行してもトラップを発生させない。PUSHF、POPF、IRETは割り込みフラグ（IF）にアクセスするが、やはり非特権モードではトラップしない。

```
Popek-Goldbergの仮想化要件:

理想的なアーキテクチャ（IBM System/360等）:

  センシティブ命令 ⊆ 特権命令

  ゲストOSがセンシティブ命令を実行
        ↓
  CPUがトラップを発生（特権命令のため）
        ↓
  VMMが捕捉してシミュレーション
        ↓
  ゲストOSは「普通に実行された」と認識


x86アーキテクチャの問題:

  センシティブ命令 ⊄ 特権命令
                     ↑
      17個の命令がセンシティブだが非特権

  ゲストOSがSGDT命令を実行
        ↓
  CPUはトラップを発生させない（非特権命令のため）
        ↓
  VMMは捕捉できない
        ↓
  ゲストOSがホストの状態を直接読み取ってしまう

  → 古典的なtrap-and-emulate方式では仮想化不可能
```

この問題は、x86が仮想化のために設計されたアーキテクチャではないことに起因する。x86はパーソナルコンピュータ用のプロセッサとして進化してきた。1台のマシンで1つのOSを動かすことが大前提だった。複数のOSを同時に動かすなどということは、設計時に想定されていなかった。

1990年代まで、学術的には「x86の仮想化は不可能」とされていた。メインフレームの世界では30年以上の歴史を持つ仮想化が、最も普及したプロセッサアーキテクチャでは実現できない。これが、VMware創業前夜の技術的な状況だった。

---

## 3. VMwareの解法——バイナリトランスレーションという発明

### 1998年：5人の創業者とx86仮想化への挑戦

1998年、Diane Greene、Mendel Rosenblum、Scott Devine、Ellen Wang、Edouard Bugnionの5名がVMwareを創業した。

Rosenblumはスタンフォード大学の准教授であり、オペレーティングシステムの研究者だった。UC Berkeleyで博士号を取得し、そこでGreeneと出会った。Rosenblumの研究グループは、仮想マシンの概念に商用的な可能性を見出していた。古いコンセプトを新しいプラットフォームに持ち込む——メインフレームの仮想化をx86の世界に移植する。問題は、x86がPopek-Goldbergの要件を満たさないことだった。

VMwareのアプローチは、バイナリトランスレーション（binary translation）だった。

trap-and-emulateが使えないなら、ゲストOSが実行しようとするコードを動的に書き換えてしまえばいい。問題のある17個のセンシティブ非特権命令を、実行前に検出し、VMMが制御を取り戻せるコードに置き換える。ゲストOSのバイナリコードを「翻訳」するのだ。

```
VMwareのバイナリトランスレーション:

通常の実行フロー（仮想化なし）:
  ゲストOS → [命令A] → [SGDT] → [命令B] → CPU実行

バイナリトランスレーション:
  ゲストOS → VMwareの翻訳エンジン
                    ↓
              コードブロックを検査
                    ↓
              [命令A]  → そのまま実行（安全）
              [SGDT]   → VMMの処理コードに置換
              [命令B]  → そのまま実行（安全）
                    ↓
              翻訳済みコードをキャッシュ
                    ↓
              CPU実行

  → センシティブ命令が実行される前に、
    VMMが介入できるコードに変換済み
  → ゲストOSは書き換えに気づかない
  → 翻訳結果はキャッシュされるため、
    同じコードの再翻訳は不要（性能の確保）
```

このアプローチの巧妙さは、「不可能」を「可能」に変えた点にある。Popek-Goldbergの要件を満たさないアーキテクチャでも、ソフトウェアの力でtrap-and-emulateと同等の効果を実現できる。翻訳エンジンはセンシティブ命令を含むコードブロックを検出し、翻訳結果をキャッシュする。同じコードが再度実行される場合はキャッシュから取得するため、翻訳のオーバーヘッドは繰り返しの実行で償却される。

もちろん、バイナリトランスレーションはタダではない。翻訳エンジン自体がCPU時間を消費する。だが、x86プロセッサの大半の命令はセンシティブではないため、翻訳が必要なコードブロックは全体のごく一部にすぎない。大部分の命令はネイティブ速度で実行される。VMwareの技術的な卓越さは、この「大部分はネイティブ、一部だけ翻訳」というバランスを実用的なレベルで実現した点にある。

### 1999年：VMware Workstation——Type 2ハイパーバイザの誕生

1999年5月15日、VMwareは最初の製品であるVMware Workstation 1.0をリリースした。x86アーキテクチャ上で複数のOSを同時に実行できる初の商用製品だった。

Workstationは、既存のOS（Windows、Linux）の上にインストールされるアプリケーションとして動作する。ホストOS上のアプリケーションとして仮想化レイヤーが存在し、その上でゲストOSが動く。この構造はType 2ハイパーバイザと呼ばれる。

```
Type 2ハイパーバイザ（VMware Workstation, VirtualBox）:

  ┌──────────────┐  ┌──────────────┐
  │ ゲストOS-A   │  │ ゲストOS-B   │
  │ (Linux)      │  │ (Windows)    │
  └──────────────┘  └──────────────┘
  ┌─────────────────────────────────┐
  │  VMware Workstation            │  ← 仮想化レイヤー
  │  （ホストOS上のアプリケーション）│
  └─────────────────────────────────┘
  ┌─────────────────────────────────┐
  │  ホストOS（Windows / Linux）    │  ← ホストOS
  └─────────────────────────────────┘
  ┌─────────────────────────────────┐
  │  物理ハードウェア               │
  └─────────────────────────────────┘

  特徴:
  - ホストOSが存在する（デスクトップOSの上で動く）
  - ゲストOSのI/OはホストOSのデバイスドライバを利用
  - 開発・テスト用途に適する
  - ホストOSのオーバーヘッドが存在する
```

Workstationの主な用途は開発とテストだった。前述した私の体験——LinuxとWindowsを同時に動かして開発とブラウザテストを並行する——がまさにその典型的なユースケースだ。OSを切り替えるために再起動する必要がなくなった。複数のOSの環境を仮想マシンとして保存し、必要に応じて起動・停止できる。スナップショットを取れば、状態を保存し、いつでもその時点に戻れる。

Workstationは、仮想化の価値をx86の世界に証明した最初の製品だった。だが、VMwareの真の革命はここからだった。

### 2001年：ESX Server——Type 1ハイパーバイザ、ベアメタルの征服

2001年3月、VMwareはESX Server 1.0をリリースした。Workstationとは根本的に異なる設計のサーバ製品だ。

ESX Serverは、ホストOSを必要としない。物理ハードウェアの上に直接インストールされ、ハードウェアを直接制御する。この構造はType 1ハイパーバイザ（ベアメタルハイパーバイザ）と呼ばれる。

```
Type 1ハイパーバイザ（VMware ESX/ESXi）:

  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐
  │ VM-A         │  │ VM-B         │  │ VM-C         │
  │ (Web Server) │  │ (DB Server)  │  │ (App Server) │
  │ CentOS       │  │ RHEL         │  │ Ubuntu       │
  └──────────────┘  └──────────────┘  └──────────────┘
  ┌─────────────────────────────────────────────────────┐
  │  VMware ESX（ハイパーバイザ）                       │  ← ベアメタル上に
  │  ── ハードウェアを直接制御                          │     直接インストール
  │  ── VMカーネル（vmkernel）がリソースを管理          │
  └─────────────────────────────────────────────────────┘
  ┌─────────────────────────────────────────────────────┐
  │  物理ハードウェア                                   │
  └─────────────────────────────────────────────────────┘

  特徴:
  - ホストOSが存在しない（ハイパーバイザが直接ハードウェアを制御）
  - デバイスドライバはハイパーバイザが提供
  - 本番環境のサーバ統合に適する
  - オーバーヘッドが最小限
```

Type 2とType 1の差は、ホストOSの有無だ。Type 2ではゲストOSのI/O操作がホストOSのデバイスドライバを経由するため、オーバーヘッドが発生する。Type 1ではハイパーバイザがハードウェアを直接制御するため、オーバーヘッドが小さい。本番環境のサーバでは、このオーバーヘッドの差がスループットとレイテンシーに直接影響する。

ESX Serverは、仮想化を「開発者の便利ツール」から「データセンターのインフラ基盤」に昇格させた。企業はESX Serverの導入によってサーバ統合を実現し、ハードウェアコスト、電力コスト、運用コストを削減した。仮想化以前、平均的なx86サーバの利用率は10-15%程度だった。ESXの導入により、1台の物理サーバに10台以上の仮想マシンを集約し、利用率を60-80%まで引き上げることが可能になった。

数字を具体的に示そう。10台の物理サーバを2台に統合したとする。ハードウェアの購入費用が5分の1になる。8台分のラックスペースが空く。8台分の電力と冷却が不要になる。8台分のハードウェア障害の確率がなくなる。8台分のOSパッチ適用、ファームウェアアップデート、物理メンテナンスが消える。サーバ統合のROIは明確であり、仮想化はビジネスケースとして経営陣を説得できる技術だった。

### 2003年：vMotion——「止めずに引っ越す」という魔法

2003年、VMwareはVirtualCenter 1.0とともにvMotionをリリースした。vMotionは、稼働中の仮想マシンを別の物理ホストにダウンタイムなしで移動する技術だ。

これが何を意味するか、考えてほしい。

仮想化以前、サーバのメンテナンス——ハードウェアの交換、ファームウェアのアップデート、OSのパッチ適用——は、サービスの停止を伴った。「メンテナンス窓」と呼ばれる計画停止時間を設定し、ユーザーに事前告知し、深夜にデータセンターに行ってメンテナンスを行う。私は何度もこの作業を経験した。深夜3時のデータセンター、冷たいサーバルームの中で、「メンテナンス完了、サービス再開」の報告メールを送る瞬間の安堵感は今でも忘れない。

vMotionは、この「計画停止」の概念を根本から変えた。物理ホストAで動いている仮想マシンを、物理ホストBに移動する。移動の間、仮想マシンは動き続ける。ユーザーは何も気づかない。物理ホストAの保守が必要なら、そこで動いている仮想マシンをすべて物理ホストBに移動し、物理ホストAを空にしてからメンテナンスすればいい。サービスは一秒も止まらない。

```
vMotionの動作概念:

                 ネットワーク（vMotion用）
                 ┌──────────────────────┐
                 │                      │
  物理ホストA    │                      │   物理ホストB
  ┌──────────┐  │                      │  ┌──────────┐
  │ ESX      │  │   メモリの内容を     │  │ ESX      │
  │          │  │   逐次転送           │  │          │
  │ ┌──────┐ │  │  ─────────────→      │  │ ┌──────┐ │
  │ │ VM   │ │  │                      │  │ │ VM   │ │
  │ │(稼働 │ │  │   最後に残った       │  │ │(移動 │ │
  │ │ 中)  │ │  │   差分を転送し       │  │ │ 先)  │ │
  │ └──────┘ │  │   切り替え           │  │ └──────┘ │
  └──────────┘  │  ─────────────→      │  └──────────┘
                │                      │
                └──────────────────────┘

  1. メモリの内容をホストBに転送開始
  2. 転送中も仮想マシンは稼働継続（メモリ変更は追跡）
  3. 未転送のメモリが十分小さくなったら、
     仮想マシンを一瞬停止し、残りの差分を転送
  4. ホストBで仮想マシンを再開
  5. 停止時間は通常ミリ秒〜数秒
```

vMotionの初期のデモンストレーションでは、Windows上で3D Pinballを動かしたまま仮想マシンを別の物理ホストに移動し、ゲームのプレイに影響がないことを示したという。技術的な優雅さを端的に表現した逸話だ。

vMotionの技術的な意味は深い。仮想マシンが物理ホストから「解放」されたのだ。仮想化以前、サービスと物理サーバは一対一で結びついていた。「このサービスはサーバAで動いている」——この事実は、サーバAが物理的に存在し続ける限り不変だった。vMotionは、サービスを物理的な「場所」から分離した。サービスはどの物理ホストでも動けるようになった。

この「場所からの解放」は、クラウドコンピューティングの根幹をなす概念だ。EC2インスタンスが特定の物理サーバに固定されていたら、AWSはAvailability Zoneの冗長化も、ホストの保守透過性も実現できない。vMotionが切り拓いた「仮想マシンは物理ホスト間を自由に移動できる」という概念は、クラウドのアーキテクチャの前提条件となった。

---

## 3. 仮想化の3つのアプローチ——完全仮想化、準仮想化、ハードウェア支援

VMwareのバイナリトランスレーションは、x86仮想化の最初のアプローチだった。だが、仮想化の方法はこれだけではない。x86の仮想化には3つの主要なアプローチが存在し、それぞれ異なるトレードオフを持つ。

### 完全仮想化（Full Virtualization）

VMwareが採用した方式だ。ゲストOSに一切の修正を加えず、そのまま仮想マシン上で動作させる。バイナリトランスレーションによって問題のある命令を動的に書き換え、ゲストOSは「自分が仮想化されている」ことを知らない。

利点は明確だ。既存のOS——Windows、Linux、FreeBSD——を修正せずにそのまま動かせる。OSのソースコードにアクセスする必要がない。WindowsのようなプロプライエタリなOSも仮想化できる。

欠点は、バイナリトランスレーションのオーバーヘッドだ。センシティブ命令を含むコードブロックの翻訳にはCPU時間がかかる。特にI/O操作が頻繁なワークロードでは、このオーバーヘッドが顕著になる。

### 準仮想化（Paravirtualization）

2003年にケンブリッジ大学のIan PrattとKeir Fraserが発表したXenは、異なるアプローチを取った。バイナリトランスレーションでゲストOSを「騙す」のではなく、ゲストOSのカーネルを修正して、ハイパーバイザと「協調」させる。ゲストOSは自分が仮想化されていることを知っており、ハイパーバイザが提供するAPI（ハイパーコール）を通じてハードウェアにアクセスする。

利点は性能だ。バイナリトランスレーションのオーバーヘッドがなく、ハイパーコールによるI/Oは効率的だった。

欠点は、ゲストOSのカーネル修正が必要なことだ。Linuxのようなオープンソースのカーネルは修正できるが、Windowsのカーネルは修正できない。この制約は、準仮想化の採用を限定した。

### ハードウェア支援仮想化（Hardware-Assisted Virtualization）

2005年にIntelがVT-x（Vanderpool Technology）を、2006年にAMDがAMD-V（Pacifica）をそれぞれ導入した。x86プロセッサ自体に仮想化のためのハードウェアサポートを組み込んだのだ。

新しいCPUモードとして「VMX root」（ホスト用）と「VMX non-root」（ゲスト用）が追加された。センシティブ命令はVMX non-rootモードで実行されると自動的にVMExitを発生させ、ハイパーバイザに制御が戻る。Popek-Goldbergの要件が、ハードウェアレベルで満たされるようになったのだ。

```
3つの仮想化アプローチの比較:

                     完全仮想化        準仮想化         HW支援仮想化
                     (VMware)         (Xen)           (VT-x/AMD-V)
─────────────────────────────────────────────────────────────────
ゲストOS修正          不要             必要             不要
バイナリ
トランスレーション    必要             不要             不要
HWサポート           不要             不要             必要
Windows対応          可能             不可(注1)        可能
I/O性能              中               高               中〜高(注2)
CPU性能              高(大部分は      高               高
                     ネイティブ実行)
登場時期             1999年           2003年           2005/2006年

注1: XenはHVM（Hardware Virtual Machine）モードで
     VT-x/AMD-Vを利用し、未修正ゲストOSもサポート
注2: virtio等の準仮想化I/Oドライバとの併用で
     I/O性能を大幅に改善可能
```

興味深いのは、初期のハードウェア支援仮想化は、十分にチューニングされたバイナリトランスレーションよりも遅いケースがあったことだ。VMExitとVMEntryの切り替えコストが大きく、頻繁にトラップが発生するワークロードでは、バイナリトランスレーションのキャッシュ効率の方が勝った。後続のプロセッサ世代でVMExit/VMEntryのレイテンシーが改善され、Extended Page Tables（EPT、Intel）やNested Page Tables（NPT、AMD）のような機能が追加されることで、ハードウェア支援仮想化が性能面でも優位に立つようになった。

この進化の過程は、技術選択におけるトレードオフの典型だ。「ハードウェアが解決すべき問題」と「ソフトウェアが解決すべき問題」の境界は、時間とともに移動する。VMwareはソフトウェアの力でx86仮想化を実現した。IntelとAMDはハードウェアの力でそれを最適化した。最終的には両者の協調——ハードウェア支援とソフトウェアの最適化の組み合わせ——が最高の性能を実現する。

---

## 4. ハンズオン——QEMUとKVMで仮想化のオーバーヘッドを測定する

ここからは、QEMUとKVMを使って仮想マシンを構築し、仮想化のオーバーヘッドを実測する。「仮想化はタダではない」——この事実を数値で確認する。

### 環境

- Docker（Ubuntu 24.04ベースコンテナ、`--privileged`オプションとKVMデバイスアクセスが必要）
- ホストマシンがKVM（Intel VT-x/AMD-V）をサポートしていること

### 演習1：QEMUによるCPU仮想化——エミュレーションのオーバーヘッド

まずはQEMUをソフトウェアエミュレーション（TCG: Tiny Code Generator）モードで使い、仮想化の「コスト」を可視化する。

```bash
# Docker環境に入る（KVMデバイスへのアクセスが必要）
docker run -it --rm --privileged \
  --device /dev/kvm:/dev/kvm \
  --name vmware-handson ubuntu:24.04 bash

# 必要なツールをインストール
apt-get update && apt-get install -y qemu-system-x86 qemu-utils \
  cpu-checker stress-ng bc procps linux-tools-common util-linux
```

```bash
echo "=== 演習1: 仮想化オーバーヘッドの可視化 ==="
echo ""

# KVMが利用可能か確認
echo "--- KVMサポートの確認 ---"
if [ -e /dev/kvm ]; then
  echo "/dev/kvm が存在: ハードウェア仮想化が利用可能"
else
  echo "/dev/kvm が存在しない: ソフトウェアエミュレーションのみ"
fi
echo ""

# CPUが仮想化をサポートしているか確認
echo "--- CPU仮想化フラグの確認 ---"
if grep -q -E '(vmx|svm)' /proc/cpuinfo; then
  echo "CPU仮想化フラグ検出:"
  grep -m1 -oE '(vmx|svm)' /proc/cpuinfo
  echo "  vmx = Intel VT-x, svm = AMD-V"
else
  echo "CPU仮想化フラグが見つからない"
fi
echo ""

# ホスト環境での基準性能測定
echo "--- ホスト環境（ベアメタル相当）でのCPU性能 ---"
echo "円周率計算（bc、10000桁）を3回実行"
echo ""

TOTAL_HOST=0
for i in 1 2 3; do
  TIME_HOST=$( { time echo "scale=10000; 4*a(1)" | bc -l > /dev/null; } 2>&1 \
    | grep real | awk '{print $2}')
  echo "  実行${i}: ${TIME_HOST}"
  # 秒数を抽出（m*60+s）
  MINS=$(echo "$TIME_HOST" | sed 's/m.*//')
  SECS=$(echo "$TIME_HOST" | sed 's/.*m//' | sed 's/s//')
  TOTAL_SEC=$(echo "$MINS * 60 + $SECS" | bc)
  TOTAL_HOST=$(echo "$TOTAL_HOST + $TOTAL_SEC" | bc)
done
AVG_HOST=$(echo "scale=3; $TOTAL_HOST / 3" | bc)
echo ""
echo "ホスト平均: ${AVG_HOST}秒"
echo ""

echo "=== 演習1完了 ==="
echo ""
echo "この測定値が「ベースライン」となる。"
echo "仮想マシン内で同じ計算を実行すると、仮想化の"
echo "オーバーヘッド分だけ遅くなる。"
echo ""
echo "VMwareが解決した問題の本質:"
echo "  このオーバーヘッドを「実用的に無視できるレベル」まで"
echo "  抑えること。バイナリトランスレーション（VMware）や"
echo "  ハードウェア支援（VT-x/AMD-V）がそれを実現した。"
```

### 演習2：KVM仮想マシンのライフサイクル管理

仮想マシンの作成・起動・停止という基本的なライフサイクルを体験する。これはvMotion以前の、最も基本的な仮想マシン管理だ。

```bash
echo "=== 演習2: 仮想マシンのライフサイクル ==="
echo ""

# 最小限のLinuxディスクイメージを作成
echo "--- 仮想マシン用ディスクイメージの作成 ---"
qemu-img create -f qcow2 /tmp/test-vm.qcow2 1G
echo ""
echo "フォーマット: qcow2（QEMU Copy-On-Write 2）"
echo "  - 実際に書き込まれたデータ分だけディスク容量を消費"
echo "  - スナップショット機能をサポート"
echo "  - VMwareのvmdk、Hyper-Vのvhdxに相当するフォーマット"
echo ""

# ディスクイメージの情報表示
qemu-img info /tmp/test-vm.qcow2
echo ""

echo "--- 仮想マシンのスナップショット機能 ---"
echo "スナップショットは仮想化の重要な機能の一つ"
echo "VMwareはスナップショットによって「状態の保存と復元」を実現した"
echo ""

# 追加のディスクイメージでスナップショットを実演
qemu-img create -f qcow2 /tmp/snapshot-demo.qcow2 512M

echo "初期状態のイメージサイズ:"
ls -lh /tmp/snapshot-demo.qcow2 | awk '{print "  " $5}'

# スナップショットの作成（ディスクのみ、VMが動いていない状態）
qemu-img snapshot -c "before-install" /tmp/snapshot-demo.qcow2
echo ""
echo "スナップショット 'before-install' を作成"

qemu-img snapshot -c "after-config" /tmp/snapshot-demo.qcow2
echo "スナップショット 'after-config' を作成"
echo ""

echo "スナップショット一覧:"
qemu-img snapshot -l /tmp/snapshot-demo.qcow2
echo ""

echo "スナップショットの意義:"
echo "  - 設定変更前に状態を保存 → 失敗したら即座に復元"
echo "  - テスト環境を「クリーン状態」から何度でも再開"
echo "  - VMwareがこの機能を商用化し、運用の安全性を飛躍的に向上させた"
echo ""
echo "=== 演習2完了 ==="
```

### 演習3：vCPUとpCPUのマッピング——仮想化の核心

仮想CPUと物理CPUの関係を観察する。これは仮想化の最も核心的な部分だ。

```bash
echo "=== 演習3: vCPUとpCPU（物理CPU）のマッピング ==="
echo ""

# 物理CPUの情報
echo "--- 物理CPU情報 ---"
PCPU_COUNT=$(nproc)
echo "物理（ホスト）CPU数: ${PCPU_COUNT}"
echo ""

echo "CPU情報（抜粋）:"
lscpu | grep -E '(Model name|CPU\(s\)|Thread|Core|Socket|MHz|Virtualization)'
echo ""

echo "--- 仮想化におけるCPUの割り当て ---"
echo ""
echo "物理サーバのCPU: ${PCPU_COUNT}コア"
echo ""
echo "VMwareの仮想化では、以下のように物理CPUを分割する:"
echo ""
echo "  物理CPU ${PCPU_COUNT}コア"
echo "  ├── VM-A: vCPU 2コア"
echo "  ├── VM-B: vCPU 2コア"
echo "  ├── VM-C: vCPU 1コア"
echo "  └── ハイパーバイザ自身が使用"
echo ""
echo "オーバーコミット（物理CPUより多くのvCPUを割り当てる）:"
echo "  物理CPU ${PCPU_COUNT}コア に対して"
echo "  合計 $((PCPU_COUNT * 3))vCPU を割り当てることも可能"
echo "  → 全VMが同時に100%使用しなければ成立する"
echo "  → VMwareのスケジューラが動的にタイムスライスを配分"
echo ""

# CPU使用率の時分割を実演
echo "--- CPUタイムスライスの実演 ---"
echo "2つのプロセスが1コアのCPUを時分割で共有する様子:"
echo "（仮想化のCPUスケジューリングの簡易的な再現）"
echo ""

# バックグラウンドでCPU負荷を生成
taskset -c 0 stress-ng --cpu 1 --timeout 5s &
PID_A=$!
taskset -c 0 stress-ng --cpu 1 --timeout 5s &
PID_B=$!

echo "プロセスA (PID: $PID_A) と プロセスB (PID: $PID_B)"
echo "を同じCPUコア(0)にバインド"
echo ""

sleep 2
echo "CPU 0の使用状況（2秒後）:"
echo "  両プロセスがCPU時間を約50%ずつ分け合っている"
echo "  → これが仮想化におけるvCPUスケジューリングの本質"
echo ""

wait 2>/dev/null

echo "VMwareのCPUスケジューラは、この時分割を仮想マシン単位で行う。"
echo "各VMには「CPUの重み」が設定され、重みに応じて"
echo "タイムスライスが配分される。"
echo ""
echo "メインフレームのタイムシェアリング（第2回）から"
echo "VMwareのCPUスケジューラまで、「CPUを分け合う」"
echo "という原理は60年間変わっていない。"
echo ""
echo "=== 演習3完了 ==="
```

### この演習で何がわかるか

**第一に、仮想化にはコストがある。** ソフトウェアエミュレーション（QEMUのTCGモード）では、ネイティブ実行と比較して大きなオーバーヘッドが発生する。VMwareのバイナリトランスレーションは、このオーバーヘッドを大幅に削減した。さらにIntel VT-x/AMD-Vのハードウェア支援により、CPU仮想化のオーバーヘッドは数パーセント以下にまで抑えられるようになった。

**第二に、スナップショットは仮想化がもたらした運用革命だ。** 物理サーバでは「変更前の状態に戻す」ことは極めて困難だった。仮想マシンのスナップショットは、状態の保存と復元を瞬時に行える。この機能は、テスト環境の構築、本番環境のメンテナンス、障害からの復旧に革命をもたらした。

**第三に、vCPUの概念が仮想化の核心だ。** 1台の物理CPUの処理能力を複数の仮想マシンが時分割で共有する。これはメインフレームのタイムシェアリングと同じ原理だ。第2回で扱った1960年代のCTSSから、2000年代のVMwareまで、「CPUを分け合う」という基本概念は変わっていない。変わったのは抽象化のレイヤーと精度だ。

ハンズオンの詳細な手順と自動セットアップスクリプトは、本リポジトリの `handson/cloud-history/06-vmware-revolution/` に用意してある。

---

## 5. まとめと次回予告

### この回のまとめ

第6回では、仮想化技術がどのように生まれ、VMwareがx86の世界にそれを持ち込み、クラウドの基盤を築いたかを追った。

**仮想化はメインフレームで生まれた。** 1967年のIBM CP-40/CMS、1972年のVM/370——計算資源を仮想的に分割し、複数のOSを同時に動かす技術は、メインフレームの世界では半世紀以上の歴史を持つ。しかし、x86アーキテクチャには17個のセンシティブ非特権命令が存在し、Popek-Goldberg（1974年）の仮想化要件を満たさなかった。1990年代まで、x86の仮想化は「不可能」とされていた。

**VMwareはバイナリトランスレーションでその不可能を可能にした。** 1998年の創業、1999年のWorkstationリリース。ゲストOSのコードを動的に書き換え、センシティブ命令をVMMが捕捉可能なコードに変換する。この独創的なアプローチが、x86仮想化の扉を開いた。

**ESX ServerとvMotionがデータセンターを変えた。** 2001年のESX Server（Type 1ハイパーバイザ）は、仮想化を本番環境のインフラ基盤に昇格させた。サーバ利用率を10-15%から60-80%に引き上げ、ハードウェアコスト、電力コスト、運用コストを劇的に削減した。2003年のvMotionは、仮想マシンを物理ホストから解放し、「サービスは場所に縛られない」という概念を実現した。

**ハードウェアもソフトウェアに応えた。** 2005年のIntel VT-x、2006年のAMD-Vにより、x86プロセッサがハードウェアレベルで仮想化をサポートするようになった。ソフトウェアとハードウェアの協調進化が、仮想化の性能を極限まで高めた。

冒頭の問いに答えよう。「仮想化技術は何を可能にし、なぜクラウドの基盤技術となったのか？」——仮想化は、計算資源を物理的なハードウェアから切り離した。サーバは「固定スペックの鉄の箱」ではなく、「ソフトウェアで定義された論理的な存在」になった。この抽象化なくして、「APIを叩けば数分でサーバが立ち上がる」というクラウドの世界は存在しえない。仮想化はクラウドの必要条件であり、VMwareはその必要条件を商用的に実現した最初の企業だった。

### 次回予告

第7回では、「Xen、KVM——オープンソース仮想化が切り拓いた道」を探る。

VMwareは仮想化の商用化に成功した。だが、VMwareのライセンスは安価ではなかった。企業がデータセンター全体を仮想化するには、相応のコストがかかった。オープンソースの世界は、この独占に異議を唱えた。

2003年にケンブリッジ大学からXenが登場し、準仮想化というアプローチで注目を集めた。2007年にはKVM（Kernel-based Virtual Machine）がLinuxカーネルに統合され、「Linuxカーネルそのものがハイパーバイザになる」という世界が実現した。AWSのEC2は初期にXenベースで構築され、後にKVMベースのNitroハイパーバイザに移行している。オープンソース仮想化がクラウドの「民主化」をどう推し進めたのか、その技術的な系譜を辿る。

VMwareが証明した「x86仮想化は可能である」という事実は、XenとKVMによって「x86仮想化は誰もが利用可能である」という段階に進む。仮想化の民主化——それがクラウドへの直接の道筋となった。

---

## 参考文献

- IBM, "z/VM History: Timeline". <https://www.vm.ibm.com/history/timeline.html>
- Wikipedia, "IBM CP-40". <https://en.wikipedia.org/wiki/IBM_CP-40>
- Wikipedia, "VM (operating system)". <https://en.wikipedia.org/wiki/VM_(operating_system)>
- Popek, G.J. and Goldberg, R.P., "Formal Requirements for Virtualizable Third Generation Architectures", Communications of the ACM, Vol.17, No.7, 1974.
- Wikipedia, "Popek and Goldberg virtualization requirements". <https://en.wikipedia.org/wiki/Popek_and_Goldberg_virtualization_requirements>
- Wikipedia, "VMware". <https://en.wikipedia.org/wiki/VMware>
- Wikipedia, "Diane Greene". <https://en.wikipedia.org/wiki/Diane_Greene>
- Wikipedia, "VMware Workstation". <https://en.wikipedia.org/wiki/VMware_Workstation>
- Bugnion, E. et al., "Bringing Virtualization to the x86 Architecture with the Original VMware Workstation", ACM Transactions on Computer Systems, 2012. <https://dl.acm.org/doi/10.1145/2382553.2382554>
- virtualg.uk, "The History of VMware ESXi (2001 to 2025)". <https://virtualg.uk/the-history-of-vmware-esxi-2001-to-2025/>
- Wikipedia, "VMware ESX". <https://en.wikipedia.org/wiki/VMware_ESX>
- VMware Cloud Foundation Blog, "The vMotion Process Under the Hood". <https://blogs.vmware.com/cloud-foundation/2019/07/09/the-vmotion-process-under-the-hood/>
- Virtualization Review, "The Evolution of VMware's vMotion". <https://virtualizationreview.com/articles/2016/09/14/evolution-of-vmware-vmotion.aspx>
- Wikipedia, "x86 virtualization". <https://en.wikipedia.org/wiki/X86_virtualization>
