# クラウドの考古学

## ——メインフレームからサーバーレスへ、計算資源の民主化史

### 第2回：メインフレームとタイムシェアリング——計算資源を共有した最初の時代

**連載「クラウドの考古学——メインフレームからサーバーレスへ、計算資源の民主化史」**
**著：佐藤裕介（Engineers Hub株式会社 CEO / Technical Lead）**

---

**この回で学べること：**

- メインフレーム時代の「バッチ処理」がなぜ限界を迎えたのか
- タイムシェアリングの発明が計算機の使い方をどう変えたか
- CTSS、Multics、CP-67/CMSの設計思想とクラウドへの接続
- Linuxのプロセススケジューリングを通じてタイムシェアリングの原理を体感する方法

---

## 1. 大学の計算機センターで古いマニュアルを開いたとき

1990年代の後半、私は大学の計算機センターの片隅で、古いメインフレームのマニュアルを手に取ったことがある。

正確に言えば、マニュアルというよりはオペレータ向けの手順書だった。黄ばんだバインダーの中に、JCL（Job Control Language）のサンプルコードが印刷されていた。`//JOBNAME JOB (ACCT),'PROGRAMMER NAME'` から始まるあの独特の構文。`//` で始まる制御文。`DD` 文によるデータセット定義。今の私はシェルスクリプトやTerraformの定義ファイルを日常的に書いているが、JCLはそれらとはまったく異なる設計思想の上にあった。

JCLは人間のためではなく、計算機のために設計された言語だった。1964年にSystem/360が発表されたとき、低端モデルの360/30のCPU処理速度は毎秒1,800〜34,500命令にすぎなかった。計算機の時間は人間の時間より遥かに高価だった。だから、計算機が効率的にジョブを処理できるよう、人間が計算機にとって読みやすい形式で指示を書いた。人間の利便性は二の次だった。

だが、私がマニュアルをめくっていて本当に驚いたのは、JCLの構文ではない。ジョブカードの実物が挟まっていたことだ。80列のパンチカードに長方形の穴が穿たれている。このカードの束が、プログラムだった。プログラマはカードパンチ機の前に座り、一行ずつコードを穿孔し、カードの束をオペレータに渡す。オペレータがカードリーダーにかけ、メインフレームがバッチ処理でジョブを実行する。結果は印刷されて出力される。結果が返ってくるまで数時間、場合によっては翌日。

翌日。

今、`aws ec2 run-instances` と入力すれば数分でサーバが立ち上がる時代に、結果が返ってくるのが「翌日」だった時代がある。この体感は、文字で読んだだけでは伝わらない。だが、あのジョブカードを手に取ったとき、私は確かに「計算機を待つ」という体験の重みを想像できた。

そして、あのジョブカードの束を見つめながら、ある事実に気づいた。バッチ処理からタイムシェアリングへの転換——計算機を「待つ」ものから「対話する」ものへと変えたあの革命——は、60年後の今、クラウドコンピューティングの設計思想にそのまま貫かれている。

「複数のユーザーが一つの計算機を共有する」という発想は、どこから来たのか。なぜそれが必要だったのか。そしてその設計思想は、今日の `aws ec2 run-instances` の裏側にどう接続しているのか。

今回は、この問いを掘り下げる。

---

## 2. 「計算機を待つ」時代——バッチ処理の世界

タイムシェアリングの意味を理解するためには、まずその前提となった「バッチ処理」の世界を知る必要がある。

### パンチカードとバッチ処理——計算機が「聖域」だった時代

1950年代から1960年代前半にかけて、計算機は極めて高価な装置だった。IBMが1959年に発表したIBM 1401は、当時としてはコストパフォーマンスに優れた商用コンピュータだったが、それでも月額レンタル料は数千ドルに及んだ。1960年代中盤までに10,000台以上が導入されたIBM 1401は、パンチカード時代のベストセラーだった。より高性能なIBM 7094——CTSSが動作することになるマシンだ——は1台約350万ドル。1960年代の350万ドルは、現在の価値に換算すれば数十億円に相当する。

計算機がそれほど高価であれば、1秒たりとも遊ばせるわけにはいかない。だから、バッチ処理が合理的だった。

バッチ処理の流れはこうだ。

1. プログラマはカードパンチ機でプログラムとデータをパンチカードに穿孔する
2. カードの束をオペレータに提出する
3. オペレータがジョブの優先度に応じてスケジューリングし、カードリーダーに投入する
4. 計算機がジョブを逐次実行する
5. 結果が紙テープやラインプリンタで出力される
6. プログラマは出力を受け取り、バグがあればカードを修正して再提出する

このサイクルのターンアラウンドタイム——ジョブ投入から結果受領まで——は数時間から一日が当たり前だった。プログラムに1行のタイプミスがあっただけで、修正して再投入し、また数時間待つことになる。

計算機は効率的に動いていた。だが、プログラマは効率的ではなかった。

### 人間の時間 vs 計算機の時間

ここに根本的な緊張がある。バッチ処理は計算機の利用効率を最大化する仕組みだ。CPUが遊ぶ時間を最小化し、ジョブを途切れなく処理する。だが、それは人間の待ち時間を犠牲にしている。

1960年代の初め、この構造に疑問を呈した人々がいた。計算機の価格は下がり続けている。一方、プログラマの給料は上がり続けている。計算機の効率を多少犠牲にしてでも、人間の生産性を上げた方が総合的には合理的ではないか——と。

この問いへの答えが、タイムシェアリングだった。

---

## 3. タイムシェアリングの夜明け——「対話する計算機」の誕生

### 1961年、MITの実験

1961年の春、MITのComputation Center副所長だったFernando Corbatoは、一つの実験を始めた。IBM 709（後にIBM 7094にアップグレード）の上で、複数のユーザーが同時に計算機を利用できるシステムを構築する試みだ。

Corbatoのアイデアは単純にして革命的だった。CPUの処理時間を細かいスライスに分割し、各ユーザーに順番に割り当てる。各スライスは非常に短いため、ユーザーからは「自分専用の計算機を使っている」ように感じられる。実際には一台の計算機を複数人で共有しているのだが、人間の反応速度は計算機の処理速度に比べて圧倒的に遅いため、切り替えに気づかない。

1961年7月、IBM 709上で最初のタイムシェアリングコマンドが動作した。同年11月、CorbatoはMITでCTSS（Compatible Time-Sharing System）の公開デモンストレーションを行った。これは、汎用タイムシェアリングシステムの世界初の公開デモンストレーションとされている。

「Compatible」という名前には意味がある。CTSSはIBM 7094の既存のバッチ処理システムと「互換性」を保ちながらタイムシェアリングを実現した。計算機の時間を完全にタイムシェアリングに奪うのではなく、バッチ処理との共存を図ったのだ。この現実的な設計判断が、CTSSの普及を可能にした。

CTSSの定常的なサービス提供は1963年夏に始まり、1968年まで運用された。MIT Computation Centerの利用者は、テレタイプ端末からIBM 7094に接続し、コマンドを入力し、即座に結果を得ることができた。バッチ処理のターンアラウンドタイムが数時間だった世界で、応答が秒単位で返ってくる。この体験の衝撃は、想像以上に大きかった。

### CTSSがもたらした概念——パスワード、ファイルシステム、メールシステム

CTSSが画期的だったのは、タイムシェアリングそのものだけではない。複数のユーザーが一つの計算機を共有するという前提が、数々の新しい概念を必然的に生み出した。

**パスワード認証。** 複数のユーザーが一台の計算機を共有するならば、各ユーザーのデータを他のユーザーから保護する必要がある。CTSSはコンピュータにパスワード認証を導入した最初のシステムとされている。ただし、パスワードは平文で保存されていた。1966年春、Allan Scherrというユーザーが全ユーザーのパスワードリストを入手することに成功した——これは最初のコンピュータパスワードハッキング事例としても知られている。セキュリティの歴史は、共有の歴史と表裏一体だ。

**ファイルシステム。** 各ユーザーのファイルを管理するディレクトリ構造が必要になった。CTSSのファイルシステムは2階層だったが、これが後のMulticsで階層型ファイルシステムに発展し、Unixのファイルシステムの原型となった。

**メールシステム。** 複数のユーザーが同じ計算機上にいるならば、ユーザー間でメッセージを送り合うことができる。CTSSでは1965年にMAILコマンドが実装された。電子メールの原型である。

気づいてほしい。これらの概念——認証、ファイルシステム、メッセージング——は、現代のクラウドサービスの基本構成要素そのものだ。AWS IAMの認証・認可、S3のオブジェクトストレージ、SQSのメッセージキュー。根源を辿れば、1960年代のタイムシェアリングシステムが「複数のユーザーが一つの計算機を共有する」ために必要としたものに行き着く。

### Corbatoの慧眼とチューリング賞

Corbatoは1990年にACMチューリング賞を受賞した。授賞理由は「汎用・大規模なタイムシェアリングおよびリソースシェアリング計算機システムCTSSとMulticsの概念を組織し、開発をリードした先駆的業績」だった。

Corbatoが見抜いていたのは、計算機の利用モデルの本質的な転換だ。バッチ処理は「計算機中心」のモデルだった。人間が計算機のスケジュールに合わせる。タイムシェアリングは「人間中心」のモデルへの転換だった。計算機が人間のペースに合わせる。

この「人間中心」への転換は、60年後のクラウドでも繰り返されている。オンプレミスのサーバ調達は「インフラ中心」だ。ハードウェアの調達リードタイムに人間が合わせる。クラウドは「開発者中心」への転換だ。インフラが開発者のペースに合わせる。`aws ec2 run-instances` と入力すれば、数分でサーバが手に入る。

同じ構造の転換が、異なる時代に繰り返されている。

---

## 4. Project MAC、Multics、そして「計算ユーティリティ」の夢

### Project MAC——タイムシェアリング研究の一大拠点

CTSSの成功を受けて、MITは1963年7月にProject MACを発足させた。ARPA（Advanced Research Projects Agency、後のDARPA）の資金援助を受けたこのプロジェクトは、タイムシェアリング研究の一大拠点となった。初代ディレクターはRobert M. Fano。Corbatoも創設メンバーの一人だった。

MACという名前の由来は意図的に曖昧にされていた。「Machine-Aided Cognition」とも「Multiple Access Computer」とも解釈できた。どちらの解釈も正しかった。

Project MACへのARPAの支出は1963年から1970年の期間で約2,500万ドルにのぼった。当時の2,500万ドルは途方もない金額だ。だがこの投資は、その後のコンピュータサイエンスの発展を考えれば、破格のリターンを生んだ。

### Multics——野心的すぎた「計算ユーティリティ」

Project MACの最も野心的な成果が、Multics（Multiplexed Information and Computing Service）だった。

1964年8月、MIT Project MAC、Bell Telephone Laboratories、General Electricの3者間で開発契約が締結された。Bell Labsは同年11月からソフトウェア開発に参加し、約50人のプログラマがMIT、GE、Bell Labsの3拠点で開発に従事した。

Multicsの目標は、CTSSの概念をはるかに超えるものだった。単なるタイムシェアリングシステムではなく、「計算ユーティリティ」——電力や水道のように、いつでも必要なだけ利用できる計算サービス——を実現することが目標だった。John McCarthyが1961年のMIT100周年記念講演で「コンピューティングは公共ユーティリティになる」と予言した、あのビジョンの具現化だ。

Multicsは数々の革新的な技術を導入した。

**階層型ファイルシステム。** CTSSの2階層ディレクトリを拡張し、ツリー構造のファイルシステムを実現した。`/usr/bin/program` のようなパスの概念は、ここで生まれた。

**メモリ保護リング。** Multicsは「保護リング」の概念を導入した。特権レベルの異なるリングを設け、カーネルは最も内側の特権リングで動作し、ユーザープログラムは外側のリングで動作する。あるリングのコードは、自分より内側のリングのデータに直接アクセスできない。この設計思想は、現代のx86プロセッサのRing 0〜Ring 3にそのまま受け継がれている。GE-645のハードウェアではリング遷移をソフトウェアでトラップする必要があったが、後継のHoneywell 6180では8つのリングをハードウェアでサポートした。

**動的リンク。** プログラムの実行時にライブラリを動的に結合する仕組み。現代の共有ライブラリ（`.so`、`.dll`）の概念の先駆けだ。

**アクセス制御リスト（ACL）。** ファイルやリソースに対して、ユーザーごとにアクセス権限を細かく設定できる仕組み。AWS IAMのポリシーが実現していることの原型がここにある。

だが、Multicsの開発は困難を極めた。最初のGE-645ハードウェアがMITに納入されたのは1967年1月。プロジェクト構想から3年近くが経過していた。ソフトウェアの完成度は予定を大幅に遅れ、Bell Labsは1969年にプロジェクトから撤退した。

### Bell Labsの撤退、そしてUnixの誕生

Bell Labsの撤退は、コンピュータ史にとって逆説的な幸運だった。

Multicsプロジェクトに参加していたBell Labsのエンジニアたち——Ken ThompsonとDennis Ritchieが筆頭だ——は、Multicsの経験から多くを学んでいた。だが同時に、Multicsの「あまりに野心的な」アプローチに限界も感じていた。

彼らはより小さく、よりシンプルなシステムを作ることにした。最初はPDP-7の上で、後にPDP-11で。Multicsの「Multiplexed」に対して「Uniplexed」——多重化ではなく単一化——という皮肉を込めて、このシステムは「Unics」と呼ばれ、やがて「Unix」となった。

UnixはMulticsの直系の子孫である。階層型ファイルシステム、プロセスの概念、シェルによる対話的操作——これらはすべてMulticsから引き継がれた。だが、Multicsの壮大さを切り捨て、小さなマシンでも動く簡潔さを選んだ。この設計判断が、Unixを世界中に普及させ、やがてLinuxへと繋がり、今日のクラウドインフラの基盤を形成することになる。

歴史の皮肉とはこういうものだ。「計算ユーティリティ」を目指したMulticsは、その目標を直接は達成しなかった。だが、Multicsの失敗から生まれたUnixが、Linuxとなり、KVMとなり、クラウドの仮想化基盤となって、McCarthyの「計算ユーティリティ」の夢を実現した。

---

## 5. 仮想マシンの原点——CP-67/CMSとVM/370

### 「各ユーザーに仮想的な計算機を」

Multicsとは異なるアプローチで「計算資源の共有」に挑んだのが、IBMのCP-67/CMSだった。

IBMのケンブリッジ科学センター（MIT近くに位置していた）で開発されたCP-67は、System/360 Model 67上で動作するハイパーバイザ——当時の用語では「仮想マシンモニタ」——だった。Model 67は1965年8月に発表され、System/360ファミリーの中で唯一、動的アドレス変換（DAT）ハードウェアを搭載したモデルだった。仮想メモリをサポートした最初の量産IBMシステムである。

CP-67の設計思想は明快だった。一台の物理的なSystem/360-67を、複数の「仮想的なSystem/360」に分割する。各ユーザーは自分専用の仮想マシンを手に入れる。その仮想マシンの中では、好きなOSを動かすことができる。ある仮想マシンではバッチ処理のOS/360を動かし、別の仮想マシンではCMS（Cambridge Monitor System、後にConversational Monitor System）を動かす。

ここで立ち止まって考えてほしい。これは、AWS EC2がやっていることと本質的に同じだ。

EC2は物理サーバを仮想マシンに分割し、各ユーザーにインスタンスとして提供する。各インスタンスではAmazon Linux、Ubuntu、Windows Serverなど、好きなOSを動かすことができる。CP-67が1967年に実現していたことを、EC2は2006年にインターネットスケールで再実現したのだ。

CP-67は1967年4月までに日常的に稼働していた。CP-40（CP-67の前身）も同時期に運用されていた。CP/CMSは「最初の仮想マシンオペレーティングシステム」として広く認知されている。

### VM/370——仮想マシンの商用化

CP-67の成功を受けて、IBMは1972年8月2日にVM/370を発表した。System/370の仮想メモリ対応モデルとともにリリースされたVM/370は、CP-67/CMSの概念を商用製品として再実装したものだ。

VM/370の系譜を整理すると、次のようになる。

```
CP-40（研究プロトタイプ）
  └─→ CP-67/CMS（System/360-67向け、実運用）
        └─→ CP-370/CMS（System/370向け、内部版）
              └─→ VM/370（1972年、商用製品）
                    └─→ VM/SP → VM/XA → z/VM（現在も稼働）
```

注目すべきは、この系譜の末端にあるz/VMが2020年代の今も現役で稼働していることだ。IBMのメインフレーム上で仮想マシンを提供するプラットフォームとして、50年以上の歴史を持つ。仮想マシンという概念の生命力は、驚嘆に値する。

### タイムシェアリングと仮想マシンの設計思想の比較

CTSSとCP-67は、「計算資源の共有」という同じ目標に対して、異なるアプローチを取った。

```
┌─────────────────────────────────────────────────────────┐
│ CTSS のアプローチ（タイムシェアリング）                   │
│                                                          │
│  ┌──────────────────┐                                    │
│  │    OS（CTSS）     │ ← 一つのOSが全ユーザーを管理     │
│  ├──────────────────┤                                    │
│  │ User A │ User B  │ ← ユーザーはOSの「中」にいる     │
│  ├──────────────────┤                                    │
│  │  物理ハードウェア │                                    │
│  └──────────────────┘                                    │
│                                                          │
│  特徴: ユーザーはOSを共有する                            │
│  利点: リソース効率が良い、軽量                           │
│  制約: ユーザーはOSの制約を受ける                         │
└─────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────┐
│ CP-67 のアプローチ（仮想マシン）                         │
│                                                          │
│  ┌────────┐ ┌────────┐                                   │
│  │ OS/360 │ │  CMS   │ ← 各ユーザーが独自のOSを持つ    │
│  │ User A │ │ User B │                                   │
│  ├────────┴─┴────────┤                                   │
│  │   CP（ハイパーバイザ）│ ← ハードウェアを仮想化       │
│  ├───────────────────┤                                   │
│  │  物理ハードウェア │                                    │
│  └───────────────────┘                                   │
│                                                          │
│  特徴: ユーザーは仮想マシンを所有する                    │
│  利点: 完全な隔離、OS選択の自由                           │
│  制約: 仮想化のオーバーヘッドがある                       │
└─────────────────────────────────────────────────────────┘
```

CTSSのアプローチは、現代で言えばコンテナ（Docker）に近い。一つのOS上でプロセスを隔離し、リソースを共有する。効率的だが、カーネルを共有しているため隔離は不完全だ。

CP-67のアプローチは、現代の仮想マシン（EC2インスタンス）そのものだ。ハイパーバイザが物理ハードウェアを抽象化し、各ユーザーに独立した仮想マシンを提供する。完全な隔離が得られるが、仮想化のオーバーヘッドがある。

この二つのアプローチの比較は、60年後の今も続いている。コンテナか仮想マシンか。Docker/KubernetesかEC2か。歴史は、同じ問いを異なる技術的文脈で繰り返す。

---

## 6. タイムシェアリングの技術——CPUタイムスライスとメモリ保護

ここからは、タイムシェアリングを実現する技術的な仕組みを掘り下げる。

### CPUタイムスライス——「見せかけの同時実行」

タイムシェアリングの核心は、CPUの処理時間を細かいスライス（量子、quantum）に分割し、各ユーザーのプロセスに順番に割り当てることだ。

```
時間軸 →
┌───┐┌───┐┌───┐┌───┐┌───┐┌───┐┌───┐┌───┐┌───┐
│ A ││ B ││ C ││ A ││ B ││ C ││ A ││ B ││ C │
└───┘└───┘└───┘└───┘└───┘└───┘└───┘└───┘└───┘
 20ms 20ms 20ms 20ms 20ms 20ms 20ms 20ms 20ms

ユーザー A: 「自分の入力にすぐ反応してくれる」
ユーザー B: 「自分の入力にすぐ反応してくれる」
ユーザー C: 「自分の入力にすぐ反応してくれる」
（実際には 60ms ごとに 20ms しかCPUを使えていない）
```

各スライスは数十ミリ秒程度。人間が入力してから応答を待つまでの時間は数百ミリ秒から数秒だから、CPUが他のユーザーの処理を行っている間に、人間はまだ次の入力を考えている。だから、各ユーザーは「自分専用の計算機を使っている」ように感じる。

この仕組みは、現代のOSにおけるプロセススケジューリングと同じ原理だ。Linuxカーネルのスケジューラは、CFS（Completely Fair Scheduler）というアルゴリズムでプロセスにCPU時間を分配している。マルチタスクの原理は、1960年代のタイムシェアリングから連綿と続いている。

### メモリ保護——「他人のデータを読めないようにする」

タイムシェアリングでもう一つ不可欠な技術がメモリ保護だ。複数のユーザーが一台の計算機を共有するとき、あるユーザーのプロセスが他のユーザーのメモリ領域を読み書きできてしまっては困る。

CTSSではIBM 7094のハードウェア機能を利用して、ユーザーのメモリ空間を保護した。だが、7094のメモリ保護機能は限定的だった。

この問題を本格的に解決したのが仮想メモリだ。System/360 Model 67の動的アドレス変換（DAT）ハードウェアは、各プロセスに独立した仮想アドレス空間を提供した。プロセスAが「メモリアドレス0x1000」にアクセスしても、プロセスBの「メモリアドレス0x1000」とは物理的に異なるメモリ領域にマッピングされる。各プロセスは、自分だけが計算機を使っているかのように振る舞える。

この仮想メモリの概念は、現代のOSすべてに実装されている。そしてクラウドにおいても、同じ原理が拡張されて使われている。EC2インスタンス間のメモリ隔離は、ハイパーバイザによる仮想アドレス空間の分離によって実現されている。1960年代のModel 67が達成した仮想メモリの隔離を、現代のNitroハイパーバイザがインスタンスレベルで実現している。

### プロセスの切り替え——コンテキストスイッチ

タイムスライスが切れたとき、CPUは現在のプロセスの状態（レジスタの内容、プログラムカウンタ、スタックポインタなど）を保存し、次のプロセスの状態を復元して実行を再開する。この操作をコンテキストスイッチと呼ぶ。

コンテキストスイッチにはコストがかかる。状態の保存と復元に要するCPUサイクル、キャッシュの無効化、TLB（Translation Lookaside Buffer）のフラッシュ。タイムスライスを短くすれば応答性は向上するが、コンテキストスイッチの頻度が上がり、実質的な処理効率が下がる。タイムスライスを長くすれば処理効率は上がるが、応答性が悪化する。

このトレードオフは60年間変わっていない。Linuxカーネルのスケジューラは、このバランスを絶えず調整し続けている。そしてクラウドのハイパーバイザもまた、物理CPUコアを仮想CPU（vCPU）間で切り替える際に、同じトレードオフに直面している。

---

## 7. ダートマスの実験——タイムシェアリングが「民主化」を意味した瞬間

タイムシェアリングの歴史を語る上で、もう一つ触れなければならないシステムがある。ダートマス大学のDTSS（Dartmouth Time-Sharing System）だ。

1964年5月1日午前4時、John KemenyとThomas KurtzはダートマスTSSを稼働させた。Kemenyはコンピュータサイエンスの教授であると同時に数学科長であり、計算機を専門家だけのものにしておくべきではないと考えていた。彼の有名な言葉がある。「ダートマスでは、数百万の人々が自分のコンピュータプログラムを書ける可能性を構想していた」。

DTSSのためにKemenyとKurtzが開発したのが、BASIC（Beginners' All-purpose Symbolic Instruction Code）だ。BASICは、プログラミングの専門家ではない学生が計算機を使えるよう設計された言語だった。1964年秋には、20台のテレタイプ端末を通じて数百人の新入生がDTSSを利用していた。

ここに、タイムシェアリングの本質的な意味がある。タイムシェアリングは単なる技術ではなかった。「計算資源の民主化」だった。

バッチ処理の時代、計算機は専門のオペレータを介してしか利用できなかった。プログラマですら、計算機に直接触ることは稀だった。タイムシェアリングは、テレタイプ端末を通じて、誰もが直接計算機と対話できる環境を作った。

この「民主化」の構造は、クラウドの本質と重なる。オンプレミスの時代、サーバの調達と運用はインフラエンジニアの専権事項だった。開発者がサーバを直接操作することは稀だった。クラウドは、APIを通じて、誰もが直接インフラを調達・操作できる環境を作った。

CTSSやDTSSがテレタイプ端末で計算機を民主化したように、AWSはAPIで計算資源を民主化した。手段は変わった。だが、「より多くの人が計算資源に直接アクセスできるようにする」という方向性は、60年間一貫している。

---

## 8. ハンズオン——タイムシェアリングの原理を体感する

ここからは、タイムシェアリングの原理を自分の手で体感する。1960年代のメインフレームは手元にないが、Linuxのプロセススケジューリングの仕組みは、タイムシェアリングの直系の子孫だ。

### 環境

- Docker（Ubuntu 24.04ベースコンテナ）
- Linux標準ツール（`/proc` ファイルシステム、`taskset`、`stress-ng`等）

### 演習1：CPUタイムスライスを観察する

まず、Linuxカーネルのスケジューリング設定を確認する。

```bash
# Docker環境に入る
docker run -it --rm --privileged ubuntu:24.04 bash

# 必要なツールをインストール
apt-get update && apt-get install -y procps stress-ng util-linux time sysstat

# カーネルのスケジューラ設定を確認
cat /proc/sys/kernel/sched_min_granularity_ns
# → 通常 3000000（3ミリ秒）

cat /proc/sys/kernel/sched_latency_ns
# → 通常 24000000（24ミリ秒）

# この値は「スケジューラが全てのタスクに少なくとも1回CPUを割り当てるまでの最大遅延」を意味する
# つまり、24ミリ秒以内に全てのプロセスが少なくとも1回は実行される
```

`sched_latency_ns` は、1960年代のタイムシェアリングにおけるタイムスライスの現代版だ。CTSSでは各ユーザーに約200ミリ秒のタイムスライスを割り当てていた。現代のLinuxでは、スケジューラの粒度はミリ秒単位にまで細かくなっている。

### 演習2：複数プロセスのCPU時間分配を観察する

タイムシェアリングの核心——複数のプロセスが1つのCPUを共有する——を観察する。

```bash
# 1つのCPUコアに制限する（タイムシェアリングを強制する）
# CPUコア0のみを使用する3つのCPU負荷プロセスを起動
taskset -c 0 stress-ng --cpu 1 --timeout 30s &
PID1=$!
taskset -c 0 stress-ng --cpu 1 --timeout 30s &
PID2=$!
taskset -c 0 stress-ng --cpu 1 --timeout 30s &
PID3=$!

echo "PID1=$PID1, PID2=$PID2, PID3=$PID3"

# 5秒待ってからCPU時間の分配を観察
sleep 5

# 各プロセスのCPU使用時間を確認
for PID in $PID1 $PID2 $PID3; do
  if [ -d "/proc/$PID" ]; then
    UTIME=$(awk '{print $14}' /proc/$PID/stat)
    STIME=$(awk '{print $15}' /proc/$PID/stat)
    echo "PID $PID: user=$UTIME, system=$STIME (clock ticks)"
  fi
done

# 全プロセスを停止
kill $PID1 $PID2 $PID3 2>/dev/null
wait 2>/dev/null
```

3つのプロセスが1つのCPUコアを共有しているとき、各プロセスのCPU時間はほぼ均等に分配されるはずだ。これが「公平なスケジューリング」（CFS: Completely Fair Scheduler）の効果であり、1960年代のタイムシェアリングが目指したものの現代版だ。

### 演習3：「Noisy Neighbor」問題を再現する

クラウドで問題になる「Noisy Neighbor」——同じ物理ホスト上の他のテナントがリソースを占有することで、自分のパフォーマンスが低下する現象——は、タイムシェアリングの時代から存在する問題だ。

```bash
# ベースラインの計測：CPUを独占できる場合
echo "=== ベースライン（CPU独占）==="
taskset -c 0 bash -c '
  START=$(date +%s%N)
  for i in $(seq 1 1000000); do
    : # 何もしないループ
  done
  END=$(date +%s%N)
  echo "所要時間: $(( (END - START) / 1000000 )) ミリ秒"
'

# Noisy Neighbor がいる場合
echo "=== Noisy Neighbor あり（CPU共有）==="
# バックグラウンドで3つの負荷プロセスを起動（同じCPUコア）
taskset -c 0 stress-ng --cpu 1 --timeout 30s &
NP1=$!
taskset -c 0 stress-ng --cpu 1 --timeout 30s &
NP2=$!
taskset -c 0 stress-ng --cpu 1 --timeout 30s &
NP3=$!

sleep 2

# 同じ処理を実行
taskset -c 0 bash -c '
  START=$(date +%s%N)
  for i in $(seq 1 1000000); do
    : # 何もしないループ
  done
  END=$(date +%s%N)
  echo "所要時間: $(( (END - START) / 1000000 )) ミリ秒"
'

# クリーンアップ
kill $NP1 $NP2 $NP3 2>/dev/null
wait 2>/dev/null
```

CPU独占の場合と、3つの「Noisy Neighbor」がいる場合で、同じ処理の所要時間が大幅に異なるはずだ。CPUを4プロセスで共有しているため、理論的には4倍近くの時間がかかる。

これは、1960年代のタイムシェアリングでユーザー数が増えるとレスポンスが悪化した問題と同じ構造だ。そしてクラウドのEC2インスタンスでも、同じ物理ホスト上の他のテナントの負荷によってパフォーマンスが変動する。60年間、問題の構造は変わっていない。

### 演習4：コンテキストスイッチを計測する

タイムシェアリングの「見えないコスト」——コンテキストスイッチのオーバーヘッドを計測する。

```bash
# 現在のコンテキストスイッチ回数を確認
grep ctxt /proc/stat
# → ctxt 12345678（システム起動以降の総コンテキストスイッチ回数）

# 1秒間のコンテキストスイッチ回数を計測
echo "=== 低負荷時のコンテキストスイッチ ==="
CS1=$(awk '/ctxt/ {print $2}' /proc/stat)
sleep 1
CS2=$(awk '/ctxt/ {print $2}' /proc/stat)
echo "1秒間のコンテキストスイッチ: $((CS2 - CS1)) 回"

# 高負荷をかけてコンテキストスイッチ回数の変化を観察
echo "=== 高負荷時のコンテキストスイッチ ==="
for i in $(seq 1 10); do
  taskset -c 0 stress-ng --cpu 1 --timeout 10s &
done

sleep 2
CS3=$(awk '/ctxt/ {print $2}' /proc/stat)
sleep 1
CS4=$(awk '/ctxt/ {print $2}' /proc/stat)
echo "1秒間のコンテキストスイッチ: $((CS4 - CS3)) 回"

# クリーンアップ
pkill stress-ng 2>/dev/null
wait 2>/dev/null
```

プロセス数が増えると、コンテキストスイッチの回数も増える。各コンテキストスイッチには数マイクロ秒のオーバーヘッドがかかる。これが積み重なると、システム全体のスループットが低下する。1960年代のCTSSでも、接続ユーザー数が増えるとオーバーヘッドが顕在化した。

### この演習で何がわかるか

これらの演習を通じて、以下のことが体感できるはずだ。

**第一に、タイムシェアリングの原理は現代のOSにそのまま生きている。** Linuxのプロセススケジューラは、CTSSが1961年に実現した「CPUタイムスライスの分配」の直系の子孫だ。原理は同じで、精度と規模が異なるだけだ。

**第二に、「共有」にはコストがある。** CPUを共有するとNoisy Neighbor問題が発生し、コンテキストスイッチのオーバーヘッドが生じる。これは1960年代から変わらない構造的な問題であり、クラウドにおいても同様だ。

**第三に、「隔離」と「効率」のトレードオフは普遍的だ。** 完全に隔離すれば安全だが効率が下がる。共有すれば効率が上がるがリスクが増す。タイムシェアリングからクラウドまで、この設計判断は常に存在する。

ハンズオンの詳細な手順と自動セットアップスクリプトは、本リポジトリの `handson/cloud-history/02-mainframe-timesharing/` に用意してある。

---

## 9. まとめと次回予告

### この回のまとめ

第2回では、メインフレームとタイムシェアリングの時代——計算資源を共有するという発想が生まれた時代——を探った。

**バッチ処理は計算機中心のモデルだった。** 計算機の利用効率を最大化する代わりに、人間に数時間の待ち時間を強いた。パンチカードにプログラムを穿ち、オペレータに渡し、結果を待つ。計算機は聖域であり、人間はその前に並ぶ列の一人だった。

**タイムシェアリングは、この関係を逆転させた。** 1961年、Fernando CorbatoはMITでCTSSの公開デモンストレーションを行った。CPUの処理時間を細かいスライスに分割し、各ユーザーに順番に割り当てることで、複数のユーザーが一台の計算機を「同時に」利用できるようにした。計算機が人間のペースに合わせる。この転換は、クラウドが「インフラのペースではなく開発者のペースに合わせる」のと同じ構造だ。

**タイムシェアリングは「計算資源の民主化」の始まりだった。** ダートマス大学のDTSSは、専門家以外の学生にも計算機への直接アクセスを提供し、そのためにBASIC言語を生み出した。バッチ処理時代にはオペレータを介さなければ使えなかった計算機が、テレタイプ端末から誰でも直接利用できるようになった。

**仮想マシンの概念は1960年代に既に完成していた。** IBMのCP-67/CMSは、一台の物理マシンを複数の仮想マシンに分割し、各ユーザーに独立した環境を提供した。この設計思想は、2006年のAWS EC2にそのまま受け継がれている。

**Multicsは失敗から成功を生んだ。** 「計算ユーティリティ」を目指したMulticsは直接的には目標を達成しなかったが、その技術的遺産（階層型ファイルシステム、保護リング、動的リンク）はUnix/Linuxを通じて現代のクラウド基盤に流れ込んだ。Multicsの撤退メンバーが作ったUnixが、Linuxとなり、KVMとなり、クラウドの仮想化基盤となった。

冒頭の問いに答えよう。「複数のユーザーが一つの計算機を共有する」という発想は、どこから来たのか。それは、計算機が高価すぎて一人で占有するのが非合理的だった時代に、計算機の効率ではなく人間の生産性を優先しようとした先駆者たちの問題意識から来た。そしてこの「共有」の設計思想は、形を変えながらも、現代のクラウドコンピューティングの根幹として生き続けている。

### 次回予告

第3回では、「クライアント/サーバモデル——計算の分散が始まった日」を探る。

メインフレームの時代、計算は一箇所に集中していた。一台の巨大な計算機がすべてを処理し、ユーザーはターミナルからアクセスする。だが1980年代、パーソナルコンピュータの登場とLANの普及によって、計算は「分散」し始めた。

「手元で計算する」と「向こう側で計算する」——この分割は何を解決し、何を生み出したのか。Novell NetWare、Sun Microsystemsの「ネットワークはコンピュータだ」、Windows NT Server。計算が分散した時代を振り返ることで、なぜクラウドが再び「集中」に回帰したのかが見えてくる。

あなたは、計算を「手元」で行うべきか「向こう側」で行うべきか、その判断基準を持っているだろうか。

---

## 参考文献

- Fernando J. Corbato, "An Experimental Time-Sharing System", AFIPS Spring Joint Computer Conference, 1962. <https://multicians.org/thvv/compatible-time-sharing-system.pdf>
- ACM, "A.M. Turing Award - Fernando J. Corbato", 1990. <https://amturing.acm.org/award_winners/corbato_1009471.cfm>
- Multicians.org, "Multics History". <https://multicians.org/history.html>
- Wikipedia, "CP-67". <https://en.wikipedia.org/wiki/CP-67>
- IBM, "VM 50th Anniversary". <https://www.vm.ibm.com/history/50th/index.html>
- R. J. Creasy, "The Origin of the VM/370 Time-sharing System", IBM Journal of Research and Development, Vol. 25, No. 5, 1981. <https://pages.cs.wisc.edu/~stjones/proj/vm_reading/ibmrd2505M.pdf>
- Wikipedia, "Dartmouth Time-Sharing System". <https://en.wikipedia.org/wiki/Dartmouth_Time-Sharing_System>
- Dartmouth College, "BASIC at Dartmouth". <https://www.dartmouth.edu/basicfifty/basic.html>
- Schroeder, M.D. and Saltzer, J.H., "A Hardware Architecture for Implementing Protection Rings", Communications of the ACM, Vol. 15, No. 3, 1972. <https://multicians.org/protection.html>
- IBM, "The punched card". <https://www.ibm.com/history/punched-card>
- Wikipedia, "Job Control Language". <https://en.wikipedia.org/wiki/Job_Control_Language>
- Multicians.org, "The IBM 7094 and CTSS". <https://www.multicians.org/thvv/7094.html>
