# クラウドの考古学

## ——メインフレームからサーバーレスへ、計算資源の民主化史

### 第5回：ホスティングサービス——サーバ管理を他人に委ねる

**連載「クラウドの考古学——メインフレームからサーバーレスへ、計算資源の民主化史」**
**著：佐藤裕介（Engineers Hub株式会社 CEO / Technical Lead）**

---

**この回で学べること：**

- 共有ホスティングから専用サーバ、VPSに至るホスティングサービスの進化の系譜
- Apache仮想ホストが可能にした「1台のサーバで複数のサイト」という発想
- 共有ホスティングの「見えない隣人」問題とオーバーセリングの構造
- VPS（Virtual Private Server）が実現した「安価な疑似専用環境」の技術的背景
- cgroupsによるリソース制限の仕組みと「リソースの切り売り」の概念

---

## 1. SSHでログインするだけでよかった

2003年のある日、私はさくらインターネットの専用サーバを契約した。

それまでの私は、秋葉原でサーバを買い、車でデータセンターに運び込み、ラッキングし、ケーブリングし、OSをインストールするという一連の儀式を繰り返していた。前回書いた通り、コロケーションとはそういうものだった。サーバの「場所」は解決されたが、サーバの「調達」と「構築」は依然として自分の手で行わなければならなかった。

さくらインターネットの専用サーバは、その前提を覆した。Webの申し込みフォームに必要事項を入力し、スペックを選び、支払い手続きを済ませる。数日後、メールが届く。IPアドレスと初期パスワードが記載されている。ターミナルを開き、SSHで接続する。ログインできた。それだけだった。

秋葉原に行く必要がない。車を出す必要がない。ラックの前に立つ必要がない。電源ケーブルに触れる必要がない。サーバはもうそこにあり、ネットワークに接続され、OSがインストールされ、私を待っていた。

この体験は、コロケーションの苦労を知る人間にとっては革命的だった。サーバの調達リードタイムが「数週間から1ヶ月」から「数日」に縮まった。物理的な搬入作業が完全に消えた。ハードウェアの初期不良に悩まされることもない。「サーバを用意する」という行為の重さが、劇的に軽くなった。

だが、使い始めてすぐに限界も見えた。

あるWebサービスのトラフィックが急増した。メモリが足りない。CPUが張り付いている。スペックを上げたい。だが、専用サーバのスペック変更は「申請して数日待つ」プロセスだった。物理的なサーバを入れ替える必要があるのだから当然だ。データの移行も自分で行わなければならない。トラフィックが急増しているのは今この瞬間なのに、対処できるのは数日後。その数日の間にビジネスチャンスは過ぎ去っていく。

コロケーションでは「場所」が問題だった。専用サーバでは「場所」は解決されたが、「弾力性」——需要に応じてリソースを動的に増減する——という概念はまだ存在しなかった。サーバは相変わらず「固定スペックの箱」だった。箱の大きさを変えるには、別の箱を用意して中身を移すしかない。

あなたが今使っているクラウドの `--instance-type` オプション——t3.microからm5.xlargeに変更するだけでスペックが変わる——は、この時代の苦痛への直接的な回答だ。だが、その回答に至るまでには、ホスティングサービスの長い進化があった。

---

## 2. ホスティングサービスの系譜——共有から専用、そしてVPSへ

### 共有ホスティング——1台のサーバを数百人で分け合う

ホスティングサービスの歴史は、1990年代中盤に始まる。

1994年、GeoCitiesがDavid BohnettとJohn Reznerによって設立された（当初はBeverly Hills Internetという名称だった）。ユーザーは無料でWebページを作成でき、「Hollywood」「WallStreet」「SunsetStrip」といったテーマ別の「ネイバーフッド」に自分のページを置いた。1999年にYahoo!に買収された時点で、GeoCitiesは世界で3番目にアクセスの多いWebサイトだった。1996年にはAngelfireが、1995年にはTripodが登場し、個人がWebに存在を持つためのプラットフォームが次々と生まれた。

これらは無料ホスティングの系譜だが、商用の共有ホスティング（Shared Hosting）もこの時期に確立された。その技術的基盤を提供したのが、Apache HTTP Serverの仮想ホスト（Virtual Host）機能だ。

Apache HTTP Serverは1995年に初版がリリースされ、1996年半ばまでに世界で最も人気のあるWebサーバとなった。Apacheは初期からIPベースの仮想ホストをサポートしていた。1台の物理サーバに複数のIPアドレスを割り当て、IPアドレスごとに異なるWebサイトを配信する仕組みだ。だが、IPv4アドレスは有限であり、顧客ごとにIPアドレスを割り当てるのはコストがかかる。

転機となったのは、HTTP/1.1の`Host`ヘッダだ。1997年1月にRFC 2068として標準化されたHTTP/1.1は、リクエストに`Host`ヘッダを含めることを必須とした。これにより、同一IPアドレス上で複数のドメイン名を識別できるようになった。Apache 1.1以降で対応した名前ベースの仮想ホスト（Name-based Virtual Host）は、1つのIPアドレスで何百ものWebサイトを収容することを可能にした。

```
名前ベース仮想ホストの動作原理:

クライアントのHTTPリクエスト:
  GET /index.html HTTP/1.1
  Host: www.example-a.com        ← このヘッダでサイトを識別
                                   ↓
              ┌──────────────────────────────────┐
              │ Apache HTTP Server               │
              │ IPアドレス: 192.168.1.100        │
              │                                  │
              │ <VirtualHost *:80>               │
              │   ServerName www.example-a.com   │──→ /var/www/site-a/
              │ </VirtualHost>                   │
              │                                  │
              │ <VirtualHost *:80>               │
              │   ServerName www.example-b.com   │──→ /var/www/site-b/
              │ </VirtualHost>                   │
              │                                  │
              │ <VirtualHost *:80>               │
              │   ServerName www.example-c.com   │──→ /var/www/site-c/
              │ </VirtualHost>                   │
              └──────────────────────────────────┘
              1つのIPアドレス、1台のサーバで
              数百のWebサイトを収容可能
```

この技術的基盤の上に、共有ホスティングのビジネスモデルが成立した。1台の物理サーバのリソース——CPU、メモリ、ストレージ、帯域——を数十から数百の顧客で分け合う。1台のサーバの月額コストが10万円だとして、100人の顧客で分ければ1人あたり月額1,000円。個人や小規模事業者でも手が届く価格でWebサイトを公開できるようになった。

1996年にJ. Nick KostonがSpeed Hosting社向けに開発を始めたcPanelは、共有ホスティングの管理を劇的に簡素化した。1999年にリリースされたcPanel 3にはWHM（Web Host Manager）が含まれ、ホスティング事業者はWebベースのインターフェースで顧客アカウントの作成、ドメイン設定、メールアカウント管理を行えるようになった。技術的な知識がなくても、cPanelのGUIからファイルをアップロードし、データベースを作成し、メールアドレスを設定できる。この「管理の民主化」が、共有ホスティングの爆発的普及を支えた。

だが、共有ホスティングには構造的な問題があった。

### 「見えない隣人」問題——オーバーセリングの闇

共有ホスティングの顧客は、自分のWebサイトが物理的にどのサーバで動いているか知らない。そして、そのサーバを何人の「隣人」と共有しているかも知らない。

ホスティング事業者は、利益率を高めるためにオーバーセリング（overselling）を行う。「ディスク容量10GB、転送量無制限」と謳いながら、実際の物理サーバのディスク容量は500GB。100人の顧客にそれぞれ10GBを「割り当てた」としても、全顧客が上限まで使うことは統計的にない。実際の平均使用量が2GBなら、500GBのディスクに250人を詰め込める。これがオーバーセリングだ。

CPUとメモリについてはさらに深刻だ。ディスクは使わなければ消費されないが、CPUとメモリは動的に変動する。ある顧客のサイトがテレビで紹介されてアクセスが急増すれば、その顧客のプロセスがCPUとメモリを食い尽くし、同じサーバの他の顧客のサイトが遅延する。あるいは落ちる。

これが「Noisy Neighbor（うるさい隣人）」問題だ。

私も経験がある。共有ホスティングで動かしていた小規模なWebサイトが、ある日突然遅くなった。自分のサイトには変更を加えていない。アクセス数も変わっていない。原因は、同じサーバの別の顧客だった。ホスティング事業者に問い合わせると、「同一サーバの他のお客様のアクセス集中により、一時的にリソースが逼迫しています」という回答が返ってきた。

自分のサイトの性能が、自分の知らない「隣人」の挙動に左右される。これは共有ホスティングの本質的な欠陥だった。リソースの隔離が不十分なのだ。後にCloudLinuxのようなソフトウェアがユーザーごとの「リソースケージ」を実現し、一定の緩和策を提供するようになるが、共有ホスティングの根本的な制約——物理リソースを論理的に分割しているにすぎない——は変わらなかった。

### 専用サーバホスティング——「自分だけの箱」を借りる

共有ホスティングの限界は明白だった。他の顧客の影響を受けない、自分だけのサーバがほしい。だが、コロケーションのように物理サーバを自分で購入し、データセンターに搬入する手間は避けたい。

この需要に応えたのが、専用サーバホスティング（Dedicated Server Hosting）だ。

さくらインターネットは1996年にサービスを開始した（法人化は1999年）。創業者の田中邦裕は舞鶴工業高等専門学校の在学中に、学生寮内で友人にサーバを貸し出したことがきっかけでこの事業を始めた。日本のインターネットインフラを支えたホスティング事業者の原点が、高専の学生寮にあったという事実は、計算資源の民主化の歴史を象徴している。

米国ではRackspaceが1996年に創業した（当初はCymitar Technology Group、1998年にRackspaceに改名）。Rackspaceが差別化のポイントとしたのは技術ではなく、「Fanatical Support（熱狂的なサポート）」という顧客サービスの理念だった。当時の多くのホスティング企業が技術インフラに注力する中、Rackspaceは24時間365日の手厚いサポートを前面に打ち出した。サーバの物理的な管理はRackspaceが担い、顧客はSSHでログインしてアプリケーションのデプロイと運用に集中する。

SoftLayerは2005年にLance Crosbyらによって設立され、当初はゲーム企業やスタートアップ向けの専用サーバを提供していた。2013年にIBMが買収し（推定20億ドル超の取引）、後にIBM Cloudの基盤となった。

専用サーバホスティングのモデルは明快だ。

```
コロケーションとの比較:

コロケーション:
  顧客の責任: [サーバ購入] → [OS構築] → [搬入・ラッキング] → [運用・障害対応]
  施設の責任: [電力] [冷却] [ネットワーク] [物理セキュリティ]

専用サーバホスティング:
  顧客の責任:                  [OS設定] → [アプリデプロイ] → [運用]
  事業者の責任: [サーバ調達] → [OS基本インストール] → [ラッキング] → [HW障害対応]
  施設の責任: [電力] [冷却] [ネットワーク] [物理セキュリティ]

                  ↑ 顧客の負担が大幅に軽減
                  ↑ ただし「固定スペック」「スケール不可」は変わらず
```

コロケーションと比べて、顧客の負担は大幅に軽減された。サーバの調達、設置、ハードウェア障害の対応はすべてホスティング事業者の責任だ。顧客はOSの設定とアプリケーションのデプロイに集中できる。

だが、根本的な制約は残っていた。サーバは「固定スペックの箱」であり、スペック変更には物理的なサーバの入れ替えが必要だった。月額契約が基本であり、「必要なときだけ使う」という柔軟性はなかった。1台の物理サーバ丸ごとの料金なので、その利用率が10%であっても100%であっても、コストは同じだった。

### VPS——仮想化が生んだ「中間解」

共有ホスティングは安価だがリソースが隔離されない。専用サーバは隔離されるが高価で融通が利かない。この二者択一の間を埋めたのが、VPS（Virtual Private Server）だった。

VPSの技術的な起源は、OSレベルの仮想化にある。FreeBSD jailが2000年にリリースされた。開発者のPoul-Henning Kampは、小規模ホスティング事業者R&D Associates, Inc.からの委託でこの機能を実装した。顧客ごとにサーバを物理的に分けるのはコストが高すぎる。だが、共有ホスティングのように何も隔離しないのはセキュリティ上の問題がある。jailは、1つのFreeBSDカーネルの上で複数の独立した環境を動かす——ファイルシステム、ネットワーク、プロセス空間を隔離する——ことでこの問題に回答した。ホスティング事業者の現場から生まれた技術だったのだ。

2001年、SWsoft社（1997年設立、後にParallelsに改名）がVirtuozzo for Linuxをリリースした。VirtuozzoはOSレベルの仮想化をLinuxで商用化した先駆者だ。1つのLinuxカーネルの上に複数の隔離された環境（コンテナ）を作り、それぞれに独立したファイルシステム、プロセス空間、ネットワークスタックを持たせる。各コンテナはあたかも独立したLinuxサーバのように振る舞うが、物理的には1台のサーバだ。ハイパーバイザ型の仮想化（VMware等）と異なり、ゲストOSのカーネルを別途動かす必要がないため、オーバーヘッドが極めて小さかった。

2003年6月、Christopher AkerがLinodeを設立した。当初はUML（User Mode Linux）を仮想化技術として採用し、月額約20ドルで256MBのRAMと10GBのストレージを持つVPSを提供した。後にXen（2008年）、KVM（2015年）へと仮想化基盤を移行している。Linodeの革新は、VPSを開発者向けに「シンプルで安価」に提供したことにある。コントロールパネルからインスタンスを選び、Linuxディストリビューションを選び、デプロイする。コロケーションの物理的な重さも、共有ホスティングの不自由さもなかった。

2011年にはBen Uretsky、Moisey Uretskyらによって DigitalOceanが設立された。2012年にプロダクトをローンチしたDigitalOceanは、「5ドルのDroplet」で知られるようになった。月額5ドルで512MBのRAMと20GBのSSDストレージを持つVPSが手に入る。55秒でデプロイ完了。DigitalOceanは、VPSの民主化をさらに一歩進め、個人の開発者や学生でもインターネット上にサーバを持てる世界を実現した。

```
ホスティングサービスの進化と価格帯の変遷:

            価格        リソース隔離    root権限    セットアップ
共有         月額数百     なし           なし        即時
 ホスティング   〜数千円   （Noisy Neighbor）

VPS          月額数千     あり（OS/HV    あり        数分〜
              〜数万円    レベル分離）               数十分

専用サーバ    月額数万     完全           あり        数日
              〜数十万円  （物理分離）

コロケーション 月額数万     完全           あり        数週間
              〜数百万円  （物理分離）               〜数ヶ月
              + HW購入費

↑ VPSは「共有の価格」で「専用に近い隔離」を実現した中間解
```

VPSの登場は、ホスティングサービスの進化における重要な転換点だった。共有ホスティングの価格帯で、専用サーバに近いリソースの隔離を実現する。root権限が与えられ、好きなソフトウェアをインストールでき、他の顧客の影響を受けにくい。「安価な疑似専用環境」——これがVPSの本質だった。

---

## 3. VPSの技術的基盤——リソースの切り売りを支えた仕組み

### OSレベル仮想化 vs ハイパーバイザ型仮想化

VPSの技術的実現には、大きく2つのアプローチがある。OSレベル仮想化とハイパーバイザ型仮想化だ。この違いを理解することは、後のコンテナ技術やクラウドの設計思想を理解する基盤となる。

```
OSレベル仮想化（Virtuozzo, OpenVZ, LXC）:

    ┌───────────────────────────────────────────────┐
    │          1つのLinuxカーネル                     │
    │                                               │
    │  ┌──────────┐ ┌──────────┐ ┌──────────┐      │
    │  │ VPS-A    │ │ VPS-B    │ │ VPS-C    │      │
    │  │ プロセス │ │ プロセス │ │ プロセス │      │
    │  │ ファイル │ │ ファイル │ │ ファイル │      │
    │  │ ネット   │ │ ネット   │ │ ネット   │      │
    │  └──────────┘ └──────────┘ └──────────┘      │
    │     namespace + cgroupsで隔離                  │
    └───────────────────────────────────────────────┘
    │              物理サーバ                        │
    └───────────────────────────────────────────────┘

    利点: オーバーヘッドが極めて小さい（カーネル共有）
    欠点: 全VPSが同一カーネル。異なるOSは動かせない


ハイパーバイザ型仮想化（Xen, KVM）:

    ┌───────────────────────────────────────────────┐
    │  ┌──────────┐ ┌──────────┐ ┌──────────┐      │
    │  │ VPS-A    │ │ VPS-B    │ │ VPS-C    │      │
    │  │ ゲストOS │ │ ゲストOS │ │ ゲストOS │      │
    │  │(Ubuntu)  │ │(CentOS)  │ │(Debian)  │      │
    │  │ 独自     │ │ 独自     │ │ 独自     │      │
    │  │ カーネル │ │ カーネル │ │ カーネル │      │
    │  └──────────┘ └──────────┘ └──────────┘      │
    │        ハイパーバイザ（Xen / KVM）             │
    └───────────────────────────────────────────────┘
    │              物理サーバ                        │
    └───────────────────────────────────────────────┘

    利点: 各VPSが独自のカーネルを持つ。隔離が強固
    欠点: カーネルの重複分、リソースのオーバーヘッドが大きい
```

初期のVPS事業者の多くはOSレベル仮想化を採用した。Virtuozzo（およびそのオープンソース版OpenVZ）が代表格だ。1つのLinuxカーネルの上に複数のコンテナを作り、各コンテナに独立したファイルシステム、プロセス空間、ネットワークスタックを割り当てる。カーネルは共有されるため、メモリのオーバーヘッドが小さく、1台の物理サーバにより多くのVPSを収容できる。ホスティング事業者にとっては、収容密度の高さが直接的に利益率に響く。

一方、Linode（当初のUML、後のXen/KVM）やDigitalOcean（KVM）はハイパーバイザ型仮想化を採用した。各VPSが独自のカーネルを持つため、異なるLinuxディストリビューションを自由に選択でき、カーネルのアップグレードも顧客が自分で行える。隔離の強度もOSレベル仮想化より高い。代償として、カーネルの重複分だけリソースを消費するが、ハードウェアの性能向上とIntel VT-x/AMD-Vのハードウェア仮想化支援がこのオーバーヘッドを許容可能な範囲に抑えた。

AWSのEC2は初期にXen仮想化を採用し、後にKVMベースのNitroハイパーバイザに移行している。VPSの時代に確立されたハイパーバイザ型仮想化の技術は、クラウドの直接的な基盤となった。

### cgroupsとnamespace——リソース隔離の核心

VPSのリソース隔離を支える技術の中核が、Linuxカーネルのcgroups（control groups）とnamespaceだ。

cgroupsは2006年にGoogleのPaul MenageとRohit Sethが「process containers」として開発を開始した。2007年末に「control groups」に改名され（「container」という用語がカーネル内で他の意味と混同されるのを避けるため）、2008年1月リリースのLinuxカーネル2.6.24でメインラインに統合された。

cgroupsが解決する問題は明確だ。1台のLinuxマシン上で動作する複数のプロセスグループに対し、CPU時間、メモリ、ディスクI/O、ネットワーク帯域を個別に制限する。VPSの文脈で言えば、「VPS-AにはCPU 2コア分、メモリ2GBまで」「VPS-BにはCPU 1コア分、メモリ1GBまで」というリソースの「切り売り」を実現する。

```
cgroupsによるリソース制限の概念:

物理サーバ: CPU 8コア, メモリ 32GB, ディスクI/O 1000MB/s

   ┌────────────────────────────────────────────────────┐
   │ cgroup: /vps-a                                     │
   │   cpu.shares: 200  (CPUの25%)                      │
   │   memory.limit_in_bytes: 4GB                       │
   │   blkio.throttle: 200MB/s                          │
   │                                                    │
   │   └── VPS-Aのプロセス群                            │
   ├────────────────────────────────────────────────────┤
   │ cgroup: /vps-b                                     │
   │   cpu.shares: 100  (CPUの12.5%)                    │
   │   memory.limit_in_bytes: 2GB                       │
   │   blkio.throttle: 100MB/s                          │
   │                                                    │
   │   └── VPS-Bのプロセス群                            │
   ├────────────────────────────────────────────────────┤
   │ cgroup: /vps-c                                     │
   │   cpu.shares: 100  (CPUの12.5%)                    │
   │   memory.limit_in_bytes: 2GB                       │
   │   blkio.throttle: 100MB/s                          │
   │                                                    │
   │   └── VPS-Cのプロセス群                            │
   └────────────────────────────────────────────────────┘

   ※ 残りのリソースは追加のVPSや管理プロセスに割り当て
```

namespaceは、プロセスに「見える世界」を限定する。PID namespace（プロセスID）、NET namespace（ネットワークインターフェース）、MNT namespace（マウントポイント）、UTS namespace（ホスト名）、IPC namespace（プロセス間通信）、USER namespace（ユーザーID）——これらをVPSごとに分離することで、各VPSは「自分だけのLinuxシステム」として振る舞う。VPS-Aのプロセスからは、VPS-Bのプロセスもファイルもネットワークインターフェースも見えない。

LXC（Linux Containers）は2008年に登場し、cgroupsとnamespacesを組み合わせた初のメインラインカーネルベースのコンテナ実装となった。それまでのVirtuozzo/OpenVZがカーネルパッチを必要としたのに対し、LXCは標準のLinuxカーネルだけで動作した。この「カーネルパッチ不要」という特性は、LXCの採用障壁を大幅に下げた。

### ホスティングの限界——「弾力性」の不在

VPSは、共有ホスティングと専用サーバの間を埋める優れた中間解だった。だが、VPSにもなお克服できない限界があった。

**固定スペック**: VPSのリソース割り当て（CPU、メモリ、ストレージ）は契約時に決定され、変更には手続きが必要だった。需要の変動に応じて動的にリソースを増減する仕組みは、多くのVPS事業者では提供されていなかった。

**手動スケーリング**: トラフィックが増えた場合、上位プランへの移行は手動プロセスだ。データの移行も自分で行う必要がある。「オートスケーリング」——負荷に応じて自動的にサーバ台数を増減する——という概念は、VPSの世界には存在しなかった。

**長期契約**: 月額課金が基本であり、「1時間だけ使って捨てる」という利用形態は想定されていなかった。テスト環境を一時的に立ち上げ、用が済んだら消す。このワークフローが実現するのは、EC2の従量課金モデルを待たなければならなかった。

**単一サーバの限界**: VPSは1台の物理サーバの範囲内でリソースを分割する。サーバを跨いだ冗長化やロードバランシングは、顧客が自力で構築する必要があった。

これらの限界は、ホスティングサービスが「サーバの貸し出し」というモデルの延長線上にある以上、構造的に解消が困難だった。サーバは固定的な物理リソースであり、その固定性がそのままサービスの制約となった。

この制約を打ち破るには、「サーバ」という概念そのものを再定義する必要があった。サーバを「固定スペックの箱」ではなく、「APIで制御できる抽象化されたリソースプール」として捉え直す。その再定義を行ったのが、2006年にAWSが公開するEC2だ。

---

## 4. ハンズオン——LXCでVPSに近い環境を構築し、リソース制限を体感する

ここからは、LXCに代わりLinuxのcgroupsを直接操作して、VPSのリソース制限の仕組みを体感する。1台のサーバの物理リソースを複数の「テナント」に切り分け、それぞれのリソース使用量を制限する——VPS事業者が行っていたことの本質を再現する。

### 環境

- Docker（Ubuntu 24.04ベースコンテナ、`--privileged`オプションが必要）
- Linux標準ツール

### 演習1：cgroupsによるCPU制限——「リソースの切り売り」を再現する

VPS事業者が1台のサーバのCPUリソースを複数の顧客に分配する仕組みを、cgroupsで直接体験する。

```bash
# Docker環境に入る（cgroup操作にはprivilegedが必要）
docker run -it --rm --privileged --name vps-handson ubuntu:24.04 bash

# 必要なツールをインストール
apt-get update && apt-get install -y cgroup-tools stress-ng procps bc
```

```bash
# --- 演習1: CPU制限 ---
echo "=== 演習1: cgroupsによるCPU制限 ==="
echo ""

# cgroup v2の確認
mount | grep cgroup2

# テナント用のcgroupを作成
# VPS-A: CPUの重みを100（標準）
# VPS-B: CPUの重みを50（VPS-Aの半分）
mkdir -p /sys/fs/cgroup/vps-a
mkdir -p /sys/fs/cgroup/vps-b

# CPU重みの設定（cgroup v2のcpu.weight）
# デフォルトは100。値が大きいほどCPU時間を多く受け取る
echo 100 > /sys/fs/cgroup/vps-a/cpu.weight
echo 50 > /sys/fs/cgroup/vps-b/cpu.weight

echo "VPS-A cpu.weight: $(cat /sys/fs/cgroup/vps-a/cpu.weight)"
echo "VPS-B cpu.weight: $(cat /sys/fs/cgroup/vps-b/cpu.weight)"
echo ""
echo "VPS-AはVPS-Bの2倍のCPU時間を受け取る設定"
echo "（実際のVPS事業者はこの仕組みでプランごとのCPU割り当てを実現）"
echo ""

# 両方のcgroupで同時にCPU負荷を発生させる
echo "--- CPU負荷テスト開始（10秒間）---"

# VPS-AでCPU負荷を発生
stress-ng --cpu 1 --timeout 10s --metrics-brief &
VPS_A_PID=$!
echo $VPS_A_PID > /sys/fs/cgroup/vps-a/cgroup.procs

# VPS-BでCPU負荷を発生
stress-ng --cpu 1 --timeout 10s --metrics-brief &
VPS_B_PID=$!
echo $VPS_B_PID > /sys/fs/cgroup/vps-b/cgroup.procs

echo "VPS-A (PID: $VPS_A_PID) → cpu.weight=100"
echo "VPS-B (PID: $VPS_B_PID) → cpu.weight=50"
echo ""

# CPU使用状況を確認
sleep 5
echo "--- 5秒後のCPU統計 ---"
echo "VPS-A: $(cat /sys/fs/cgroup/vps-a/cpu.stat | head -3)"
echo "VPS-B: $(cat /sys/fs/cgroup/vps-b/cpu.stat | head -3)"

wait 2>/dev/null
echo ""
echo "=== CPU制限テスト完了 ==="
```

### 演習2：メモリ制限——「契約プラン」のリソース上限

VPSの「メモリ2GBプラン」が意味するものをcgroupsで再現する。

```bash
echo ""
echo "=== 演習2: メモリ制限 ==="
echo ""

# VPS-Aのメモリ制限を64MBに設定
echo $((64 * 1024 * 1024)) > /sys/fs/cgroup/vps-a/memory.max

echo "VPS-A memory.max: $(cat /sys/fs/cgroup/vps-a/memory.max) bytes"
echo "（64MBに制限。実際のVPSでは512MB〜数GBが一般的）"
echo ""

# メモリ使用状況を確認
echo "--- 現在のメモリ使用状況 ---"
echo "VPS-A memory.current: $(cat /sys/fs/cgroup/vps-a/memory.current) bytes"
echo ""

# メモリ制限を超えるプロセスを実行（OOM Killerが発動する）
echo "--- メモリ制限を超える処理を実行 ---"
echo "（VPS-Aのプロセスが64MBを超えようとすると、OOM Killerが発動する）"
echo ""

# 安全なテスト: 制限内でのメモリ確保
bash -c '
  echo $$ > /sys/fs/cgroup/vps-a/cgroup.procs
  # 32MBを確保（制限内）
  python3 -c "
data = bytearray(32 * 1024 * 1024)  # 32MB
print(f\"32MB確保成功: VPS-Aのメモリ制限(64MB)内\")
"
'

echo ""
echo "--- 制限超過テスト ---"
bash -c '
  echo $$ > /sys/fs/cgroup/vps-a/cgroup.procs
  # 128MBを確保しようとする（制限超過）
  python3 -c "
try:
    data = bytearray(128 * 1024 * 1024)  # 128MB
    print(\"128MB確保成功（想定外）\")
except MemoryError:
    print(\"MemoryError: メモリ制限(64MB)を超過。確保に失敗\")
    print(\"→ これがVPSのメモリプラン制限の実体\")
" 2>/dev/null
' 2>/dev/null

echo ""
echo "=== メモリ制限テスト完了 ==="
```

### 演習3：「Noisy Neighbor」の再現——共有ホスティングの問題を体感する

共有ホスティングでリソース制限が不十分な場合に何が起きるかを実験する。

```bash
echo ""
echo "=== 演習3: Noisy Neighbor問題の再現 ==="
echo ""

# 制限なしの環境（共有ホスティング相当）で1つのプロセスがCPUを占有
echo "--- シナリオA: リソース制限なし（共有ホスティング相当）---"
echo "「隣人」がCPUを占有し、他のテナントに影響する"
echo ""

# 「善良なテナント」の基準性能を測定
echo "基準測定: bcで10000回の計算を実行"
TIME_ALONE=$(bash -c '
  start=$(date +%s%N)
  echo "scale=10; s(1)" | bc -l > /dev/null 2>&1
  for i in $(seq 1 100); do echo "scale=10; a(1)*4" | bc -l > /dev/null 2>&1; done
  end=$(date +%s%N)
  echo $(( (end - start) / 1000000 ))
')
echo "単独実行時間: ${TIME_ALONE}ms"
echo ""

# 「うるさい隣人」がCPUを占有する中で同じ処理を実行
echo "「うるさい隣人」がCPUを100%使用中..."
stress-ng --cpu 2 --timeout 15s &
NOISY_PID=$!
sleep 2

TIME_NOISY=$(bash -c '
  start=$(date +%s%N)
  for i in $(seq 1 100); do echo "scale=10; a(1)*4" | bc -l > /dev/null 2>&1; done
  end=$(date +%s%N)
  echo $(( (end - start) / 1000000 ))
')
echo "Noisy Neighbor存在時: ${TIME_NOISY}ms"
kill $NOISY_PID 2>/dev/null
wait $NOISY_PID 2>/dev/null

echo ""
echo "--- シナリオB: cgroup制限あり（VPS相当）---"
echo "「隣人」のCPU使用を制限し、影響を最小化"
echo ""

# Noisy Neighborをcgroupで制限
mkdir -p /sys/fs/cgroup/noisy-tenant
echo 20 > /sys/fs/cgroup/noisy-tenant/cpu.weight  # 低い重みに制限

stress-ng --cpu 2 --timeout 15s &
NOISY_PID2=$!
echo $NOISY_PID2 > /sys/fs/cgroup/noisy-tenant/cgroup.procs
sleep 2

TIME_ISOLATED=$(bash -c '
  start=$(date +%s%N)
  for i in $(seq 1 100); do echo "scale=10; a(1)*4" | bc -l > /dev/null 2>&1; done
  end=$(date +%s%N)
  echo $(( (end - start) / 1000000 ))
')
echo "cgroup制限下のNoisy Neighbor存在時: ${TIME_ISOLATED}ms"
kill $NOISY_PID2 2>/dev/null
wait $NOISY_PID2 2>/dev/null

echo ""
echo "=== 結果比較 ==="
echo "  単独実行:           ${TIME_ALONE}ms"
echo "  Noisy Neighbor:     ${TIME_NOISY}ms（制限なし=共有ホスティング相当）"
echo "  cgroup制限あり:     ${TIME_ISOLATED}ms（VPS相当）"
echo ""
echo "→ cgroupsによるリソース制限が、Noisy Neighbor問題を緩和する"
echo "  これがVPSが共有ホスティングより信頼性が高い技術的理由"
echo ""
echo "=== 演習完了 ==="
```

### この演習で何がわかるか

**第一に、cgroupsはリソースの「切り売り」の技術的基盤である。** VPS事業者は1台の物理サーバのCPU、メモリ、ストレージをcgroupsで区切り、プランごとに異なるリソース量を割り当てる。月額1,000円のプランと5,000円のプランの違いは、このcgroupsのパラメータの違いにすぎない。

**第二に、メモリ制限はハード制限として機能する。** VPSのメモリプランが2GBなら、プロセスが2GBを超えてメモリを確保しようとした時点でOOM Killerが発動するか、アロケーションが失敗する。「メモリ2GB」とは、物理的なメモリの量ではなく、cgroupsが設定する上限値だ。

**第三に、Noisy Neighbor問題はリソース隔離の有無で決まる。** 共有ホスティングのように隔離がない環境では、1つのテナントの暴走が全体に影響する。VPSのようにcgroupsでリソースを制限すれば、暴走の影響を封じ込められる。この隔離の度合いが、共有ホスティング、VPS、専用サーバ、クラウドの本質的な差異を生んでいる。

ハンズオンの詳細な手順と自動セットアップスクリプトは、本リポジトリの `handson/cloud-history/05-hosting-services/` に用意してある。

---

## 5. まとめと次回予告

### この回のまとめ

第5回では、ホスティングサービス——サーバそのものを借りるというモデル——の進化を追った。

**共有ホスティングは「Web上の存在」を民主化した。** Apache HTTP Serverの仮想ホスト機能とcPanelに代表される管理ツールが、月額数百円でWebサイトを公開できる世界を実現した。だが、リソースの隔離が不十分な共有環境は「Noisy Neighbor」問題を抱えていた。

**専用サーバホスティングは「サーバ調達」の問題を解決した。** さくらインターネット（1996年サービス開始）やRackspace（1996年創業）に代表される事業者は、物理サーバの調達・設置・ハードウェア障害対応を引き受け、顧客の負担を大幅に軽減した。だが、固定スペック、手動スケーリング、長期契約という制約は残った。

**VPSは「安価な疑似専用環境」を実現した。** FreeBSD jail（2000年）、Virtuozzo（2001年）、Linode（2003年）、DigitalOcean（2011年）と続くVPSの系譜は、1台のサーバのリソースを仮想的に分割し、共有ホスティングの価格帯で専用サーバに近い隔離を提供した。cgroupsとnamespaceがこの「リソースの切り売り」の技術的基盤だ。

**だが「弾力性」は実現されなかった。** 共有ホスティングも専用サーバもVPSも、スペックは固定的であり、需要に応じて動的にリソースを増減する仕組みは持たなかった。「必要なときに必要なだけ、不要になったら即座に返す」——この発想が実現するには、サーバの概念そのものの再定義が必要だった。

冒頭の問いに答えよう。「サーバそのものを借りるというモデルは、何を変えたのか？」——ホスティングは、サーバの調達と物理的な運用を顧客の手から切り離し、インフラの「利用」に専念できる環境を作った。「何が足りなかったのか？」——弾力性だ。需要の変動に応じてリソースを瞬時に増減するメカニズムが、ホスティングの世界には存在しなかった。この「足りなかったもの」が、クラウドコンピューティングへの原動力となる。

### 次回予告

第6回では、「VMwareの革命——一台の物理マシンに複数のOSを走らせる」を探る。

ホスティングサービスはサーバの「調達」を解決した。だが、サーバは依然として「固定スペックの物理的な箱」だった。この前提を覆したのが、仮想化技術だ。1998年に創業したVMwareは、x86アーキテクチャ上での完全仮想化を商用化し、「1台の物理マシンで複数のOSを同時に動かす」という世界を切り拓いた。VMware Workstation（1999年）からESX Server（2001年）、そしてvMotion（2003年）へ——仮想化技術がサーバ統合を実現し、計算資源を物理的な制約から解放していく過程を追う。

私がVMwareに初めて触れたときの衝撃を、今でも覚えている。LinuxとWindowsが1台のマシンの上で同時に動いている。2つのOSが、1つのCPU、1つのメモリ空間を、気づかないうちに分け合っている。「これは、サーバの概念が変わる」——その直感は正しかった。仮想化なくして、クラウドは生まれなかった。

---

## 参考文献

- さくらインターネット, "沿革". <https://www.sakura.ad.jp/corporate/corp/history/>
- Wikipedia, "Rackspace Technology". <https://en.wikipedia.org/wiki/Rackspace_Technology>
- Wikipedia, "Virtuozzo (company)". <https://en.wikipedia.org/wiki/Virtuozzo_(company)>
- Wikipedia, "Linode". <https://en.wikipedia.org/wiki/Linode>
- Wikipedia, "DigitalOcean". <https://en.wikipedia.org/wiki/DigitalOcean>
- Wikipedia, "IBM Cloud (SoftLayer)". <https://en.wikipedia.org/wiki/SoftLayer>
- Wikipedia, "cgroups". <https://en.wikipedia.org/wiki/Cgroups>
- Wikipedia, "LXC". <https://en.wikipedia.org/wiki/LXC>
- Wikipedia, "FreeBSD jail". <https://en.wikipedia.org/wiki/FreeBSD_jail>
- Wikipedia, "Apache HTTP Server". <https://en.wikipedia.org/wiki/Apache_HTTP_Server>
- Wikipedia, "GeoCities". <https://en.wikipedia.org/wiki/GeoCities>
- Wikipedia, "cPanel". <https://en.wikipedia.org/wiki/CPanel>
- Apache Software Foundation, "Name-based Virtual Host Support". <https://httpd.apache.org/docs/2.4/vhosts/name-based.html>
