# UNIXという思想

## ——パイプ、プロセス、ファイル――すべてはここから始まった

### 第7回：「テキストストリーム——万能インタフェースとしてのテキスト」

**連載「UNIXという思想——パイプ、プロセス、ファイル――すべてはここから始まった」**
**著：佐藤裕介（Engineers Hub株式会社 CEO / Technical Lead）**

---

**この回で学べること：**

- UNIXがバイナリではなくテキストを「共通語」に選んだ歴史的背景と設計上の動機
- ed（1969年）、sed（1973-1974年）、awk（1977年）——テキスト処理三銃士の系譜と、それぞれが担った役割の違い
- Ken Thompsonの正規表現エンジン（1968年）からPOSIX BRE/EREに至る正規表現の標準化の歴史
- テキストストリームが「万能インタフェース」として機能するための条件——人間可読性、行指向構造、暗黙の合意
- テキストストリームの限界——型情報の欠如、パース曖昧性、性能問題
- PowerShellのオブジェクトパイプラインが突きつけた問い——テキストは本当に最善の選択だったのか
- JSON、YAML、TOML——構造化テキストの進化と、UNIXの伝統との接続

---

## 1. 設定ファイルを`sed`で書き換えた日

2004年頃のことだ。私は数十台のサーバの設定ファイルを一括変更する必要に迫られていた。Apache HTTPサーバの`MaxClients`ディレクティブを、全サーバで`150`から`256`に変更する。当時はまだPuppetもChefも普及していない時代だった。構成管理といえば、シェルスクリプトとsshの組み合わせだ。

私はこう打った。

```bash
for host in $(cat servers.txt); do
  ssh "$host" "sed -i 's/MaxClients 150/MaxClients 256/' /etc/httpd/conf/httpd.conf && systemctl restart httpd"
done
```

`sed`——stream editor。テキストのストリームを受け取り、パターンに一致する部分を変換し、結果を出力する。この場合、`MaxClients 150`という文字列を`MaxClients 256`に置換している。設定ファイルがテキストであること、設定項目が決まったパターンで記述されていること——この二つの前提が、`sed`による一括変更を可能にしている。

この作業を終えたとき、私はふと考えた。なぜこれが可能なのだろう。

Apacheの設定ファイルはテキストだ。`MaxClients 150`という文字列がそこにある。だから`sed`で書き換えられる。だがもしApacheの設定がバイナリ形式——たとえばWindowsレジストリのような構造——で格納されていたらどうだろう。専用の管理ツールが必要になる。`sed`は使えない。`grep`で検索もできない。`diff`で差分を確認することもできない。

UNIXのツール群が強力なのは、それぞれのツールが優秀だからだけではない。ツールが扱うデータが「テキスト」であるという共通の前提があるからだ。設定ファイルがテキスト、ログがテキスト、プロセス情報（/proc）がテキスト、ネットワーク設定がテキスト。あらゆるデータがテキストで表現されているから、`cat`で読め、`grep`で検索でき、`sed`で変換でき、`awk`で集計できる。

Doug McIlroyはこう述べた。「テキストストリームを扱うプログラムを書け、それが万能インタフェースだからだ（Write programs to handle text streams, because that is a universal interface）」。この定式化は1994年にPeter Salusが『A Quarter Century of UNIX』で広めたものだが、その思想はUNIXの最初期から暗黙の前提として存在していた。

だが「万能インタフェース」とは何だろう。テキストは本当に万能なのか。なぜバイナリではなくテキストだったのか。そしてその選択は、50年以上経った今も正しいと言えるのか。

---

## 2. テキスト処理文化の形成——ed、sed、awk

### 正規表現の誕生——1968年のCACM論文

UNIXのテキスト処理文化を語るには、まず正規表現から始めなければならない。

1968年6月、Ken Thompsonは「Regular Expression Search Algorithm」と題した論文をCommunications of the ACM（Vol. 11, No. 6）に発表した。正規表現のパターンをNFA（非決定性有限オートマトン）に変換し、テキスト検索を行うアルゴリズムの記述だ。Thompsonはこのアルゴリズムを、IBM 7094マシンコードへのJITコンパイル（just-in-time compilation）として実装した——これはJITコンパイルの初期の重要な事例としても知られている。

この正規表現エンジンの実装は、ThompsonがUCバークレー時代に開発したQEDテキストエディタに由来する。QEDはテキスト中の文字列パターンを正規表現で指定し、検索や置換を行う機能を持っていた。Thompsonはこの機能を後にed、そしてgrepへと受け継がせた。

正規表現がテキスト処理の基盤技術となった背景には、UNIXが「テキストを扱う」ことを根本的な設計方針としていた事実がある。テキストを処理するためにはパターンマッチが必要だ。パターンマッチには形式的な記法が必要だ。その形式的記法が正規表現だった。

### ed——すべての始まり（1969年）

1969年、Ken ThompsonはPDP-7上でUNIXの一部としてedを実装した。UNIXの最初の三要素——アセンブラ、エディタ、シェル——のうちの一つであり、Thompsonが約一週間でこれらを書き上げたとされる。

edは行指向のテキストエディタだ。画面にテキスト全体を表示するのではなく、行番号を指定して特定の行を表示し、編集する。この設計は、当時のハードウェア制約——テレタイプ端末は1行ずつしか出力できない——から必然的に生まれたものだ。

```
# edの基本操作
$ ed
a                    # 追加モードに入る
Hello, world.
This is ed.
.                    # 追加モードを終了
1,2p                 # 1行目から2行目を表示
Hello, world.
This is ed.
1s/Hello/Goodbye/    # 1行目の "Hello" を "Goodbye" に置換
1p                   # 1行目を表示
Goodbye, world.
```

edの置換コマンド`s/pattern/replacement/`は、今日に至るまでテキスト処理の基本構文として生き続けている。`sed`の`s`コマンド、Vimの`:s`コマンド、Perlの`s///`演算子——すべてがedの構文に遡る。

edが確立したのは、テキストを「行の集合」として扱い、パターンマッチで特定の行を選択し、変換を適用するという処理モデルだ。このモデルは、UNIXのテキスト処理文化の原型となった。

### sed——ストリームへの拡張（1973-1974年）

edは対話的なエディタだった。ユーザがコマンドを入力し、edがそれに応答する。だがテキスト処理を自動化するには、対話的でない——つまり人間の介入なしに動く——テキスト変換ツールが必要だった。

1973年から1974年にかけて、Bell LabsのLee E. McMahonがsed（stream editor）を開発した。McMahonのsed開発には興味深い背景がある。McMahonはBob Morrisと共同で、連邦主義者論文（Federalist Papers）の著者判定という統計的テキスト分析に取り組んでいた。大量のテキストに対して複雑な繰り返し編集を行う必要があり、対話的なedでは到底追いつかない。この実用的な需要が、sedという非対話的テキスト処理ツールを生んだ。

sedの革新は、edの編集コマンドをスクリプトとして記述し、テキストストリームに対してパイプラインの一部として適用できるようにしたことだ。

```
edとsedの違い:

  ed（対話型）:
    ユーザ → コマンド入力 → ed → ファイル読み書き
    人間がコマンドを打つ。一つずつ。

  sed（ストリーム型）:
    stdin → sed（スクリプト適用）→ stdout
    パイプラインの一部として自動実行。人間の介入なし。
```

edからsedへの進化は、テキスト処理を「対話」から「自動化」へ、「ファイル」から「ストリーム」へ転換した。この転換は、UNIXのパイプライン思想と不可分だ。テキスト処理がストリーム化されることで、パイプラインの一段として組み込めるようになった。

### awk——テキストに計算能力を加える（1977年）

edとsedはテキストの検索と変換を担った。だが、テキストデータの集計や計算——たとえば「ログファイルからIPアドレスごとのアクセス数を集計する」——には力不足だった。

1977年、Alfred V. Aho、Peter J. Weinberger、Brian W. KernighanがBell Labsでawkを設計した。名前は三人の頭文字に由来する。awkはVersion 7 Unix（1979年）に含まれた初期のツールの一つであり、パイプラインに計算能力を加えた。

awkの設計思想は、テキストを「レコード」と「フィールド」に分解し、パターンに一致するレコードに対してアクションを実行するというものだ。

```bash
# /etc/passwdの各ユーザのシェルを一覧する
awk -F: '{print $1, $7}' /etc/passwd
# root /bin/bash
# nobody /usr/sbin/nologin
# ...
```

`-F:`でフィールド区切り文字をコロンに設定し、`$1`（ユーザ名）と`$7`（シェル）を取り出す。awkは`/etc/passwd`がテキストであり、コロンでフィールドが区切られているという「暗黙の合意」を前提としている。

awkが重要なのは、UNIXパイプラインの中で「データの集計と計算」という役割を担えるプログラミング言語を提供したことだ。sedがテキスト変換のためのスクリプト言語なら、awkはテキスト処理のためのプログラミング言語だった。

### ed → sed → awk——テキスト処理三銃士の系譜

```
UNIXテキスト処理ツールの系譜:

  1968  Thompson, 正規表現エンジン (CACM論文)
    │
    ▼
  1969  ed (Ken Thompson) ── 行指向の対話的エディタ
    │                        パターンマッチと置換の原型
    │
    ├── 1973  grep (Ken Thompson) ── パターン検索の特化ツール
    │                                 "Global Regular Expression Print"
    │
    ├── 1973-74  sed (Lee McMahon) ── 非対話的ストリーム処理
    │                                  パイプラインの一段として動作
    │
    └── 1977  awk (Aho, Weinberger, Kernighan)
                                       ── レコード/フィールド分解
                                          計算能力の追加
                                          パターン-アクションモデル
```

この三つのツール——ed、sed、awk——が確立したのは、「テキストを行単位で処理し、パターンマッチで対象を選択し、変換や計算を適用する」という処理パラダイムだ。そしてこのパラダイムが機能するための暗黙の前提がある。データがテキストであること。行が区切り文字で構造化されていること。エンコーディングが共通であること。

この暗黙の前提——データはテキストである——が、UNIXの「テキストストリーム文化」の基盤だ。

---

## 3. テキストが「万能インタフェース」として機能する条件

### なぜテキストだったのか

UNIXがテキストを「共通語」に選んだ理由は、歴史的な必然と設計上の判断の両方にある。

**歴史的必然**として、1969年のUNIXが動いていたPDP-7の端末はテレタイプだった。テレタイプは紙にテキストを印字する装置であり、扱えるデータは必然的にテキストだ。UNIXの初期のI/Oモデルは、テレタイプの特性——1行ずつテキストを出力する——に適合するように設計された。行指向の入出力、改行文字による行の区切り、印字可能なASCII文字の使用——これらはすべてテレタイプという物理的制約から生まれた。

**設計上の判断**として、テキストには他のデータ形式にない特性がある。

**第一に、人間可読性**。テキストは人間が直接読める。`cat /etc/passwd`と打てば、ユーザ一覧がそのまま画面に表示される。バイナリ形式なら`xxd`や専用のパーサが必要だ。人間が読めることで、デバッグが容易になる。中間データをパイプの途中で`tee`を使ってファイルに保存し、何が流れているかを目視確認できる。

**第二に、行指向の構造化**。テキストを改行文字（`\n`）で区切ることで、データは「行の列」として構造化される。行はレコードに対応し、行内のフィールドは区切り文字（スペース、タブ、コロン、カンマなど）で分離される。この構造は単純だが、驚くほど多くのデータを表現できる。

```
テキストストリームの構造:

  ┌────────────────────────────────┐
  │  行1（レコード1）               │
  │  field1:field2:field3:field4   │
  ├────────────────────────────────┤
  │  行2（レコード2）               │
  │  field1:field2:field3:field4   │
  ├────────────────────────────────┤
  │  行3（レコード3）               │
  │  field1:field2:field3:field4   │
  └────────────────────────────────┘

  改行 (\n) がレコード区切り
  区切り文字 (:, TAB, スペース等) がフィールド区切り
```

**第三に、ツールの汎用性**。テキストが共通フォーマットであることで、同じツール群がどんなデータにも適用できる。`grep`はApacheのログもsyslogもアプリケーションログも検索できる。`sort`はどんなテキストデータでもソートできる。`wc -l`はどんなファイルでも行数を数えられる。データの意味論を知らなくても、テキストとしての操作が可能だ。

### テキストストリームの「暗黙の合意」

テキストが万能インタフェースとして機能するためには、生産者と消費者の間に「暗黙の合意」が必要だ。

`/etc/passwd`ファイルを例に取る。

```
root:x:0:0:root:/root:/bin/bash
daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin
```

この行が意味を持つのは、「フィールドはコロンで区切られ、順番にユーザ名、パスワードプレースホルダー、UID、GID、GECOS、ホームディレクトリ、シェルである」という合意があるからだ。`cut -d: -f1`が「ユーザ名を取り出す」のは、`cut`がコロン区切りの最初のフィールドを切り出すからであり、`cut`自身はそれがユーザ名であることを知らない。

この「暗黙の合意」は、UNIXのテキスト処理における最大の強みであり、同時に最大の弱点だ。

**強み**は、新しいフォーマットの定義が容易であることだ。形式的なスキーマ定義やIDLは不要だ。テキストファイルに行を書き、フィールドを区切り文字で分ければ、それで新しいデータフォーマットの出来上がりだ。UNIXの`/etc`配下の設定ファイル群——`/etc/fstab`、`/etc/hosts`、`/etc/passwd`——はそれぞれ異なるフォーマットだが、すべてテキストの「暗黙の合意」で成り立っている。

**弱点**は、合意が暗黙的であるがゆえに、破綻しやすいことだ。フィールドの順序が変わったら。区切り文字がデータ内に出現したら。エンコーディングが異なっていたら。テキストストリームには、こうした問題に対する組み込みの防御機構がない。

### /etc配下のテキスト設定ファイル文化

UNIXの設定ファイルがテキストであることは、偶然ではなく設計思想の帰結だ。

`/etc`ディレクトリには、UNIXシステムの設定ファイルが集約される。`/etc/fstab`（ファイルシステムのマウント情報）、`/etc/hosts`（ホスト名とIPアドレスの対応）、`/etc/resolv.conf`（DNSリゾルバの設定）、`/etc/crontab`（定期実行タスク）——これらはすべてプレーンテキストだ。

```bash
# /etc/fstab の例
cat /etc/fstab
# UUID=xxxx-xxxx  /       ext4  defaults  0  1
# UUID=yyyy-yyyy  /home   ext4  defaults  0  2

# /etc/hosts の例
cat /etc/hosts
# 127.0.0.1  localhost
# 192.168.1.10  webserver
```

テキストであることの実用上の利点は計り知れない。`grep`で設定を検索できる。`diff`で変更箇所を確認できる。`git`でバージョン管理できる。`sed`で一括変更できる。テキストエディタがあれば、どんな設定も編集できる。専用のGUIツールは不要だ。

対照的に、Windowsのレジストリ（バイナリ形式のデータベース）は、専用のレジストリエディタ（regedit）がなければ操作できない。`grep`で検索することも、`diff`で差分を取ることも、直接はできない。レジストリの破損はシステム全体を不安定にし、バイナリであるがゆえに手動での復旧が困難だ。

UNIXの「設定ファイルはテキスト」という方針は、「最悪の場合でも、テキストエディタがあれば何とかなる」という安心感を提供する。この安心感は、運用の現場では計り知れない価値を持つ。

---

## 4. テキストストリームの限界

### 型情報の欠如

テキストストリームの最も根本的な限界は、型情報を持たないことだ。

`/etc/passwd`の第3フィールドはUIDだ。数値だ。だがテキストストリームにとって、`0`はゼロという数値ではなく、`0`という文字だ。`1000`も数値ではなく、4文字の文字列だ。`sort`で並べると、`1000`は`2`より前に来る。辞書順でソートされるからだ。数値としてソートしたければ、`sort -n`と明示的に指定しなければならない。

```bash
# 辞書順ソート（デフォルト）
echo -e "1000\n2\n30" | sort
# 1000
# 2
# 30

# 数値順ソート（明示的な指定が必要）
echo -e "1000\n2\n30" | sort -n
# 2
# 30
# 1000
```

テキストストリームには「このフィールドは数値である」「このフィールドは日付である」「このフィールドはIPアドレスである」という型情報が埋め込まれていない。型を解釈するのは、パイプラインの各段階のツールの責任だ。そしてツールが型を間違えれば、結果は無言で壊れる。エラーは出ない。テキストは「ただの文字列」として処理され、意味論的に不正な結果が生まれる。

### パースの曖昧性

テキストストリームのパースは、しばしば曖昧だ。

```bash
# ファイル名にスペースが含まれている場合
ls -l
# -rw-r--r-- 1 user group 1024 Jan 1 12:00 my file.txt
```

`ls -l`の出力をawkでファイル名を取り出そうとすると、`$9`は`my`だけになる。`file.txt`は`$10`に分離されてしまう。スペースがフィールド区切りとファイル名の一部の両方として機能しているからだ。

同様の問題はCSV（Comma-Separated Values）でも起きる。フィールド内にカンマが含まれる場合、クォーティングが必要だ。クォート内にクォートが含まれる場合は、エスケープが必要だ。「テキストを区切り文字で分割する」という単純なモデルでは、現実のデータを正確にパースできないケースが無数にある。

### 性能の問題

テキストは人間にとっては読みやすいが、機械にとっては非効率だ。

数値`1000000`をテキストで表現すると7バイトだが、32ビット整数として表現すれば4バイトだ。比較も、テキストなら文字ごとの比較が必要だが、整数なら1回のCPU命令で完了する。大量のデータを処理する場合、テキストのパース（文字列から数値への変換）のオーバーヘッドは無視できない。

データベースがテキストではなくバイナリ形式でデータを格納するのは、この性能上の理由だ。Protocol BuffersやApache Avroなどのバイナリシリアライゼーション形式が存在するのも同じ理由だ。

テキストストリームは「小さなデータを柔軟に扱う」場面では最適だが、「大量のデータを高速に処理する」場面では、バイナリ形式に劣る。

### PowerShellの問題提起——オブジェクトパイプライン

UNIXのテキストストリームに対する最も体系的な批判は、Microsoftから来た。

2002年8月8日、Jeffrey Snoverは「Monad Manifesto」と題した文書でオブジェクトパイプラインの構想を記述した。Snoverの問題意識は明確だった。UNIXのパイプラインは強力だが、テキスト解析（parsing）が常にボトルネックになる。パイプラインの各段階で、前段の出力テキストをパースし、意味を解釈し、処理し、再びテキストとして出力する。このパース-再フォーマットの繰り返しは、本質的に無駄な作業だ。

2006年11月にリリースされたWindows PowerShell 1.0は、この問題に対する回答だった。パイプラインにテキストではなく.NETオブジェクトを流す。オブジェクトは型情報を持ち、プロパティに名前でアクセスできる。パースは不要だ。

```powershell
# PowerShell: オブジェクトパイプライン
Get-Process | Where-Object { $_.CPU -gt 100 } | Select-Object Name, CPU
# Name         CPU
# ----         ---
# chrome     245.3
# code       180.7
```

```bash
# UNIX: テキストパイプライン
ps aux | awk '$3 > 10.0 {print $11, $3}'
# /usr/bin/chrome 24.5
# /usr/bin/code 18.0
```

PowerShellの場合、`$_.CPU`はCPU使用率の「数値」だ。比較演算子`-gt`は数値比較を行う。`$_.Name`はプロセス名の「文字列」だ。型情報が保持されている。

UNIXの場合、`$3`は「3番目のフィールド」だ。それが数値であることは、awkのコード内で暗黙的に仮定されている。`ps`の出力フォーマットが変わったら——たとえばカラムの順序が変わったら——スクリプトは壊れる。

Snoverの批判は正当だった。テキストパイプラインには本質的な脆弱性がある。だがPowerShellが示したのは、オブジェクトパイプラインにもトレードオフがあるということだ。オブジェクトは型に依存する。.NETのオブジェクトモデルに依存する。異なるプラットフォームや異なる言語間でオブジェクトを共有するには、追加の仕組みが必要だ。テキストの「どんな環境でも読める」という普遍性は、オブジェクトパイプラインでは失われる。

---

## 5. 構造化テキストの進化——JSON、YAML、TOML、そしてjq

### テキストの限界を超える試み

UNIXのプレーンテキスト——行指向、区切り文字ベース——には限界がある。型情報がない。ネストした構造を表現できない。スキーマが暗黙的だ。

この限界を超えるために、さまざまな「構造化テキスト」フォーマットが生まれた。テキストの人間可読性を保ちつつ、データの構造を明示的に表現する試みだ。

**XML**（1998年、W3C勧告）は、構造化テキストの最初の大規模な試みだった。タグによるマークアップでデータ構造を明示し、DTDやXML Schemaでスキーマを定義できる。だがXMLは冗長だった。データ量に対してタグのオーバーヘッドが大きく、人間にとっての可読性は高くない。

**JSON**は、XMLへの反動として生まれた。2001年にDouglas CrockfordらがState Software社でJSONを考案し、2002年にJSON.orgで文法と実装例を公開した。JavaScriptのオブジェクトリテラル構文のサブセットとしてデータを表現するJSONは、XMLよりも遥かに簡潔だった。2006年にRFC 4627として仕様化され、2013年にECMA-404として公式標準化された。

```json
{
  "user": "root",
  "uid": 0,
  "shell": "/bin/bash",
  "groups": ["root", "wheel"]
}
```

JSONは型情報を持つ。`"uid": 0`の`0`は数値であり、`"user": "root"`の`"root"`は文字列だ。ネストした構造も表現できる。だがJSONは設定ファイルにはコメントが書けないという問題を持つ。

**YAML**は2001年にClark Evansが提案し、Ingy dot NetおよびOren Ben-Kikiと共同で設計した。インデントベースの構文で人間可読性を重視し、コメントも書ける。だがYAMLのパースは複雑であり、意図しない型変換（`yes`がブーリアン`true`に、`1.0`が浮動小数点数に）がセキュリティ上の問題を引き起こすことがある。

**TOML**は2013年にGitHub共同創業者のTom Preston-Wernerが公開した。設定ファイルに特化し、INIファイル形式の進化形として設計された。曖昧性が少なく、パースが容易だ。

```toml
[server]
host = "0.0.0.0"
port = 8080

[database]
url = "postgres://localhost/mydb"
max_connections = 20
```

これらの構造化テキストフォーマットは、UNIXのプレーンテキストの限界を部分的に克服した。型情報を持ち、ネスト構造を表現でき、スキーマが明示的だ。だが同時に、UNIXの伝統的なテキスト処理ツール——`grep`、`sed`、`awk`——との相性は悪くなった。JSONの特定のフィールドを`grep`で検索することは技術的には可能だが、ネストしたJSONの正確なパースを`grep`で行うことはできない。

### jq——構造化テキストのためのフィルタ

この問題に対する回答が、2012年にStephen Dolanがリリースしたjqだ。「JSONのためのsed」とも呼ばれるjqは、UNIXのフィルタ設計思想を構造化データの世界に持ち込んだ。

```bash
# JSONからユーザ名を抽出
echo '{"user":"root","uid":0}' | jq '.user'
# "root"

# 配列の各要素を処理
echo '[{"name":"alice","age":30},{"name":"bob","age":25}]' | jq '.[] | select(.age > 28) | .name'
# "alice"
```

jqはJSONの構造を理解し、フィールドに名前でアクセスし、型を保持したまま変換を行う。`jq '.age > 28'`は数値比較であり、文字列比較ではない。awkの`$3 > 10.0`とは異なり、フィールドの位置ではなく名前で指定する。出力フォーマットの変更に対して頑健だ。

jqの設計は、UNIXのフィルタモデル——stdin → 変換 → stdout——を踏襲している。パイプラインの一段として機能し、他のUNIXツールと組み合わせられる。だが扱うデータはプレーンテキストではなく構造化されたJSONだ。

```bash
# UNIXの伝統的パイプラインとjqの併用
curl -s https://api.example.com/users | jq '.[] | .name' | sort | uniq -c | sort -rn
```

この例は興味深い。APIからJSONを取得し（curl）、jqで名前を抽出し（jq）、ソートして（sort）、重複をカウントし（uniq）、降順に並べる（sort）。jqがJSONの構造化データをプレーンテキスト（名前の文字列、1行1件）に変換することで、後段のUNIXツール群がそのまま使える。jqは、構造化データの世界とUNIXのテキストストリームの世界を橋渡しする存在だ。

### テキストストリームは死んだのか

構造化テキスト（JSON、YAML、TOML）とそのためのツール（jq）の登場は、UNIXの伝統的なテキストストリームを時代遅れにしたのだろうか。

私はそう思わない。

まず、UNIXの伝統的なテキスト処理は、ログ解析、設定ファイルの操作、テキストデータの加工という日常的なタスクにおいて、今も最も効率的なアプローチだ。`grep ERROR /var/log/syslog | wc -l`——エラー行を数えるのに、JSONパーサは不要だ。

次に、構造化テキストフォーマット自身がテキストである。JSONもYAMLもTOMLも、テキストエディタで開いて読め、`diff`で差分を取れ、`git`でバージョン管理できる。テキストの「人間可読性」と「ツールの汎用性」は、構造化テキストにも継承されている。

そして、jqの設計が示しているように、UNIXのフィルタモデル——stdin → 変換 → stdout——は、扱うデータの形式がテキストからJSONに変わっても有効だ。パイプラインの思想は、データ形式に依存しない。

テキストストリームは死んでいない。進化している。プレーンテキストの上に構造化テキストが載り、その上にjqのような構造化フィルタが載る。UNIXの「テキストは万能インタフェース」という原則は、「テキストベースのデータは万能インタフェース」に拡張されつつある。

---

## 6. ハンズオン：テキスト処理の系譜を手で辿る

ここからは手を動かす。sed、awk、正規表現を使ったテキスト処理パイプラインを実際に構築し、次にjqでJSONを扱い、両者のアプローチの違いを体感する。

### 環境構築

```bash
docker run -it --rm ubuntu:24.04 bash
```

コンテナ内で必要なツールを用意する。

```bash
apt-get update && apt-get install -y jq curl gawk
```

### 演習1：sed——テキストストリームの変換

sedの基本操作を確認する。edの置換コマンド`s/pattern/replacement/`がストリーム上で動く様子を体感する。

```bash
# サンプルの設定ファイルを作成
cat << 'EOF' > /tmp/httpd.conf
ServerRoot "/etc/httpd"
Listen 80
MaxClients 150
ServerName www.example.com
DocumentRoot "/var/www/html"
MaxClients 150
EOF

# sedで設定値を一括変更
sed 's/MaxClients 150/MaxClients 256/' /tmp/httpd.conf
```

出力を確認すると、2箇所の`MaxClients 150`が両方とも`MaxClients 256`に変更されている。sedは各行を順番に処理し、パターンに一致すれば置換を適用する。ファイルの構造を「知る」必要はない。テキストパターンだけで操作する。

```bash
# 複数の変換を組み合わせる
sed -e 's/Listen 80/Listen 8080/' \
    -e 's/MaxClients 150/MaxClients 256/' \
    -e '/^#/d' \
    /tmp/httpd.conf
```

`-e`で複数の編集コマンドを指定できる。`/^#/d`はコメント行（#で始まる行）を削除する。sedはパイプラインの一段として、テキストを受け取り、変換し、出力する。

### 演習2：awk——テキストデータの集計

awkのフィールド分解と集計能力を使い、テキストデータを分析する。

```bash
# サンプルのアクセスログを作成
cat << 'EOF' > /tmp/access.log
192.168.1.10 - - [01/Jan/2026:10:00:01] "GET /index.html HTTP/1.1" 200 1234
192.168.1.20 - - [01/Jan/2026:10:00:02] "GET /about.html HTTP/1.1" 200 5678
192.168.1.10 - - [01/Jan/2026:10:00:03] "POST /api/data HTTP/1.1" 201 90
192.168.1.30 - - [01/Jan/2026:10:00:04] "GET /index.html HTTP/1.1" 200 1234
192.168.1.10 - - [01/Jan/2026:10:00:05] "GET /style.css HTTP/1.1" 200 456
192.168.1.20 - - [01/Jan/2026:10:00:06] "GET /index.html HTTP/1.1" 304 0
192.168.1.10 - - [01/Jan/2026:10:00:07] "GET /favicon.ico HTTP/1.1" 404 0
192.168.1.30 - - [01/Jan/2026:10:00:08] "GET /about.html HTTP/1.1" 200 5678
EOF

# IPアドレスごとのアクセス数を集計
awk '{count[$1]++} END {for (ip in count) print ip, count[ip]}' /tmp/access.log | sort -k2 -rn
```

awkは第1フィールド（`$1`、IPアドレス）をキーとして連想配列にカウントを蓄積し、最後に結果を出力する。`sort -k2 -rn`で数値順（降順）にソートする。

```bash
# HTTPステータスコードごとの集計
awk '{status[$9]++} END {for (s in status) print s, status[s]}' /tmp/access.log | sort

# レスポンスサイズの合計をIPアドレスごとに計算
awk '{bytes[$1] += $10} END {for (ip in bytes) print ip, bytes[ip]}' /tmp/access.log | sort -k2 -rn
```

awkはテキストの「第9フィールドがHTTPステータスコードである」「第10フィールドがレスポンスサイズである」という暗黙の合意に依存している。この合意が正しい限り、awkは強力だ。だがログフォーマットが変われば——たとえばフィールドが追加されれば——スクリプトは壊れる。

### 演習3：正規表現——パターンマッチの威力

grepとsedで正規表現を使い、テキストの構造をパターンで捉える。

```bash
# /etc/passwdからシェルが/bin/bashのユーザを抽出
grep ':/bin/bash$' /etc/passwd | cut -d: -f1

# IPアドレスのパターンマッチ（基本正規表現）
grep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}' /tmp/access.log

# sedで正規表現を使った複雑な変換
# ログからタイムスタンプを抽出してフォーマットを変更
sed -n 's/.*\[\([^]]*\)\].*/\1/p' /tmp/access.log
```

`-E`はERE（Extended Regular Expressions）を使用する指定だ。1992年のPOSIX.2（IEEE 1003.2）でBRE（Basic Regular Expressions、ed/grep系）とERE（egrep系）が標準化された。BREでは`()`をエスケープする必要があるが、EREでは不要だ。

```bash
# BRE（Basic Regular Expressions）の例
echo "2026-01-15" | grep '\([0-9]\{4\}\)-\([0-9]\{2\}\)-\([0-9]\{2\}\)'

# ERE（Extended Regular Expressions）の例（同じパターン）
echo "2026-01-15" | grep -E '([0-9]{4})-([0-9]{2})-([0-9]{2})'
```

BREとEREの違いは、正規表現の標準化における「後方互換性」と「使いやすさ」のトレードオフだ。BREは歴史的なedやgrepとの互換性を維持し、EREはより自然な記法を提供する。

### 演習4：jq——構造化テキストの処理

同じデータをJSON形式で扱い、jqで処理する。テキストパイプラインとの違いを体感する。

```bash
# アクセスログをJSON形式に変換
cat << 'EOF' > /tmp/access.json
[
  {"ip":"192.168.1.10","path":"/index.html","method":"GET","status":200,"bytes":1234},
  {"ip":"192.168.1.20","path":"/about.html","method":"GET","status":200,"bytes":5678},
  {"ip":"192.168.1.10","path":"/api/data","method":"POST","status":201,"bytes":90},
  {"ip":"192.168.1.30","path":"/index.html","method":"GET","status":200,"bytes":1234},
  {"ip":"192.168.1.10","path":"/style.css","method":"GET","status":200,"bytes":456},
  {"ip":"192.168.1.20","path":"/index.html","method":"GET","status":304,"bytes":0},
  {"ip":"192.168.1.10","path":"/favicon.ico","method":"GET","status":404,"bytes":0},
  {"ip":"192.168.1.30","path":"/about.html","method":"GET","status":200,"bytes":5678}
]
EOF

# jqでIPアドレスごとのアクセス数を集計
jq 'group_by(.ip) | map({ip: .[0].ip, count: length}) | sort_by(-.count)' /tmp/access.json
```

jqの出力は構造化されたJSONだ。フィールド名でアクセスし（`.ip`）、型を保持したまま集計する（`length`は数値）。awkの`$1`のような位置依存はない。

```bash
# ステータスコード別の集計
jq 'group_by(.status) | map({status: .[0].status, count: length})' /tmp/access.json

# 条件フィルタ: 200以外のステータスだけ抽出
jq '.[] | select(.status != 200)' /tmp/access.json

# IPアドレスごとのバイト数合計
jq 'group_by(.ip) | map({ip: .[0].ip, total_bytes: (map(.bytes) | add)})' /tmp/access.json
```

### 演習5：テキストとJSONの橋渡し

jqと伝統的なUNIXツールを組み合わせ、両者の世界を橋渡しする。

```bash
# jqの出力をテキストに変換してUNIXツールで処理
jq -r '.[] | "\(.ip) \(.status) \(.bytes)"' /tmp/access.json | \
  awk '{bytes[$1] += $3} END {for (ip in bytes) print ip, bytes[ip]}' | \
  sort -k2 -rn

# テキストデータをjqで構造化
awk '{print "{\"ip\":\""$1"\",\"status\":"$9",\"bytes\":"$10"}"}' /tmp/access.log | \
  jq -s 'group_by(.ip) | map({ip: .[0].ip, count: length})'
```

`jq -r`（raw output）はJSON文字列のクォートを除去し、プレーンテキストとして出力する。このオプションが、jqとUNIXのテキスト処理ツールをつなぐ鍵だ。jqが構造化データの世界で処理を行い、結果をプレーンテキストとして出力することで、後段の`sort`や`awk`がそのまま使える。

---

## 7. まとめと次回予告

### この回の要点

- UNIXのテキスト処理文化は、1968年のKen Thompsonの正規表現エンジン（CACM論文）に始まり、ed（1969年）、sed（1973-1974年、Lee McMahon）、awk（1977年、Aho, Weinberger, Kernighan）という系譜で発展した。edが確立した「行指向のパターンマッチと変換」というモデルは、sedでストリーム化され、awkで計算能力が加わった

- テキストが「万能インタフェース」として機能するのは、三つの条件による。人間可読性（目視でデバッグできる）、行指向の構造化（改行でレコード、区切り文字でフィールド）、ツールの汎用性（同じツール群であらゆるテキストデータを処理できる）。これらの条件が、/etc配下の設定ファイル群からログファイルまで、UNIXのあらゆるデータに一貫して適用される

- テキストストリームの限界は明確だ。型情報の欠如（数値と文字列の区別がない）、パースの曖昧性（区切り文字がデータ内に出現する問題）、性能の問題（テキストのパースはバイナリ処理より遅い）。2002年にJeffrey SnoverがMonad Manifestoで提起したオブジェクトパイプラインは、これらの限界への体系的な批判であり、2006年のPowerShell 1.0として結実した

- 構造化テキスト——JSON（2001年、Douglas Crockford）、YAML（2001年、Clark Evans）、TOML（2013年、Tom Preston-Werner）——は、テキストの人間可読性を保ちつつ、型情報とネスト構造を加えた。2012年にStephen Dolanがリリースしたjqは、UNIXのフィルタモデル（stdin → 変換 → stdout）を構造化データの世界に持ち込み、テキストストリームと構造化データの橋渡し役を担っている

- テキストストリームは「不完全な万能インタフェース」だ。だがその不完全さゆえに柔軟であり、50年以上にわたって機能し続けている。構造化テキストの進化は、テキストストリームの「死」ではなく「拡張」だ。UNIXの「テキストは万能インタフェース」という原則は、データの形式が進化しても、その根底にある思想——人間が読めるデータを、小さなツールの組み合わせで処理する——として生き続けている

### 冒頭の問いへの暫定回答

「なぜUNIXはバイナリではなくテキストを『共通語』に選んだのか？ その選択は正しかったのか？」

暫定的な答えはこうだ。テキストが選ばれたのは、テレタイプという物理的制約と、「人間が読めるデータ」という設計思想の合流点だった。テキストは人間にとって読みやすく、汎用ツールで操作でき、新しいフォーマットの定義が容易だ。バイナリ形式はこれらの性質を持たない。

その選択は正しかったのか。部分的に正しかった、と私は考える。テキストストリームは、設定ファイルの操作、ログの解析、データの加工という日常的なタスクにおいて、今も最も効率的なアプローチだ。だが大量のデータの高速処理、型安全なデータ変換、複雑な構造のデータ表現においては、テキストでは不十分だ。

重要なのは、UNIXが「テキスト以外を排除した」のではなく、「テキストをデフォルトにした」ということだ。デフォルトが適切であれば、ほとんどの場面でうまく機能する。そしてテキストは、ほとんどの場面で「十分に有用」だった。完璧ではないが、十分に有用——前回「すべてはファイルである」の結論と同じだ。UNIXの設計原則は、完璧さではなく「十分な実用性」を選び続けている。

あなたのシステムでは、プログラム間のデータ交換に何を使っているだろうか。JSONだろうか。Protocol Buffersだろうか。それとも独自のバイナリ形式だろうか。その選択の根拠は何だろうか。人間可読性か。性能か。型安全性か。UNIXが「テキスト」を選んだ理由を理解した上で、あなた自身の選択を再検討してみてほしい。

### 次回予告

次回は「小さなツールの組み合わせ——合成可能性の設計」。UNIXコマンドが「組み合わせて使える」のは、偶然ではなく設計の結果だ。stdin/stdout/stderrの統一的なインタフェース、テキスト行指向の入出力、終了コードによるエラー伝播、副作用の最小化——合成可能性（composability）を実現するための設計条件を掘り下げる。

Brian Kernighanの「Software Tools」（1976年）が提唱した「ツールボックスアプローチ」の思想を辿り、シェルが「接着剤」として果たす役割を考える。そして、合成可能性という概念が、関数型プログラミングのモナドやマイクロサービスのAPI設計と、どこで交差するのかを探る。

`cat file | grep pattern | sort | uniq -c | sort -rn | head -10`——このパイプラインが「当たり前」に動くために、個々のツールはどう設計されているのか。「組み合わせ可能」であるための条件とは何か。次回は、この問いに正面から向き合う。

---

## 参考文献

- Ken Thompson, "Programming Techniques: Regular expression search algorithm", Communications of the ACM, Vol. 11, No. 6, June 1968: <https://dl.acm.org/doi/10.1145/363347.363387>
- ed (text editor) - Wikipedia: <https://en.wikipedia.org/wiki/Ed_(text_editor)>
- Dennis Ritchie, "An incomplete history of the QED Text Editor", Nokia Bell Labs: <https://www.bell-labs.com/usr/dmr/www/qed.html>
- Lee E. McMahon, "SED — A Non-interactive Text Editor", Bell Laboratories: <https://wolfram.schneider.org/bsd/7thEdManVol2/sed/sed.pdf>
- sed - Wikipedia: <https://en.wikipedia.org/wiki/Sed>
- AWK - Wikipedia: <https://en.wikipedia.org/wiki/AWK>
- History (The GNU Awk User's Guide): <https://www.gnu.org/software/gawk/manual/html_node/History.html>
- Alfred V. Aho, Brian W. Kernighan, Peter J. Weinberger, "The AWK Programming Language", Addison-Wesley, 1988
- Peter H. Salus, "A Quarter Century of UNIX", Addison-Wesley, 1994
- Unix philosophy - Wikipedia: <https://en.wikipedia.org/wiki/Unix_philosophy>
- Doug McIlroy - Wikiquote: <https://en.wikiquote.org/wiki/Doug_McIlroy>
- IEEE Std 1003.2-1992 (POSIX.2): <https://pubs.opengroup.org/onlinepubs/9699919799/xrat/V4_xbd_chap09.html>
- Jeffrey Snover, "Monad Manifesto", August 8, 2002: <https://www.jsnover.com/Docs/MonadManifesto.pdf>
- PowerShell - Wikipedia: <https://en.wikipedia.org/wiki/PowerShell>
- JSON - Wikipedia: <https://en.wikipedia.org/wiki/JSON>
- YAML - Wikipedia: <https://en.wikipedia.org/wiki/YAML>
- TOML - Wikipedia: <https://en.wikipedia.org/wiki/TOML>
- jq (programming language) - Wikipedia: <https://en.wikipedia.org/wiki/Jq_(programming_language)>
- Brian W. Kernighan, Rob Pike, "The UNIX Programming Environment", Prentice Hall, 1984
- Eric S. Raymond, "The Art of UNIX Programming", Addison-Wesley, 2003
