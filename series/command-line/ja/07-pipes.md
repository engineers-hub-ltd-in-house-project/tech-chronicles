# ターミナルは遺物か

## ――コマンドラインの本質を問い直す

### 第7回：パイプの発明――1973年1月のコンピュータサイエンス史

**連載「ターミナルは遺物か――コマンドラインの本質を問い直す」**
**著：佐藤裕介（Engineers Hub株式会社 CEO / Technical Lead）**

---

**この回で学べること：**

- Doug McIlroyの1964年10月11日付メモ――「庭のホース」の比喩が示したソフトウェア部品の接続構想
- Ken Thompsonが1973年1月15日の一夜で実装したpipeシステムコール――その技術的背景と設計判断
- UNIXパイプの内部実装――カーネルバッファ、ファイルディスクリプタ、プロセス間通信の仕組み
- stdin/stdout/stderrの設計思想――テキストストリームが「ユニバーサルインターフェース」となった経緯
- パイプ記法の変遷――`>`から`|`への移行とその理由
- パイプなしの世界とパイプありの世界を体験するハンズオン

---

## 1. 一行の魔法

2000年代前半、私はWebサーバの運用を任されていた。Apache HTTP Serverのアクセスログは毎日数十万行に膨れ上がる。ある日、「404エラーを返しているURLの上位20件を出せ」と頼まれた。

当時の私は、まだUNIXのコマンドラインに習熟しきっていなかった。最初に思いついたのは、Perlスクリプトを書くことだった。ファイルを開き、一行ずつ読み、正規表現で404を含む行を抽出し、URLをハッシュに格納し、出現回数で降順ソートし、上位20件を出力する。30行ほどのスクリプトだ。動く。だが、次に「500エラーの上位も出してくれ」と言われたら、また書き直さなければならない。

先輩エンジニアが横から手を伸ばし、ターミナルに一行を打った。

```bash
cat access.log | grep ' 404 ' | awk '{print $7}' | sort | uniq -c | sort -rn | head -20
```

数秒で結果が表示された。数十万行のログが、一行のコマンドで分析された。500エラーに変えたければ`404`を`500`に書き換えるだけだ。

私は衝撃を受けた。30行のスクリプトが、一行のパイプラインで置き換えられた。だがそれは単なる「短さ」の問題ではなかった。このワンライナーの中では、六つの独立したプログラムが連携している。`cat`はファイルを読む。`grep`は行を絞り込む。`awk`は列を抽出する。`sort`は並べ替える。`uniq -c`は重複を数える。`head`は先頭を切り出す。それぞれが「一つのこと」だけをやり、パイプ（`|`）で繋がれることで、単体では不可能な分析を実現している。

この「小さなプログラムを組み合わせる」という思想は、どこから来たのか。パイプという仕組みは、誰が、いつ、なぜ発明したのか。その答えは、1964年のBell Labsの一枚のメモと、1973年1月の一夜にある。

---

## 2. 1964年10月11日――庭のホースの比喩

### McIlroyのメモ

1964年10月11日、Bell Labsの研究者M. Douglas McIlroyは、内部メモの中で次のように書いた。

> "We should have some ways of connecting programs like garden hose--screw in another segment when it becomes necessary to massage data in another way."
>
> （プログラム同士を庭のホースのように接続する方法があるべきだ――データを別の方法で加工する必要が出たら、新しいセグメントをねじ込めばいい。）

このメモはタイプライターで打たれ、Bell Labsの内部文書として回覧された。Dennis Ritchieは後年、このメモの10ページ目を自室のオフィスの壁に磁石で貼っていた。Ritchieはこのメモを"Prophetic Petroglyphs"（予言的な岩絵）と呼んだ。

McIlroyの着想は、1964年当時のBell Labsの計算環境から生まれたものだ。当時のBell Labsのコンピューティングは、主にIBM 7090や7094でバッチ処理として行われていた。プログラムは独立した単位として設計され、データの受け渡しは一時ファイルを介していた。プログラムAの出力をファイルに書き、そのファイルをプログラムBの入力として読み込む。単純だが、煩雑だった。

McIlroyが「庭のホース」の比喩で表現したのは、この煩雑さの解消だった。プログラムの出力を一時ファイルに落とさず、直接次のプログラムの入力に流し込む。ホースのセグメントを繋ぐように、プログラムを連結する。一時ファイルの作成も削除も不要になる。

だが、1964年の時点ではこの構想は実現されなかった。UNIXはまだ存在せず（UNIXの開発が始まるのは1969年）、McIlroyの構想は数年間、概念のまま眠ることになる。

### なぜ9年かかったのか

McIlroyの構想からパイプの実装まで、約9年の歳月が流れている。なぜこれほど時間がかかったのか。

理由の一つは、パイプを実現するにはOSレベルの支援が必要だったからだ。プログラムの出力を別のプログラムの入力に直接接続するには、OSがプロセス間のデータ転送を管理しなければならない。1964年時点のBell Labsの計算環境――バッチ処理が主流のメインフレーム――では、そもそも複数のプログラムを同時に実行すること自体が一般的ではなかった。

もう一つの理由は、UNIXの設計が段階的に成熟していったことだ。1969年にKen ThompsonとDennis RitchieがUNIXの開発を始めたとき、最初の課題はファイルシステムとプロセス管理だった。パイプの実装には、プロセス間通信の仕組み、ファイルディスクリプタによるI/O抽象化、そしてfork/execによるプロセス生成モデルが前提条件として必要だった。これらの基盤が整うまでに数年を要した。

McIlroy自身は後年、パイプの導入を「管理権限を使って実装させようかと思ったほどだ」と語っている。1964年のメモから9年間、構想は頭の中にあり続けた。そして1973年1月、その構想がようやく現実のものとなる。

### パイプ以前のデータ処理

パイプが存在しない世界で、複数のプログラムを連携させるにはどうすればよかったのか。答えは単純だ。一時ファイルを使う。

たとえば、ファイル一覧を取得し、それをページャで表示する場合を考える。パイプがなければ、次のようになる。

```
ls > /tmp/tempfile1
pr < /tmp/tempfile1 > /tmp/tempfile2
lpr < /tmp/tempfile2
rm /tmp/tempfile1 /tmp/tempfile2
```

三つのコマンドを実行するために、二つの一時ファイルを作成し、最後に手動で削除する。コマンドの数が増えれば、一時ファイルも増える。名前の衝突を避けるための工夫が必要になる。複数のユーザーが同時に同じ処理を実行すれば、一時ファイルが干渉する危険もある。

この煩雑さは、単なる不便ではない。一時ファイルを介するということは、最初のコマンドの出力がすべてディスクに書き出されるまで、次のコマンドは開始できないことを意味する。巨大なファイルを処理する場合、最初のコマンドの出力だけでディスク容量を圧迫することもある。効率の問題であり、スケーラビリティの問題だった。

パイプは、この問題を根本から解決した。

---

## 3. 1973年1月15日――一夜の実装

### Ken Thompsonの一夜

1973年1月15日、Ken Thompsonは一夜でパイプを実装した。

McIlroyの証言によれば、Thompsonは「一晩の熱狂的な作業」（"one feverish night"）でpipeシステムコールを書き、シェルにパイプ機能を追加し、`pr`や`ov`などの既存ユーティリティをフィルタとして機能するよう改修した。

この「一夜の実装」は、技術的には三つの作業を含んでいた。

第一に、**pipeシステムコール**の実装。カーネル内にバッファを確保し、二つのプロセス間でデータを受け渡すメカニズムを作る。初期の実装ではバッファサイズは504バイトだった。書き込み側がバッファを満たすと、書き込み側のプロセスはブロックされ、読み取り側がバッファを消費すると、書き込み側が再開する。

第二に、**シェルへのパイプ構文の追加**。当初の構文は `|` ではなく `>` だった。`ls > pr > lpr` のように、`>`の右側にファイル名ではなくコマンド名が来た場合、リダイレクションではなくパイプとして解釈される。この曖昧な構文は数ヶ月で問題になり、後に `|` に置き換えられることになる。

第三に、**既存コマンドのフィルタ化**。パイプの恩恵を受けるためには、各コマンドが標準入力から読み、標準出力に書く「フィルタ」として動作する必要がある。Thompsonはその夜のうちに複数のユーティリティを改修し、パイプに組み込めるようにした。

翌朝の出来事を、McIlroyは次のように回想している。

> "The next day we had a wonderful orgy of 'look at this one.'"
>
> （翌日、「これを見ろ」の素晴らしい饗宴が始まった。）

チームのメンバーが次々とワンライナーを発明し、見せ合った。一週間もすれば、秘書までもがパイプを使っていたという。パイプは即座に「あって当然の機能」として受け入れられた。

### 初期のパイプ記法とその変遷

パイプの記法の変遷は、設計の試行錯誤を示す好例だ。

Thompsonが最初に採用した記法は `>` だった。Dennis Ritchieの"The Evolution of the Unix Time-sharing System"によれば、`>`の後にファイル名が来ればリダイレクション、コマンド名が来ればパイプとして動作した。

```
# 初期のパイプ記法（1973年1月〜数ヶ月間）
ls > pr > lpr
```

この設計には明らかな曖昧性があった。`ls > pr`は「lsの出力をprというファイルにリダイレクトする」のか、「lsの出力をprコマンドにパイプする」のか。prというファイルが存在する場合とprというコマンドが存在する場合で、意味が変わる。

この曖昧性は数ヶ月で解消された。ThompsonはASCIIコード0x7Cのバーティカルバー `|` をパイプ専用の記号として導入した。McIlroyはこの `|` 記法をThompsonの功績としている。1973年11月のFourth Edition Unixのマニュアルでは、`|` がパイプ記法として正式に記載された。

```
# 現在のパイプ記法（1973年後半〜）
ls | pr | lpr
```

この変更は小さいようで大きかった。専用の演算子を持つことで、パイプはリダイレクションとは独立した概念として認識されるようになった。`|` という視覚的に明確な記号は、「プログラムの出力を次のプログラムの入力に接続する」という操作を直感的に表現する。一本の縦線が、二つのプロセスを繋ぐ管を象徴している。

### パイプの技術的構造

パイプの内部実装を理解するために、OSレベルで何が起きているかを見る。

UNIXのパイプは、カーネルが管理するメモリバッファを介したプロセス間通信（IPC: Inter-Process Communication）の仕組みだ。`cmd1 | cmd2`というパイプラインを実行するとき、シェルは以下の手順を踏む。

```
パイプラインの実行手順:

1. シェルが pipe() システムコールを呼ぶ
   → カーネルがバッファを確保し、
     2つのファイルディスクリプタ（fd）を返す
     fd[0] = 読み取り用
     fd[1] = 書き込み用

2. シェルが fork() で子プロセスを2つ生成

3. 子プロセス1（cmd1）:
   - fd[1]（書き込み用）を stdout（fd 1）に複製（dup2）
   - 不要なfdを閉じる
   - exec() で cmd1 を実行

4. 子プロセス2（cmd2）:
   - fd[0]（読み取り用）を stdin（fd 0）に複製（dup2）
   - 不要なfdを閉じる
   - exec() で cmd2 を実行

5. cmd1 が stdout に書き込む
   → データはカーネルバッファに入る

6. cmd2 が stdin から読み込む
   → カーネルバッファからデータを取得

7. バッファが満杯なら cmd1 はブロック（待機）
   バッファが空なら cmd2 はブロック（待機）
   → 生産者-消費者パターンによる自動的な流量制御
```

```
プロセス間のデータフロー:

  cmd1プロセス               cmd2プロセス
  ┌─────────────┐           ┌─────────────┐
  │             │           │             │
  │ stdout ─────┼─→ fd[1]  │  fd[0] ─→ stdin │
  │  (fd 1)     │    │     │    ↑      (fd 0) │
  └─────────────┘    │     └────┼─────────────┘
                     ↓          │
              ┌──────────────┐
              │ カーネルバッファ │
              │  (リングバッファ) │
              │              │
              │  書き込み → ... → 読み取り │
              └──────────────┘

  cmd1とcmd2は同時に実行される（マルチプロセス）
  データはメモリ上のバッファを通じて流れる
  ディスクI/Oは一切発生しない
```

ここで重要なのは、パイプの両端にいるプロセスは**同時に実行される**という点だ。cmd1がデータを生成し、cmd2がデータを消費する。カーネルバッファが緩衝材となり、生産と消費の速度差を吸収する。バッファが満杯になれば書き込み側が待ち、バッファが空になれば読み取り側が待つ。この自動的な流量制御（フロー制御）は、プログラマが意識する必要がない。OSが透過的に管理する。

初期のpipeシステムコールは1つのファイルディスクリプタだけを返し、同じfdで読み書きしていた。バッファサイズは504バイトだった。現代のLinuxでは、pipe(2)は2つのfd（読み取り用と書き込み用）を返し、バッファはデフォルトで65,536バイト（64KiB、16ページ分）のリングバッファとして実装されている。PIPE_BUF（4,096バイト）以下の書き込みはアトミック（不可分）であることが保証される。

### ファイルディスクリプタとI/Oの抽象化

パイプが「美しい」と言われる理由は、その実装がUNIXのI/O抽象化と完全に調和している点にある。

UNIXでは、すべてのI/O操作はファイルディスクリプタ（整数値）を介して行われる。通常のファイルもパイプもソケットもデバイスも、プログラムから見れば「ファイルディスクリプタに対してread/writeする」だけだ。プログラムは自分が読み書きしているのがファイルなのかパイプなのかを知る必要がない。

```
UNIXのI/O抽象化:

  ファイルディスクリプタ 0 (stdin)  ← キーボード / パイプ / ファイル
  ファイルディスクリプタ 1 (stdout) → ターミナル / パイプ / ファイル
  ファイルディスクリプタ 2 (stderr) → ターミナル / ファイル

  プログラムから見れば:
    read(0, buf, n)   ← 入力元が何かは知らない
    write(1, buf, n)  → 出力先が何かは知らない

  接続先の切り替え（リダイレクション/パイプ）は
  シェルがfork後、exec前にdup2()で行う
  → プログラム自身のコードは一切変更不要
```

この抽象化があるからこそ、`grep`は標準入力から読めばよい。入力がキーボードからの直接入力なのか、ファイルからのリダイレクションなのか、パイプを通じた別プロセスの出力なのかを、`grep`自身は意識しない。シェルがfork後、exec前にファイルディスクリプタを付け替える（dup2システムコール）ことで、プログラムのコードを一切変更せずに、入出力の接続先を切り替えられる。

この設計は、プログラムの**再利用性**を劇的に高めた。`grep`はファイルのフィルタリングにも、パイプラインの中間処理にも、対話的な入力のフィルタリングにも使える。同じバイナリが、接続先の変更だけで異なるコンテキストで機能する。

### stderrの誕生――写植機の教訓

標準入出力の設計に関連して、stderr（標準エラー出力）の誕生には逸話がある。

初期のUNIXでは、stdoutしか存在しなかった。プログラムの通常出力もエラーメッセージも、すべてstdoutに書かれた。これが問題になったのは、写植機（phototypesetter）との連携においてだった。

Dennis Ritchieによれば、Bell Labsでは文書の清書にtroffコマンドと写植機を使用していた。troffの出力をパイプ経由で写植機に送ると、処理途中でエラーが発生した場合、エラーメッセージがそのまま写植機に送られる。高価な写植用紙にエラーメッセージが印刷され、無駄になった。

この問題を解決するために導入されたのがstderr（ファイルディスクリプタ2）だ。通常の出力はstdout（fd 1）に、エラーメッセージはstderr（fd 2）に書くという規約が定められた。パイプはstdoutだけを次のプロセスに接続し、stderrはターミナルに直接表示される。これにより、パイプラインの途中でエラーが発生しても、エラーメッセージはユーザーの目に届く。

```
stderrの役割:

  cmd1 | cmd2 | cmd3

  stdout の流れ:
  cmd1 ──→ cmd2 ──→ cmd3 ──→ 最終出力

  stderr の流れ（パイプを通らない）:
  cmd1 ──→ ターミナル
  cmd2 ──→ ターミナル
  cmd3 ──→ ターミナル

  → パイプラインのどの段階でエラーが出ても
    ユーザーの画面にエラーメッセージが表示される
```

stdin、stdout、stderrという三つのストリームの設計は、パイプと不可分の関係にある。パイプが「stdoutを次のプロセスのstdinに接続する」仕組みである以上、エラー出力が別経路で流れなければ、エラー情報が失われる。stderrは、パイプというアーキテクチャの必然的な帰結だった。

---

## 4. 「小さなプログラムの組み合わせ」という思想

### パイプが可能にしたこと

パイプの真の革新は、技術的な仕組みにあるのではない。プログラム設計の思想を変えたことにある。

パイプが存在する前のプログラムは、自己完結型になりがちだった。入力の読み取り、処理、出力の整形をすべて一つのプログラムの中で行う。ログを分析するプログラムは、ファイルのオープンからフィルタリング、集計、フォーマット、出力までを一手に引き受ける。機能追加のたびにプログラムは肥大化する。

パイプが登場したことで、プログラムは「フィルタ」として設計できるようになった。標準入力からデータを受け取り、何らかの変換を施して、標準出力に結果を送る。それだけ。入力がどこから来るか、出力がどこに行くかは、プログラムの関心事ではない。シェルとパイプがそれを決める。

この設計パラダイムの転換が、後にDoug McIlroyによってUNIX哲学として定式化された。Peter H. Salusが1994年の著書"A Quarter Century of Unix"で三箇条に凝縮した表現が最も広く知られている。

> "Write programs that do one thing and do it well.
> Write programs to work together.
> Write programs to handle text streams, because that is a universal interface."
>
> （一つのことをうまくやるプログラムを書け。
> 協調して動くプログラムを書け。
> テキストストリームを扱うプログラムを書け。それがユニバーサルインターフェースだからだ。）

この三箇条の核心は、三番目にある。「テキストストリーム」がユニバーサルインターフェースであるという宣言だ。テキストは最も普遍的なデータ形式であり、人間が読め、プログラムが処理でき、任意のプログラム間で受け渡しできる。パイプは、このテキストストリームをプログラム間で流す管だ。

### フィルタの生態系

パイプの存在が、UNIXにおけるツールの「生態系」を形成した。

`grep`は行をフィルタリングする。`sort`は行を並べ替える。`uniq`は重複を除去する。`wc`は行数を数える。`head`は先頭を切り出す。`tail`は末尾を切り出す。`awk`はフィールドを操作する。`sed`は文字列を置換する。`tr`は文字を変換する。`cut`は列を切り出す。

これらのツールは、単体では限られた機能しか持たない。`grep`はフィルタリングしかできない。`sort`はソートしかできない。だがパイプで繋ぐと、任意の複雑な処理を組み立てることができる。

```bash
# Apacheアクセスログの分析例

# 1. 直近1時間の404エラーのURL上位10件
cat access.log | grep "$(date '+%d/%b/%Y:%H' -d '1 hour ago')" \
    | awk '$9 == 404 {print $7}' | sort | uniq -c | sort -rn | head -10

# 2. IPアドレスごとのリクエスト数（上位10件）
cat access.log | awk '{print $1}' | sort | uniq -c | sort -rn | head -10

# 3. レスポンスコードの分布
cat access.log | awk '{print $9}' | sort | uniq -c | sort -rn

# 4. 特定のURLパターンへのアクセス時間帯分布
cat access.log | grep '/api/v2/' | awk '{print $4}' \
    | cut -d: -f2 | sort | uniq -c
```

これらのパイプラインを見ると、同じツール群（`awk`、`sort`、`uniq -c`、`sort -rn`、`head`）が繰り返し登場することに気づく。ツールの組み合わせパターンを覚えれば、新しい分析タスクに対しても即興でパイプラインを組み立てられる。これがCLIの「組み合わせ可能性」（composability）だ。

### Brian Kernighanとパイプの思想的展開

パイプの思想を体系化し、広く伝えたのはBrian Kernighanだった。

1976年、KernighanとP. J. Plaugerは"Software Tools"を出版した。この書籍は、小さなプログラムを組み合わせてソフトウェアを構築する方法論を体系的に解説した最初の本の一つだ。パイプの思想を「ソフトウェアツール」というメタファーに昇華させた。ハンマーやのこぎりのように、ソフトウェアも道具箱の中の道具であり、組み合わせて使うものだ。

1984年、KernighanとRob Pikeは"The UNIX Programming Environment"を出版した。この書籍はUNIXのフィルタとパイプラインの設計思想を詳細に解説し、「プログラムの内部設計よりも、プログラムがプログラミング環境にどう適合し、他のプログラムとどう組み合わせられるかが重要だ」と明確に述べた。

パイプの影響は、UNIXのツール群の設計を超えて、ソフトウェア工学全体に波及した。「モジュール性」「関心の分離」「インターフェースの標準化」――これらの概念は、パイプが体現した「小さなプログラムの組み合わせ」という原則の一般化だ。

### 名前付きパイプ（FIFO）への拡張

通常のパイプ（無名パイプ）は、親子関係にあるプロセス間でのみ使用できる。シェルがfork()で子プロセスを生成し、それらの間をpipe()で接続するからだ。

この制約を超えるために、名前付きパイプ（FIFO: First In, First Out）が1982年のUNIX System IIIで導入された。名前付きパイプはファイルシステム上に特殊ファイルとして存在し、親子関係のない任意のプロセス間で通信できる。

```bash
# 名前付きパイプの作成と使用
mkfifo /tmp/mypipe

# 端末1: パイプに書き込む
echo "Hello from process A" > /tmp/mypipe

# 端末2: パイプから読み取る
cat < /tmp/mypipe
# → "Hello from process A"
```

名前付きパイプは、通常のパイプと同様にカーネルバッファを使用する。データはディスクに書き込まれない。ファイルシステム上の名前は、プロセス間の「待ち合わせ場所」としてのみ機能する。

この設計は、UNIXの"Everything is a file"（すべてはファイルである）という哲学の表れでもある。パイプというIPC機構を、ファイルシステムの名前空間に載せる。プログラムは通常のファイル操作（open/read/write/close）でパイプにアクセスできる。特別なAPIは不要だ。

---

## 5. ハンズオン：パイプの威力を体験する

パイプの設計思想を理解するには、「パイプなしの世界」と「パイプありの世界」の両方を体験するのが最も効果的だ。

### 演習1：パイプなしでログ分析を行う

```bash
# パイプなしの世界を体験する
docker run --rm -it ubuntu:24.04 bash -c '
echo "=============================================="
echo "[演習1] パイプなしでログ分析を行う"
echo "=============================================="
echo ""

# テスト用のアクセスログを生成
mkdir -p /tmp/handson
cd /tmp/handson

for i in $(seq 1 100); do
    IP="192.168.1.$((RANDOM % 20 + 1))"
    CODE=$( [ $((RANDOM % 10)) -lt 2 ] && echo 404 || echo 200 )
    URLS=("/index.html" "/api/users" "/api/orders" "/about" "/contact" "/api/v2/items" "/login" "/static/style.css")
    URL=${URLS[$((RANDOM % 8))]}
    echo "$IP - - [15/Jan/2025:10:$((RANDOM % 60)):$((RANDOM % 60)) +0900] \"GET $URL HTTP/1.1\" $CODE 1234" >> access.log
done

echo "access.logを生成した（100行）"
echo ""
echo "--- タスク: 404エラーのURL別出現回数を集計する ---"
echo ""
echo "パイプがない場合、一時ファイルを使って段階的に処理する:"
echo ""

echo "Step 1: 404を含む行を抽出して一時ファイルに書き出す"
grep " 404 " access.log > /tmp/handson/step1_filtered.txt
echo "  grep \" 404 \" access.log > step1_filtered.txt"
echo "  → $(wc -l < /tmp/handson/step1_filtered.txt) 行抽出"
echo ""

echo "Step 2: URL列（7番目のフィールド）を抽出して一時ファイルに書き出す"
awk "{print \$7}" /tmp/handson/step1_filtered.txt > /tmp/handson/step2_urls.txt
echo "  awk '"'"'{print \$7}'"'"' step1_filtered.txt > step2_urls.txt"
echo "  → $(wc -l < /tmp/handson/step2_urls.txt) 行"
echo ""

echo "Step 3: ソートして一時ファイルに書き出す"
sort /tmp/handson/step2_urls.txt > /tmp/handson/step3_sorted.txt
echo "  sort step2_urls.txt > step3_sorted.txt"
echo ""

echo "Step 4: 重複をカウントして一時ファイルに書き出す"
uniq -c /tmp/handson/step3_sorted.txt > /tmp/handson/step4_counted.txt
echo "  uniq -c step3_sorted.txt > step4_counted.txt"
echo ""

echo "Step 5: 降順ソートして結果を表示"
sort -rn /tmp/handson/step4_counted.txt
echo ""
echo "  sort -rn step4_counted.txt"
echo ""

echo "作成された一時ファイル:"
ls -la /tmp/handson/step*.txt 2>/dev/null | awk "{print \"  \" \$NF \" (\" \$5 \" bytes)\"}"
echo ""

echo "Step 6: 一時ファイルを手動で削除"
rm /tmp/handson/step*.txt
echo "  rm step*.txt"
echo ""
echo "→ 6ステップ、4つの一時ファイルが必要だった"
echo ""

rm -rf /tmp/handson
echo "=============================================="
'
```

### 演習2：パイプで同じタスクを一行で実行する

```bash
# パイプの威力を体験する
docker run --rm -it ubuntu:24.04 bash -c '
echo "=============================================="
echo "[演習2] パイプで同じタスクを一行で実行する"
echo "=============================================="
echo ""

# 同じテスト用ログを生成
mkdir -p /tmp/handson
cd /tmp/handson
RANDOM=42  # 再現性のために乱数シードを固定

for i in $(seq 1 100); do
    IP="192.168.1.$((RANDOM % 20 + 1))"
    CODE=$( [ $((RANDOM % 10)) -lt 2 ] && echo 404 || echo 200 )
    URLS=("/index.html" "/api/users" "/api/orders" "/about" "/contact" "/api/v2/items" "/login" "/static/style.css")
    URL=${URLS[$((RANDOM % 8))]}
    echo "$IP - - [15/Jan/2025:10:$((RANDOM % 60)):$((RANDOM % 60)) +0900] \"GET $URL HTTP/1.1\" $CODE 1234" >> access.log
done

echo "パイプを使って一行で実行する:"
echo ""
echo "  grep \" 404 \" access.log | awk '"'"'{print \$7}'"'"' | sort | uniq -c | sort -rn"
echo ""
echo "結果:"
grep " 404 " access.log | awk "{print \$7}" | sort | uniq -c | sort -rn | sed "s/^/  /"
echo ""
echo "→ 一時ファイルは一切作られない"
echo "→ 5つのプログラムが同時に実行される"
echo "→ データはメモリ上のカーネルバッファを流れる"
echo ""

echo "--- パイプラインの各段階を確認する ---"
echo ""

echo "Stage 1: grep で404行を抽出"
echo "  grep \" 404 \" access.log | head -3"
grep " 404 " access.log | head -3 | sed "s/^/  /"
echo "  ..."
echo ""

echo "Stage 2: awk でURL列を抽出"
echo "  grep \" 404 \" access.log | awk '"'"'{print \$7}'"'"' | head -5"
grep " 404 " access.log | awk "{print \$7}" | head -5 | sed "s/^/  /"
echo "  ..."
echo ""

echo "Stage 3: sort で整列"
echo "  ... | sort | head -5"
grep " 404 " access.log | awk "{print \$7}" | sort | head -5 | sed "s/^/  /"
echo "  ..."
echo ""

echo "Stage 4: uniq -c で集計"
echo "  ... | sort | uniq -c"
grep " 404 " access.log | awk "{print \$7}" | sort | uniq -c | sed "s/^/  /"
echo ""

echo "Stage 5: sort -rn で降順ソート（最終結果）"
echo "  ... | sort | uniq -c | sort -rn"
grep " 404 " access.log | awk "{print \$7}" | sort | uniq -c | sort -rn | sed "s/^/  /"
echo ""

rm -rf /tmp/handson
echo "=============================================="
'
```

### 演習3：パイプのバッファサイズと並行実行を観察する

```bash
# パイプの内部動作を観察する
docker run --rm -it ubuntu:24.04 bash -c '
echo "=============================================="
echo "[演習3] パイプのバッファと並行実行の観察"
echo "=============================================="
echo ""

echo "--- 1. パイプバッファサイズの確認 ---"
echo ""

# PIPE_BUF の確認
echo "PIPE_BUF（アトミック書き込み保証サイズ）:"
echo "  $(getconf PIPE_BUF /)"
echo ""

echo "パイプの容量（capacity）:"
echo "  デフォルト: 65536 bytes (64 KiB)"
echo "  = 16ページ x 4096 bytes/ページ"
echo ""
echo "  PIPE_BUFとパイプ容量は異なる概念:"
echo "  - PIPE_BUF (4096 bytes): これ以下の書き込みは"
echo "    アトミック（他のプロセスの書き込みと混ざらない）"
echo "  - 容量 (65536 bytes): バッファが満杯になると"
echo "    書き込み側がブロックされる閾値"
echo ""

echo "--- 2. パイプの並行実行を確認 ---"
echo ""
echo "sleep 2 | sleep 2 を実行:"
echo "  シーケンシャルなら4秒、並行なら2秒かかる"
echo ""
START=$(date +%s)
sleep 2 | sleep 2
END=$(date +%s)
ELAPSED=$((END - START))
echo "  実行時間: ${ELAPSED}秒"
if [ $ELAPSED -le 2 ]; then
    echo "  → 2秒で完了 = 並行実行されている"
else
    echo "  → ${ELAPSED}秒 = シーケンシャル実行"
fi
echo ""

echo "--- 3. パイプでのデータ受け渡しを可視化 ---"
echo ""
echo "  seq 1 5 | while read n; do echo \"received: \$n\"; done"
echo ""
seq 1 5 | while read n; do echo "  received: $n"; done
echo ""
echo "  seqが1〜5を出力し、パイプを通じてwhileループが受け取る"
echo "  各行がテキストストリームとして流れている"
echo ""

echo "--- 4. 名前付きパイプ（FIFO）の作成 ---"
echo ""
mkfifo /tmp/test_fifo 2>/dev/null
ls -la /tmp/test_fifo
echo ""
echo "  ファイルタイプが p（パイプ）になっている"
echo "  ファイルシステム上に名前があるが、データはメモリ上"
echo ""
rm -f /tmp/test_fifo

echo "=============================================="
'
```

これらの演習で体験したように、パイプは単なる「コマンドの連結」ではない。一時ファイルの排除、メモリ上のストリーミング処理、プロセスの並行実行という三つの特性が組み合わさった、OSレベルのアーキテクチャだ。

---

## 6. まとめと次回予告

### この回の要点

第一に、パイプの構想は1964年10月11日のDoug McIlroyのBell Labs内部メモに遡る。「プログラム同士を庭のホースのように接続する」という比喩は、9年後の1973年1月に実装として結実した。構想から実装まで9年を要したのは、パイプの実現にはマルチプロセスOS、ファイルディスクリプタ、fork/execモデルという基盤が必要だったからだ。

第二に、Ken Thompsonは1973年1月15日の一夜でpipeシステムコールを実装し、シェルとユーティリティを改修した。翌朝、Bell Labsのチームはワンライナーの饗宴に沸いた。パイプは即座に「あって当然の機能」として受け入れられた。

第三に、パイプの技術的実装はUNIXのI/O抽象化と調和している。ファイルディスクリプタを介したread/writeだけで、ファイルもパイプもソケットも同じインターフェースで操作できる。プログラムは入出力先を意識する必要がない。

第四に、パイプの記法は `>` から `|` に変遷した。リダイレクションとの曖昧性を解消するためにThompsonが `|` を導入し、パイプは独立した概念として視覚的に表現されるようになった。

第五に、パイプはプログラム設計のパラダイムを変えた。自己完結型の大きなプログラムではなく、「一つのことをうまくやる」小さなフィルタを組み合わせる思想が確立された。McIlroyの構想をSalusが三箇条に凝縮したUNIX哲学は、パイプという仕組みなしには成立しない。

### 冒頭の問いへの暫定回答

「小さなプログラムを組み合わせる」という思想は、どこから来たのか。

暫定的な答えはこうだ。**その思想は、1964年のDoug McIlroyの構想に端を発し、1973年1月15日のKen Thompsonの実装によって現実となった。パイプは単なる便利機能ではなく、プログラムの設計思想そのものを変えた仕組みだ。** 一時ファイルの煩雑さから解放されただけではない。「プログラムは一つのことをやればいい、組み合わせはパイプに任せる」という設計原則が、UNIXのツール群の爆発的な増殖を可能にした。

パイプの偉大さは、その単純さにある。テキストを流す管。それだけだ。だがその単純な抽象が、60年近くにわたってソフトウェアの組み合わせ方を規定し続けている。

### 次回予告

次回、第8回「テキスト処理の系譜――ed, grep, sed, awk」では、パイプの中を流れるテキストを操作するツール群の歴史を辿る。`grep`の名前がedコマンドの`g/re/p`に由来すること、`awk`がパターン-アクションという独自のプログラミングモデルを持つこと、そしてこれらのツールが「テキストストリーム」というユニバーサルインターフェースの上に構築された生態系であることを見ていく。

あなたが日常的に使っている`grep`は、1973年にKen Thompsonが、Lee McMahonによる「ザ・フェデラリスト・ペーパーズ」の著者分析のために作ったツールだ。学術的なテキスト分析から生まれたコマンドが、50年後の今もサーバのログ解析に使われている。次回は、その不思議な連続性の理由を探る。

---

## 参考文献

- Dennis Ritchie, "Prophetic Petroglyphs", Bell Labs, <https://www.nokia.com/bell-labs/about/dennis-m-ritchie/mdmpipe.html>
- Dennis Ritchie, "The Evolution of the Unix Time-sharing System", AT&T Bell Laboratories Technical Journal, 1984, <https://www.nokia.com/bell-labs/about/dennis-m-ritchie/hist.pdf>
- Peter H. Salus, "A Quarter Century of Unix", Addison-Wesley, 1994
- Brian Kernighan and Rob Pike, "The UNIX Programming Environment", Prentice Hall, 1984
- Brian Kernighan and P. J. Plauger, "Software Tools", Addison-Wesley, 1976
- Unix Heritage Wiki, "features:pipes", <https://wiki.tuhs.org/doku.php?id=features:pipes>
- McIlroy, M. D., "UNIX Pipe History", Computer History Museum Oral History, <https://www.computerhistory.org/collections/catalog/102740539/>
- pipe(7) Linux manual page, <https://man7.org/linux/man-pages/man7/pipe.7.html>
- Wikipedia, "Pipeline (Unix)", <https://en.wikipedia.org/wiki/Pipeline_(Unix)>
- Wikipedia, "Standard streams", <https://en.wikipedia.org/wiki/Standard_streams>
- Wikipedia, "Named pipe", <https://en.wikipedia.org/wiki/Named_pipe>
- Wikipedia, "Unix philosophy", <https://en.wikipedia.org/wiki/Unix_philosophy>
- Diomidis Spinellis, "unix-history-make", <https://github.com/dspinellis/unix-history-make>

---

**次回：** 第8回「テキスト処理の系譜――ed, grep, sed, awk」

---

_本記事は「ターミナルは遺物か――コマンドラインの本質を問い直す」連載の第7回です。_
_ライセンス：CC BY-SA 4.0_
