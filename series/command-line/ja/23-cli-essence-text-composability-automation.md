# ターミナルは遺物か

## ――コマンドラインの本質を問い直す

### 第23回：コマンドラインの本質に立ち返る――テキスト・組み合わせ・自動化

**連載「ターミナルは遺物か――コマンドラインの本質を問い直す」**
**著：佐藤裕介（Engineers Hub株式会社 CEO / Technical Lead）**

---

**この回で学べること：**

- 60年にわたるコマンドラインの歴史を貫く三つの本質――テキストストリーム（普遍性）、組み合わせ（合成可能性）、自動化（再現性）
- CTSS（1961年）からAI+CLI（2025年）まで、各時代のCLIパラダイムを三つの本質の軸で再評価する視座
- Doug McIlroyの原典（1978年）とPeter H. Salusの要約（1994年）が示したUNIX哲学の構造
- テキストストリームがなぜ60年間「普遍的インターフェース」であり続けたのか――その構造的理由
- 自分の日常のCLIワークフローを「三つの本質」で分析し、改善する方法

---

## 1. 24年間の蒸留

私はこの連載を第1回から書き始めて、22回にわたってコマンドラインの歴史を辿ってきた。テレタイプからCRT端末へ、UNIXパイプからGNU coreutilsへ、TUIの復権からAIエージェントまで。60年分の技術史を言語化する作業は、率直に言って、私自身の24年間のCLI体験を棚卸しする作業でもあった。

1998年にSlackwareの黒い画面の前に座ったとき、私は`ls`と`cd`と`cat`と`grep`しか知らなかった。それでもLinuxの世界に足を踏み入れることができた。2025年の今、私はClaude Codeに自然言語でタスクを指示し、AIエージェントがパイプラインを組み立て、テストを実行し、コードをコミットする環境にいる。27年の間に、ツールは劇的に変わった。

だが、ここで立ち止まって考えたい。

ツールが変わったことは明白だ。では、何が変わらなかったのか。

第1回で私は「テキストという最も普遍的なインターフェースが、あらゆる時代の計算モデルに適応し続けている」と書いた。22回の連載を経て、この命題をより精密に表現できる段階に来た。コマンドラインの本質は、三つの原則に蒸留できる。

第一に、**テキストストリーム**――データを人間にも機械にも読める文字列として流す普遍的インターフェース。

第二に、**組み合わせ**――小さなツールを連結し、単体では不可能な処理を実現する合成可能性。

第三に、**自動化**――手作業をスクリプトとして記録し、再現可能にする力。

この三つは、1961年のCTSSから2025年のAI+CLIに至るまで、形を変えながら一貫して存在し続けている。ツールは変わる。パラダイムも変わる。だが、この三つの原則は変わらない。

あなたは、自分が毎日使っているコマンドラインの中に、この三つの原則をどれだけ意識的に活用しているだろうか。

---

## 2. 三つの本質――60年史の横断的再読

### テキストストリーム：普遍性の源泉

コマンドラインの第一の本質は、テキストストリームだ。

この概念は、Doug McIlroyが最も簡潔に表現している。1994年、Peter H. Salusが『A Quarter Century of UNIX』でMcIlroyの思想を三つの規則に要約した。その第三の規則がこうだ。

> "Write programs to handle text streams, because that is a universal interface."
> （テキストストリームを扱うプログラムを書け。なぜならそれは普遍的インターフェースだからだ。）

「普遍的インターフェース（universal interface）」。この言葉の重みを、60年の歴史を辿った今、改めて考えたい。

なぜテキストストリームは「普遍的」なのか。三つの構造的理由がある。

**第一に、テキストは人間が読める。** バイナリプロトコルやオブジェクトシリアライゼーションと異なり、テキストは`cat`するだけで内容を確認できる。デバッグのとき、パイプラインの途中に`| tee /dev/stderr`を挟めば、データの流れを目視で確認できる。第8回で語ったed、grep、sed、awkの系譜は、すべてこの「人間が読めるテキスト」を前提としている。

**第二に、テキストはスキーマレスだ。** 構造化データ（JSONやProtocol Buffers）は送信側と受信側がスキーマを共有する必要がある。テキストストリームにはその制約がない。`ls`の出力をそのまま`grep`に渡し、`wc -l`で行数を数える。この三つのコマンドは、互いの存在を知らない。出力と入力がテキストであるという一点だけで接続されている。

**第三に、テキストは時代を超える。** 1973年にKen Thompsonが書いた`grep`の出力は、2025年のAIエージェントがそのまま解析できる。50年の間にハードウェアは何世代も入れ替わり、OSのカーネルは書き直され、プログラミング言語のトレンドは何度も変わった。だがテキストストリームのフォーマットは変わっていない。改行で区切られたテキスト行。これが、60年間変わらないインターフェースの実体だ。

第22回で語ったように、AIエージェントがCLIコマンドの出力を解析する際、テキストストリームの「スキーマレスさ」はLLMにとって予想外の利点を発揮している。LLMは構造化データよりも自然言語に近いテキストの方が得意だ。テキストストリームという50年前の設計判断が、AI時代にも最適なインターフェースであり続けている。これは偶然ではない。テキストが「人間にも機械にも読める」という性質は、機械の定義がCPUからLLMに拡張されても、なお有効なのだ。

ただし、テキストストリームには弱点もある。第10回で語ったように、スキーマレスであるがゆえに、パース処理は脆弱になりがちだ。`ps aux`の出力を`awk`で列を切り出すとき、列の区切りがスペースなのかタブなのか、プロセス名にスペースが含まれるとどうなるのか。第20回で語ったPowerShellのオブジェクトパイプラインは、この弱点に対する正当な批判だった。テキストストリームの普遍性は、構造の欠如と表裏一体だ。

### 組み合わせ：合成可能性の力

コマンドラインの第二の本質は、組み合わせだ。ソフトウェア工学の用語では「合成可能性（composability）」と呼ばれる。

McIlroyの三規則の最初の二つを引用する。1978年のBell System Technical Journalに掲載された原典はこうだ。

> "Make each program do one thing well. To do a new job, build afresh rather than complicate old programs by adding new features."
> （各プログラムに一つのことをうまくやらせよ。新しい仕事には、古いプログラムに新機能を追加して複雑にするのではなく、新しく作れ。）
>
> "Expect the output of every program to become the input to another, as yet unknown, program."
> （すべてのプログラムの出力が、まだ知らない別のプログラムの入力になることを想定せよ。）

二番目の規則が特に重要だ。「まだ知らない（as yet unknown）」という表現に注目してほしい。これは、設計時点で存在しないツールとの連携を前提にしている。1973年に書かれた`grep`は、2016年に登場した`ripgrep`とパイプで連結できる。1974年の`sed`は、2019年に登場したNushellのテーブルに変換できる。これが合成可能性の真の力だ。

合成可能性を可能にしているのは、UNIXのプロセスモデルとパイプの設計だ。第7回で詳しく語ったが、パイプで連結された各プロセスは、互いのメモリ空間を共有しない。通信はテキストストリームだけを通じて行われる。この「隔離された合成」こそが、コマンドラインの組み合わせが堅牢である理由だ。

```
隔離された合成（Isolated Composition）:

┌─────────┐    テキスト    ┌─────────┐    テキスト    ┌─────────┐
│  grep   │───ストリーム──►│  sort   │───ストリーム──►│  uniq   │
│         │               │         │               │         │
│ 独立した │               │ 独立した │               │ 独立した │
│ メモリ空間│               │ メモリ空間│               │ メモリ空間│
└─────────┘               └─────────┘               └─────────┘

各プロセスは互いの内部状態を知らない。
共有するのはテキストストリームだけ。
→ 関数呼び出しのようなメモリ共有による結合がない。
→ 一つのコンポーネントの交換・更新が他に影響しない。
```

この設計は、現代のマイクロサービスアーキテクチャが目指している「疎結合」を、50年前にプロセスレベルで実現していたとも言える。各コマンドは独立したプロセスとして実行され、stdin/stdoutだけで通信する。内部実装がCで書かれていようがRustで書かれていようが、パイプラインの他の部分には影響しない。第17回で語ったRust製CLIツール群（ripgrep、fd、bat、eza）が、既存のパイプラインにシームレスに組み込めるのは、この設計のおかげだ。

一方で、合成可能性には限界もある。第10回で議論したように、パイプラインの途中でエラーが発生した場合の処理は脆弱だ。`cmd1 | cmd2 | cmd3`というパイプラインでcmd2がエラーで終了しても、bashのデフォルト設定ではパイプライン全体の終了コードはcmd3のものになる（`set -o pipefail`を明示的に設定しない限り）。合成可能性の代償として、エラー伝播の仕組みが犠牲になっている。

### 自動化：再現性の保証

コマンドラインの第三の本質は、自動化だ。より正確には、操作のスクリプト化による再現性（reproducibility）の保証だ。

GUIでの操作は、原理的に記録が困難だ。「このボタンをクリックし、このチェックボックスを外し、このドロップダウンからこの値を選ぶ」――この手順をテキストとして記録し、後から自動的に再実行することは、専用のツール（Selenium、Playwright等）がなければ難しい。

CLIでの操作は、そのままスクリプトになる。ターミナルで実行したコマンドをシェルスクリプトにコピーすれば、それが自動化だ。この「操作とスクリプトの同型性」が、CLIの自動化を圧倒的に容易にしている。

自動化の歴史は、CLIの歴史そのものに組み込まれている。

1975年5月、Ken Thompsonが開発したcronは、コマンドの自動実行を時間軸上に配置する仕組みだった。毎日午前3時にログを圧縮する、毎週月曜にバックアップを取る――これらの定型作業を、人間が覚えておく必要がなくなった。cronの設計は驚くほどシンプルだ。五つのフィールド（分、時、日、月、曜日）でスケジュールを指定し、実行するコマンドを記述する。テキストファイル（crontab）にルールを書くだけで、自動化が完成する。

1990年代後半から2000年代にかけて、シェルスクリプトはインフラ管理の基盤となった。私自身、2000年代後半にはシェルスクリプト一つで数十台のサーバの設定を統一し、パッケージのインストール、設定ファイルの配布、サービスの再起動を自動化していた。手作業で30台のサーバを一台ずつ設定していた時代から、スクリプト一発で100台を設定できるようになった転換は、私のキャリアにおける最大の生産性向上の一つだった。

2013年3月、Solomon HykesがPyConでDockerを発表した。Dockerfileはまさに「CLIコマンドの自動化スクリプト」だ。`FROM`でベースイメージを指定し、`RUN`でコマンドを実行し、`COPY`でファイルを配置する。すべてがテキストで記述され、バージョン管理でき、再現可能だ。2014年6月にGoogleがKubernetesを発表し、コンテナオーケストレーションの世界でも`kubectl`というCLIがインフラ操作の中心に据えられた。HashiCorpのTerraform（2014年7月）は、クラウドインフラストラクチャそのものをCLIで宣言的に管理する「Infrastructure as Code」の概念を普及させた。

CI/CD（継続的インテグレーション/継続的デリバリー）パイプラインは、この自動化の原則の集大成だ。GitHub ActionsのワークフローYAMLを見れば、その中身はCLIコマンドのシーケンスそのものだ。`npm install`、`npm test`、`npm run build`、`docker push`。GUIが存在しないヘッドレス環境で、CLIコマンドだけがソフトウェアのビルド・テスト・デプロイを遂行する。

自動化の強さは、再現性だけではない。**監査可能性（auditability）** も含まれる。シェルスクリプトは差分（diff）が取れる。先月のデプロイスクリプトと今月のデプロイスクリプトの違いを、`git diff`で一目瞭然にできる。GUIでの操作手順書をどれだけ丁寧に書いても、この精度には及ばない。

---

## 3. 60年史を三つの軸で再評価する

22回にわたって語ってきたCLIの歴史を、テキスト・組み合わせ・自動化の三つの軸で横断的に再評価してみたい。各時代のパラダイムが、三つの原則のどれを推進し、どれに制約を受けていたかを整理する。

### 黎明期：対話の成立（1961年-1972年）

```
時代: 1961年-1972年（CTSS, Multics, UNIX初期）

テキスト    ████████░░  [高い] テレタイプ出力は紙テープ上のテキスト
組み合わせ  ██░░░░░░░░  [低い] パイプ未発明。コマンドは孤立して実行
自動化      ███░░░░░░░  [中低] バッチ処理（JCL）は存在するが柔軟性に欠ける
```

1961年のCTSSは、テキストベースの対話的コンピューティングを実現した。LOGIN、LISTF、EDITといったコマンドをテキストで入力し、テキストで応答を得る。テキストストリームの原則は、この時点で既に存在していた。

だが、組み合わせの原則はまだない。CTSSのコマンドは個別に実行され、あるコマンドの出力を別のコマンドの入力に直接渡す仕組みはなかった。自動化については、バッチ処理（第2回で語ったJCL）という形で存在していたが、対話的な操作を自動化する柔軟なスクリプティング機構はまだなかった。

### パイプの革命（1973年-1978年）

```
時代: 1973年-1978年（UNIX V3-V7, パイプ, coreutils確立）

テキスト    █████████░  [非常に高い] stdin/stdout/stderrの確立
組み合わせ  ████████░░  [高い] パイプ発明。小さなツールの組み合わせ
自動化      █████░░░░░  [中] シェルスクリプト登場。cronの実装（1975年）
```

1973年1月15日のパイプの実装は、組み合わせの原則を爆発的に推進した。Doug McIlroyが1964年から構想していた「プログラムの連結」が、Ken Thompsonの一晩の実装で現実になった。同時に、stdin/stdout/stderrという三つのストリームの確立が、テキストストリームの原則を制度化した。

この時代に、ed（1969年）からgrep（1973年）、sed（1974年）、awk（1977年）に至るテキスト処理ツールの生態系が成立した。各ツールは単体では限定的な機能しか持たないが、パイプで組み合わせることで、事実上無限の処理を実現できる。

自動化については、シェルスクリプトとcron（1975年）の登場で基盤が整い始めた。だが、この時代のシェルスクリプトはまだ原始的であり、エラー処理や条件分岐の機構は限られていた。

### 標準化と自由化（1979年-1992年）

```
時代: 1979年-1992年（ANSI標準, GNU, Plan 9）

テキスト    █████████░  [非常に高い] ANSI X3.64で端末の視覚表現も標準化
組み合わせ  ████████░░  [高い] POSIX標準化。ツール間の互換性向上
自動化      ██████░░░░  [中高] シェルスクリプトの成熟。Makefile文化
```

1979年、ANSI X3.64標準（エスケープシーケンス）の採択は、テキストストリームの原則を視覚表現にまで拡張した。テキストの中に制御コードを埋め込むことで、色、カーソル移動、画面クリアが可能になった。第5回で語ったように、これはテキストストリームの上に構築された「疑似グラフィック」プロトコルだ。テキストの普遍性を壊すことなく、表現力を拡張した。

1983年9月27日、Richard StallmanがGNUプロジェクトを発表した。GNUの戦略は「まずツールを再実装する」ことだった。GNU coreutilsは、AT&T UNIXのプロプライエタリなツール群を自由なソフトウェアとして再実装した。第14回で語ったように、GNU拡張（long options、`--help`フラグの統一）はCLIの使い勝手を向上させたが、同時にBSDツールとの非互換性という新たな問題も生んだ。

1992年、Plan 9がBell Labsから大学に配布された。第15回で語ったように、Plan 9はUNIX哲学を極端に推し進め、「Everything is a file」をネットワークリソースにまで拡張した。Plan 9の9PプロトコルやUTF-8（1992年9月、Ken ThompsonとRob Pikeがダイナーのプレイスマットの上で設計した）は、テキストストリームの普遍性をグローバルな文字セットに拡張する歴史的な発明だった。

### GUIとの戦い（1984年-2005年）

```
時代: 1984年-2005年（Macintosh, Windows, Web時代）

テキスト    ██████░░░░  [中高] GUIの普及でテキストの優位性に疑問
組み合わせ  █████░░░░░  [中] GUIアプリは組み合わせ困難
自動化      ██████░░░░  [中高] サーバ管理ではCLI/スクリプトが不可欠
```

第11回で語ったように、1984年のMacintosh、1995年のWindows 95は「CLIは死ぬ」という予言を生んだ。GUIは直接操作（direct manipulation）という認知モデルで、コンピュータの操作を直感的にした。デスクトップのアイコンをドラッグ&ドロップするだけでファイルを移動できる世界は、`mv source dest`とタイプする世界よりも「発見しやすい」。

だが、第12回で語ったように、GUIの普及はCLIを殺さなかった。理由は構造的だ。GUIアプリケーションは合成が困難だ。Photoshopの出力をExcelの入力に直接パイプすることはできない。各GUIアプリケーションは独自のファイルフォーマットを持ち、データの受け渡しにはファイル保存→ファイル読み込みという手動のステップが必要になる。自動化も同様だ。GUIの操作を記録し再現するには、マウスの座標やウィンドウの状態といった非テキスト的な情報を扱わなければならない。

一方、1995年にTatu YlönenがSSH（Secure Shell）を開発したことで、リモート管理におけるCLIの不可欠性は決定的になった。第16回で語ったように、SSHはテキストストリームを暗号化チャネルで保護し、帯域効率の高いリモートアクセスを実現した。GUIのリモートデスクトップ（VNC、RDP）と比較して、SSHの帯域効率は圧倒的だ。テキストストリームだからこそ、低帯域でもリモート操作が実用的に行える。

### モダンCLIルネサンス（2013年-2024年）

```
時代: 2013年-2024年（Docker, k8s, Rust CLIツール, TUI復権）

テキスト    █████████░  [非常に高い] JSON/YAML等の構造化テキストの台頭
組み合わせ  █████████░  [非常に高い] jq/yqによる構造化データの組み合わせ
自動化      ██████████  [極めて高い] IaC, CI/CD, コンテナ化。すべてがスクリプト
```

2013年以降、CLIはルネサンスを迎えた。Docker（2013年）、Kubernetes（2014年）、Terraform（2014年）の登場で、インフラストラクチャの構築と管理がCLIコマンドとテキストファイルで完全に記述可能になった。「Infrastructure as Code」の概念は、自動化の原則を極限まで推し進めた。サーバの設定、ネットワークの構成、データベースのスキーマ――すべてがテキストファイルに記述され、バージョン管理され、自動化パイプラインで実行される。

同時に、第17回で語ったRust製CLIツール群が、テキスト処理の速度とUXを劇的に改善した。ripgrep（2016年、Andrew Gallant）はgrepの10倍以上の速度を達成し、`.gitignore`の自動認識やデフォルトの色付き出力でUXを刷新した。だが、ripgrepの出力はテキストストリームであり、既存のパイプラインにそのまま組み込める。合成可能性を壊さずに、性能とUXを向上させた。

第18回で語ったTUIの復権も注目に値する。Bubbletea（2020年-、Go）やTextual（2021年、Python）といったフレームワークは、第5回で語ったANSIエスケープシーケンスの上に宣言的UIを構築している。テキストストリームの範囲内で視覚的なフィードバックを提供する。lazygit、k9s、htop――これらのTUIアプリケーションは、SSHセッション越しに使えるという意味で、テキストストリームの普遍性を活かしている。

### AI+CLIの時代（2025年-）

```
時代: 2025年-（Claude Code, エージェント型CLI）

テキスト    ██████████  [極めて高い] LLMがテキストを「理解」する
組み合わせ  ████████░░  [高い] AIがツールを動的に組み合わせる
自動化      █████████░  [非常に高い] 自然言語→スクリプト生成の自動化
```

第22回で詳しく語ったように、2025年はAI+CLIの転換点だった。Claude Code、Codex CLI、Gemini CLIといったエージェント型ツールが、自然言語の指示からCLIコマンドを組み立て、実行し、結果を解釈し、次のステップを決定する。

三つの本質の観点から見ると、AI+CLIは三つすべてを活用している。テキストストリームがLLMの入出力インターフェースとして機能し、組み合わせがAIによる動的なパイプライン構築として実現され、自動化が自然言語からのスクリプト生成として拡張されている。

だが、第22回で指摘したように、AIが生成したコマンドの「監査」には、三つの本質への深い理解が不可欠だ。テキストストリームの仕様（`-mtime -7`と`-mtime 7`の違い）、組み合わせの制約（パイプラインの評価順序）、自動化の前提（環境依存性）――これらを知らなければ、AIの出力を検証できない。

---

## 4. 三つの本質の交差点

三つの本質は独立しているのではなく、互いに強化し合っている。その交差点を見ることで、CLIの構造的な強さがより明確になる。

### テキスト × 組み合わせ：パイプラインの設計

テキストストリームと合成可能性の交差点が、UNIXパイプラインだ。

```bash
# アクセスログから404エラーの多いURLを集計する
cat access.log | grep " 404 " | awk '{print $7}' | sort | uniq -c | sort -rn | head -20
```

この一行には、テキストと組み合わせの両方の原則が凝縮されている。各コマンド（cat, grep, awk, sort, uniq, head）はテキストストリームを入力として受け取り、テキストストリームを出力する。各コマンドは独立したプロセスとして実行され、パイプで連結される。任意のコマンドを入れ替えられる。`grep`を`ripgrep`に、`sort`を`GNU sort`からBSD `sort`に置き換えても、パイプラインは動作する。

この設計が可能なのは、インターフェースが「テキスト行」という最小限の約束だけで成り立っているからだ。第10回で語ったように、この約束の「緩さ」は弱点でもある（構造化データの扱いが脆弱）。だが、緩さは同時に柔軟性でもある。50年前のツールと最新のツールが、同じパイプラインの中で共存できる。

### テキスト × 自動化：スクリプトの記録

テキストストリームと自動化の交差点が、シェルスクリプトだ。

ターミナルで実行したコマンドは、テキストとして記録されている。`history`コマンドで履歴を表示し、有用なコマンドの連なりをファイルにコピーし、先頭に`#!/bin/bash`を追加すれば、それがスクリプトだ。操作の記録とスクリプトが同じ形式（テキスト）であることが、自動化の敷居を劇的に下げている。

```bash
# ターミナルでの手動操作
$ find . -name "*.log" -mtime +30 -delete
$ systemctl restart nginx
$ curl -s http://localhost/health | jq '.status'

# そのままスクリプトになる
#!/bin/bash
set -euo pipefail
find . -name "*.log" -mtime +30 -delete
systemctl restart nginx
curl -s http://localhost/health | jq '.status'
```

GUIの操作はこうはいかない。マウスの動きをテキストとして記録するには、座標、タイミング、ウィンドウの状態といったコンテキスト情報が必要だ。CLIの操作は最初からテキストなのだから、記録も再生もテキスト処理で完結する。

### 組み合わせ × 自動化：パイプラインのスクリプト化

合成可能性と自動化の交差点が、CI/CDパイプラインだ。

```yaml
# GitHub Actions の例
steps:
  - run: npm install
  - run: npm test
  - run: npm run build
  - run: docker build -t myapp .
  - run: docker push myapp:latest
```

各ステップは独立したCLIコマンドであり（組み合わせ）、全体がYAMLファイルとして記述され（テキスト）、自動的に実行される（自動化）。三つの原則がすべて結合している。

### テキスト × 組み合わせ × 自動化：三位一体

三つの原則がすべて結合する場面こそ、CLIの真価が発揮される場面だ。

Infrastructure as Codeは、その最たる例だ。Terraformの`.tf`ファイルはテキストで書かれ（テキスト）、モジュールとして分割・組み合わせができ（組み合わせ）、`terraform plan && terraform apply`で自動的に実行される（自動化）。さらに、`.tf`ファイルはgitでバージョン管理され、PR（Pull Request）でレビューされ、CI/CDパイプラインで適用される。テキスト→組み合わせ→自動化の三重螺旋だ。

---

## 5. ハンズオン：三つの本質で自分のワークフローを分析する

### 環境構築

```bash
# Docker環境で実行（ubuntu:24.04ベース）
docker run -it --rm ubuntu:24.04 bash
```

### 演習1：テキストストリームの普遍性を確認する

この演習では、異なる時代のツールがテキストストリームを通じて連携できることを確認する。

```bash
apt-get update && apt-get install -y coreutils findutils grep gawk curl jq

echo "=== 演習1: テキストストリームの普遍性 ==="
echo ""

# テスト用データ生成（CSVログ）
mkdir -p /tmp/cli-essence
cat > /tmp/cli-essence/access.log << 'LOGEOF'
2025-01-15 10:00:01 GET /api/users 200 45ms
2025-01-15 10:00:02 POST /api/users 201 120ms
2025-01-15 10:00:03 GET /api/products 200 30ms
2025-01-15 10:00:04 GET /api/users/1 404 5ms
2025-01-15 10:00:05 GET /api/products 200 28ms
2025-01-15 10:00:06 DELETE /api/users/1 500 200ms
2025-01-15 10:00:07 GET /api/products 200 32ms
2025-01-15 10:00:08 GET /api/users 200 42ms
2025-01-15 10:00:09 POST /api/orders 201 150ms
2025-01-15 10:00:10 GET /api/orders/1 404 4ms
2025-01-15 10:00:11 GET /api/products/5 200 25ms
2025-01-15 10:00:12 PUT /api/users/2 200 80ms
2025-01-15 10:00:13 GET /api/health 200 2ms
2025-01-15 10:00:14 GET /api/users/99 404 3ms
2025-01-15 10:00:15 POST /api/products 500 300ms
LOGEOF

echo "--- 1973年のツールでログを分析する ---"
echo ""

echo "grepでエラー（4xx/5xx）を抽出:"
grep -E ' [45][0-9]{2} ' /tmp/cli-essence/access.log
echo ""

echo "awkでURLとステータスコードだけを抽出:"
awk '{print $4, $5}' /tmp/cli-essence/access.log
echo ""

echo "grep + awk + sort + uniq -c でエラーURLを集計:"
grep -E ' [45][0-9]{2} ' /tmp/cli-essence/access.log \
  | awk '{print $4}' \
  | sort \
  | uniq -c \
  | sort -rn
echo ""

echo "→ grep（1973年）、awk（1977年）、sort/uniq（1970年代）が"
echo "  2025年に書かれたログをそのまま処理できる。"
echo "  テキストストリームというインターフェースが"
echo "  50年間変わっていないからだ。"
```

### 演習2：組み合わせの力を体験する

```bash
echo ""
echo "=== 演習2: 組み合わせの力 ==="
echo ""

echo "--- 同じデータに対して異なるパイプラインを組む ---"
echo ""

echo "パイプライン1: HTTPメソッド別のリクエスト数"
awk '{print $3}' /tmp/cli-essence/access.log \
  | sort | uniq -c | sort -rn
echo ""

echo "パイプライン2: 応答時間が100msを超えるリクエスト"
awk '{
  gsub(/ms/, "", $6);
  if ($6+0 > 100) print $0
}' /tmp/cli-essence/access.log
echo ""

echo "パイプライン3: 時間帯別のリクエスト数（秒単位）"
awk '{print substr($2,1,7)}' /tmp/cli-essence/access.log \
  | sort | uniq -c
echo ""

echo "→ 同じ入力データ（access.log）に対して、"
echo "  パイプラインを組み替えるだけで異なる分析ができる。"
echo "  各コマンドは入力がどこから来たか知らない。"
echo "  出力がどこに行くかも知らない。"
echo "  この「無関心」こそが合成可能性の源泉だ。"
```

### 演習3：自動化の実践

```bash
echo ""
echo "=== 演習3: 手動操作をスクリプトに変換する ==="
echo ""

echo "--- 手動操作の記録 ---"
echo ""
echo "以下のコマンドを手で実行したとする:"
echo '  1. find /tmp/cli-essence -name "*.log" -exec wc -l {} +'
echo '  2. grep -c "500" /tmp/cli-essence/access.log'
echo '  3. awk "/500/{print}" /tmp/cli-essence/access.log'
echo ""

echo "--- これをスクリプトにする ---"
echo ""

# 簡易ログ分析スクリプトを生成
cat > /tmp/cli-essence/analyze.sh << 'SCRIPT'
#!/bin/bash
set -euo pipefail

LOG_DIR="${1:-.}"
echo "=== ログ分析レポート ==="
echo "対象ディレクトリ: ${LOG_DIR}"
echo "実行日時: $(date)"
echo ""

echo "--- ファイル一覧と行数 ---"
find "${LOG_DIR}" -name "*.log" -exec wc -l {} +
echo ""

echo "--- ステータスコード別集計 ---"
for code in 200 201 404 500; do
    count=$(grep -c " ${code} " "${LOG_DIR}"/*.log 2>/dev/null || echo "0")
    echo "  HTTP ${code}: ${count} 件"
done
echo ""

echo "--- エラーリクエスト詳細（4xx/5xx） ---"
grep -E ' [45][0-9]{2} ' "${LOG_DIR}"/*.log 2>/dev/null || echo "  エラーなし"
echo ""

echo "--- 応答時間の統計 ---"
awk '{
  gsub(/ms/, "", $6);
  sum += $6; count++;
  if ($6+0 > max) max = $6
}
END {
  if (count > 0)
    printf "  平均: %.1fms  最大: %dms  リクエスト数: %d\n", sum/count, max, count
}' "${LOG_DIR}"/*.log

echo ""
echo "=== レポート終了 ==="
SCRIPT

chmod +x /tmp/cli-essence/analyze.sh

echo "生成されたスクリプト:"
cat /tmp/cli-essence/analyze.sh
echo ""
echo "--- スクリプトを実行 ---"
echo ""
bash /tmp/cli-essence/analyze.sh /tmp/cli-essence
echo ""

echo "→ ターミナルで手動実行したコマンドが"
echo "  そのままスクリプトになった。"
echo "  テキストで操作し（テキストの原則）、"
echo "  コマンドを組み合わせ（組み合わせの原則）、"
echo "  スクリプトとして記録した（自動化の原則）。"
echo "  三つの原則が一つのワークフローに凝縮されている。"
```

### 演習4：三つの本質で自分のワークフローを棚卸しする

```bash
echo ""
echo "=== 演習4: ワークフロー棚卸しワークシート ==="
echo ""

cat << 'WORKSHEET'
以下の表を自分の日常のCLIワークフローで埋めてみよう。

┌───────────────────┬──────────┬──────────┬──────────┐
│ 日常のタスク       │テキスト  │組み合わせ│自動化    │
│                   │活用度    │活用度    │活用度    │
├───────────────────┼──────────┼──────────┼──────────┤
│ 例: ログ分析       │ ◎       │ ◎       │ △       │
│   grep+awkで都度   │テキスト  │パイプ   │毎回手動  │
│   手動実行         │ログ直接 │ライン   │で打つ    │
│                   │参照     │駆使     │         │
├───────────────────┼──────────┼──────────┼──────────┤
│ 例: デプロイ       │ ◎       │ ◎       │ ◎       │
│   CI/CDパイプライン │YAML/    │ステップ │自動実行  │
│                   │テキスト │連結     │         │
├───────────────────┼──────────┼──────────┼──────────┤
│ あなたのタスク1:    │          │          │          │
│                   │          │          │          │
├───────────────────┼──────────┼──────────┼──────────┤
│ あなたのタスク2:    │          │          │          │
│                   │          │          │          │
├───────────────────┼──────────┼──────────┼──────────┤
│ あなたのタスク3:    │          │          │          │
│                   │          │          │          │
└───────────────────┴──────────┴──────────┴──────────┘

評価基準:
  ◎ = 原則を十分に活かしている
  ○ = 部分的に活かしている
  △ = ほとんど活かしていない
  × = まったく活かしていない

改善のヒント:
  テキスト△ → 出力をJSON/テキストに変換するオプションがないか確認
  組み合わせ△ → パイプラインで他のツールと繋げられないか検討
  自動化△ → 繰り返し実行しているなら、スクリプト化を検討

WORKSHEET

echo ""
echo "→ 自分のワークフローを三つの軸で分析すると、"
echo "  どの原則を活かし、どの原則を無視しているかが見える。"
echo "  無視している原則を意識的に取り入れることが、"
echo "  CLIワークフローの改善につながる。"

# クリーンアップ
rm -rf /tmp/cli-essence
```

ハンズオンの自動セットアップスクリプトは `handson/command-line/23-cli-essence-text-composability-automation/setup.sh` を参照してほしい。

---

## 6. まとめと次回予告

### この回の要点

第一に、コマンドラインの本質は三つの原則に蒸留できる。テキストストリーム（普遍性）、組み合わせ（合成可能性）、自動化（再現性）。この三つは1961年のCTSSから2025年のAI+CLIまで、60年にわたって形を変えながら一貫して存在している。ツールは変わるが、原則は変わらない。

第二に、テキストストリームが「普遍的インターフェース」である理由は構造的だ。人間が読める、スキーマレスである、時代を超える。この三つの性質が、50年前のgrepと2025年のAIエージェントを同じインターフェースで接続可能にしている。Doug McIlroyの洞察「text streams, because that is a universal interface」（Peter H. Salusによる1994年の要約）は、AI時代に至ってなお的確だ。

第三に、組み合わせ（合成可能性）は、UNIXのプロセスモデルとパイプの設計に支えられている。各プロセスが独立したメモリ空間で動作し、テキストストリームだけで通信する「隔離された合成」が、50年前のツールと最新のツールの共存を可能にしている。

第四に、自動化は「操作とスクリプトの同型性」に根ざしている。CLIでの操作はテキストとして記録され、そのままスクリプトとして再実行できる。cron（1975年）からCI/CDパイプライン（2020年代）まで、自動化の形式は進化したが、CLIコマンドのシーケンスという本質は変わっていない。

第五に、三つの原則は独立ではなく相互に強化し合う。テキスト×組み合わせ=パイプライン。テキスト×自動化=シェルスクリプト。組み合わせ×自動化=CI/CD。三つすべての交差=Infrastructure as Code。

### 冒頭の問いへの暫定回答

結局、コマンドラインの本質とは何なのか。

テキストストリームという普遍的インターフェース。小さなツールの組み合わせによる合成可能性。操作のスクリプト化による再現性。この三つだ。

1973年にKen Thompsonがパイプを一晩で実装したとき、彼はこの三つの原則すべてを一つのシステムコールに凝縮した。パイプはテキストストリームを流し（テキスト）、プロセスを連結し（組み合わせ）、シェルスクリプトの中でそのまま再利用できる（自動化）。パイプの発明から52年が経った今、AIエージェントがCLIコマンドを組み立てるとき、依然としてこの三つの原則の上に立っている。

あなたが明日ターミナルを開くとき、自分の操作を三つの原則で分析してみてほしい。テキストストリームを活用しているか。ツールを組み合わせているか。繰り返す操作をスクリプト化しているか。三つのうち、どれかが欠けていたら、そこに改善の余地がある。

### 次回予告

次回、最終回となる第24回「ターミナルは遺物か、改めて問う――あなたのインターフェースを選べ」では、この連載を通じて得た知識を明日からどう活かすかを語る。

CLI、GUI、TUI、自然言語――インターフェースの歴史が教えてくれることは、「最適なインターフェースはタスクと文脈で変わる」ということだ。ターミナルを使うな、とは言わない。ターミナルを「選んで」使え。選ぶためには、歴史を知れ。そして、歴史を知った上で、自分のチームとプロジェクトにとって最適なインターフェースの使い分けを設計せよ。

24回にわたる旅の最終地点で、改めて問いたい。ターミナルは遺物か？

---

## 参考文献

- Doug McIlroy, "UNIX Time-Sharing System: Foreword", Bell System Technical Journal, 1978
- Peter H. Salus, "A Quarter Century of UNIX", Addison-Wesley, 1994
- Brian Kernighan, Rob Pike, "The UNIX Programming Environment", Prentice Hall, 1984
- Eric S. Raymond, "The Art of UNIX Programming", Addison-Wesley, 2003, <http://www.catb.org/esr/writings/taoup/html/>
- Wikipedia, "Compatible Time-Sharing System", <https://en.wikipedia.org/wiki/Compatible_Time-Sharing_System>
- Wikipedia, "Pipeline (Unix)", <https://en.wikipedia.org/wiki/Pipeline_(Unix)>
- The New Stack, "Pipe: How the System Call That Ties Unix Together Came About", <https://thenewstack.io/pipe-how-the-system-call-that-ties-unix-together-came-about/>
- Wikipedia, "ANSI escape code", <https://en.wikipedia.org/wiki/ANSI_escape_code>
- Wikipedia, "GNU Manifesto", <https://en.wikipedia.org/wiki/GNU_Manifesto>
- Rob Pike, "The history of UTF-8 as told by Rob Pike", <https://doc.cat-v.org/bell_labs/utf-8_history>
- Wikipedia, "Plan 9 from Bell Labs", <https://en.wikipedia.org/wiki/Plan_9_from_Bell_Labs>
- Wikipedia, "Docker (software)", <https://en.wikipedia.org/wiki/Docker_(software)>
- Wikipedia, "Kubernetes", <https://en.wikipedia.org/wiki/Kubernetes>
- Wikipedia, "Terraform (software)", <https://en.wikipedia.org/wiki/Terraform_(software)>
- Andrew Gallant, "ripgrep is faster than {grep, ag, git grep, ucg, pt, sift}", 2016, <https://burntsushi.net/ripgrep/>
- Wikipedia, "Secure Shell", <https://en.wikipedia.org/wiki/Secure_Shell>
- Wikipedia, "cron", <https://en.wikipedia.org/wiki/Cron>
- Wikipedia, "Unix philosophy", <https://en.wikipedia.org/wiki/Unix_philosophy>
